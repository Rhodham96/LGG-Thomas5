{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i30pFFO701R",
        "outputId": "d3fa40ee-2e52-433f-ed19-081d36886b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: pandas in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (2.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xlf46XgTtL8w"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# Name: Hamers Robin\n",
        "# GitHub: Rhodham96\n",
        "# Year: 2025\n",
        "# Description: Attention is all you need - Build a GPT from scratch, helped with Andrej Kartpathy video \"Let's build GPT: from scratch, in code, spelled out\"\n",
        "# =======================================================\n",
        "\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import string\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bjy9DkB_sqS"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "dropout_rate = 0.1\n",
        "vocab_size = 8000\n",
        "#max_len = 50 # max seq len\n",
        "n_embd = 384\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gorXBeeC4pa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "H3gjgLBh4sjM",
        "outputId": "3ad4c469-4dfc-45aa-eb41-694c70fbd2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"username\":\"robinhamers\",\"key\":\"f4476bf89c5cffe5f8fad8967f0f7b7c\"}\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/Users/robinhamers/Downloads/kaggle.json\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    contenu = f.read()\n",
        "    print(contenu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ35zeMS4sgE",
        "outputId": "fd837704-6eef-4cdf-b037-61332fefe955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
            "License(s): CC0-1.0\n",
            "Downloading language-translation-englishfrench.zip to /Users/robinhamers/Documents/DataAnalyst/BECODE/2_DataBootcamp/LGG-Thomas5/PersonalProjects/Robin_Hamers/Transformers/Attention_All_YOU_Need/Encoder_Decoder\n",
            " 85%|████████████████████████████████▍     | 3.00M/3.51M [00:00<00:00, 3.98MB/s]\n",
            "100%|██████████████████████████████████████| 3.51M/3.51M [00:01<00:00, 3.57MB/s]\n",
            "Archive:  language-translation-englishfrench.zip\n",
            "  inflating: eng_-french.csv         \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /Users/robinhamers/Downloads/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d devicharith/language-translation-englishfrench\n",
        "!unzip language-translation-englishfrench.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zmSvT1853lg",
        "outputId": "ecd68805-5fba-42df-f5ca-3df7f3c2abd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  English words/sentences French words/sentences\n",
            "0                     Hi.                 Salut!\n",
            "1                    Run!                Cours !\n",
            "2                    Run!               Courez !\n",
            "3                    Who?                  Qui ?\n",
            "4                    Wow!             Ça alors !\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the CSV file\n",
        "df_import = pd.read_csv('eng_-french.csv')\n",
        "print(df_import.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59L0AnMO66ca",
        "outputId": "0a493604-84c3-4077-c5d5-d86bbdcf5299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  english      french\n",
            "0     Hi.      Salut!\n",
            "1    Run!     Cours !\n",
            "2    Run!    Courez !\n",
            "3    Who?       Qui ?\n",
            "4    Wow!  Ça alors !\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "df['english'] = df_import['English words/sentences']\n",
        "df['french'] = df_import['French words/sentences']\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No need if file already created\n",
        "# Create a combined file with English and French sentences for SentencePiece training\n",
        "with open(\"combined_data.txt\", \"w\") as file:\n",
        "    for e, f in zip(df['english'], df['french']):\n",
        "        file.write(e + '\\n' + f + '\\n')  # Add each English-French sentence pair.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "z91dSmQX7znU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: combined_data.txt\n",
            "  input_format: \n",
            "  model_prefix: spm_model\n",
            "  model_type: BPE\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: combined_data.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 351242 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=12087108\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9523% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999523\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 351242 sentences.\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 351242\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 69178\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=190023 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70879 size=20 all=2212 active=1908 piece=▁n\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44616 size=40 all=2904 active=2600 piece=ar\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27972 size=60 all=3960 active=3656 piece=us\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18915 size=80 all=4890 active=4586 piece=ow\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14554 size=100 all=5702 active=5398 piece=ch\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14503 min_freq=1006\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12591 size=120 all=6503 active=1699 piece=ke\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11262 size=140 all=7327 active=2523 piece=tu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9328 size=160 all=8125 active=3321 piece=ad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8148 size=180 all=9086 active=4282 piece=▁ét\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7065 size=200 all=10023 active=5219 piece=oir\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7004 min_freq=951\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6246 size=220 all=10564 active=1505 piece=ang\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5561 size=240 all=11143 active=2084 piece=▁She\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4876 size=260 all=11916 active=2857 piece=ez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4573 size=280 all=12667 active=3608 piece=▁think\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4155 size=300 all=13240 active=4181 piece=▁him\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4144 min_freq=832\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3772 size=320 all=13773 active=1533 piece=▁us\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3489 size=340 all=14421 active=2181 piece=ate\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3189 size=360 all=14908 active=2668 piece=ind\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3046 size=380 all=15407 active=3167 piece=ten\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2886 size=400 all=15919 active=3679 piece=▁Ce\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2879 min_freq=707\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2756 size=420 all=16158 active=1236 piece=mer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2593 size=440 all=16537 active=1615 piece=▁était\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2459 size=460 all=16909 active=1987 piece=ak\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2311 size=480 all=17464 active=2542 piece=▁too\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2170 size=500 all=17841 active=2919 piece=▁there\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2168 min_freq=630\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2103 size=520 all=18252 active=1411 piece=▁imp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2000 size=540 all=18815 active=1974 piece=ign\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1880 size=560 all=19231 active=2390 piece=▁liv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1798 size=580 all=19540 active=2699 piece=▁somet\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1710 size=600 all=19877 active=3036 piece=▁avons\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1708 min_freq=566\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1644 size=620 all=20345 active=1469 piece=▁something\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1561 size=640 all=20721 active=1845 piece=ême\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1520 size=660 all=21001 active=2125 piece=▁heure\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1463 size=680 all=21272 active=2396 piece=rait\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1424 size=700 all=21543 active=2667 piece=▁thought\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1423 min_freq=479\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1382 size=720 all=21822 active=1354 piece=▁bon\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1317 size=740 all=22044 active=1576 piece=▁told\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1292 size=760 all=22324 active=1856 piece=▁avoir\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1256 size=780 all=22523 active=2055 piece=▁dois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1215 size=800 all=22840 active=2372 piece=▁vie\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1212 min_freq=419\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1171 size=820 all=23020 active=1319 piece=▁mo\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1124 size=840 all=23309 active=1608 piece=▁Who\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1091 size=860 all=23524 active=1823 piece=▁fam\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1072 size=880 all=23672 active=1971 piece=rench\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1048 size=900 all=23905 active=2204 piece=▁No\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1047 min_freq=364\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1020 size=920 all=24168 active=1453 piece=▁rest\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=990 size=940 all=24399 active=1684 piece=▁veu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=966 size=960 all=24598 active=1883 piece=▁child\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=940 size=980 all=24989 active=2274 piece=▁underst\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=908 size=1000 all=25270 active=2555 piece=▁père\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=907 min_freq=326\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=891 size=1020 all=25452 active=1446 piece=esterday\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=874 size=1040 all=25732 active=1726 piece=▁tomorrow\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=846 size=1060 all=26077 active=2071 piece=▁arriv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=830 size=1080 all=26221 active=2215 piece=▁inte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=805 size=1100 all=26503 active=2497 piece=▁end\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=804 min_freq=293\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=778 size=1120 all=26710 active=1516 piece=▁sens\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=761 size=1140 all=26886 active=1692 piece=tement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=748 size=1160 all=27172 active=1978 piece=▁why\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=726 size=1180 all=27454 active=2260 piece=▁hope\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=704 size=1200 all=27652 active=2458 piece=▁mont\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=702 min_freq=267\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=687 size=1220 all=27847 active=1553 piece=▁sûr\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=667 size=1240 all=27956 active=1662 piece=▁idea\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=651 size=1260 all=28082 active=1788 piece=▁St\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=634 size=1280 all=28400 active=2106 piece=ious\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=627 size=1300 all=28543 active=2249 piece=▁bed\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=627 min_freq=246\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=617 size=1320 all=28626 active=1507 piece=▁mother\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=606 size=1340 all=28804 active=1685 piece=▁rain\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=593 size=1360 all=29067 active=1948 piece=igh\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=582 size=1380 all=29308 active=2189 piece=▁heard\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=570 size=1400 all=29422 active=2303 piece=ale\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=570 min_freq=226\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=565 size=1420 all=29597 active=1577 piece=▁accident\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=554 size=1440 all=29742 active=1722 piece=jeun\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=540 size=1460 all=29857 active=1837 piece=▁piè\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=525 size=1480 all=30141 active=2121 piece=river\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=518 size=1500 all=30291 active=2271 piece=▁homme\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=518 min_freq=209\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=506 size=1520 all=30512 active=1736 piece=▁conv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=496 size=1540 all=30682 active=1906 piece=ations\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=483 size=1560 all=31010 active=2234 piece=▁acheté\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=474 size=1580 all=31158 active=2382 piece=reak\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=466 size=1600 all=31503 active=2727 piece=▁letter\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=465 min_freq=190\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=459 size=1620 all=31748 active=1820 piece=▁moins\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=1640 all=31885 active=1957 piece=▁ép\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=445 size=1660 all=32055 active=2127 piece=illez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=435 size=1680 all=32262 active=2334 piece=any\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=428 size=1700 all=32428 active=2500 piece=tiful\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=427 min_freq=179\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=423 size=1720 all=32524 active=1716 piece=▁invit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=414 size=1740 all=32626 active=1818 piece=▁â\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=403 size=1760 all=32768 active=1960 piece=▁ey\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=397 size=1780 all=32817 active=2009 piece=▁tennis\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=391 size=1800 all=32918 active=2110 piece=▁rel\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=391 min_freq=168\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=385 size=1820 all=33027 active=1731 piece=▁books\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=379 size=1840 all=33179 active=1883 piece=▁together\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=372 size=1860 all=33370 active=2074 piece=aught\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=366 size=1880 all=33520 active=2224 piece=▁chat\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=360 size=1900 all=33716 active=2420 piece=oyez\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=360 min_freq=158\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=1920 all=33837 active=1798 piece=▁fini\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=351 size=1940 all=33976 active=1937 piece=▁étais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=345 size=1960 all=34087 active=2048 piece=▁nager\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=339 size=1980 all=34237 active=2198 piece=uv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=336 size=2000 all=34380 active=2341 piece=▁Merci\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=336 min_freq=148\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=333 size=2020 all=34461 active=1800 piece=▁réunion\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=329 size=2040 all=34535 active=1874 piece=▁lorsque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=2060 all=34620 active=1959 piece=▁Voulez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=320 size=2080 all=34682 active=2021 piece=▁fig\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=316 size=2100 all=34716 active=2055 piece=▁men\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=316 min_freq=139\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=312 size=2120 all=34824 active=1829 piece=▁bor\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=307 size=2140 all=34928 active=1933 piece=vision\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=303 size=2160 all=35023 active=2028 piece=▁serais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=2180 all=35204 active=2209 piece=▁supposed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=295 size=2200 all=35312 active=2317 piece=▁regret\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=295 min_freq=131\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=292 size=2220 all=35480 active=1926 piece=▁anybody\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288 size=2240 all=35645 active=2091 piece=oyer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=2260 all=35759 active=2205 piece=▁parlé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=280 size=2280 all=35912 active=2358 piece=▁advice\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=276 size=2300 all=36017 active=2463 piece=vert\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=276 min_freq=124\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=2320 all=36138 active=1900 piece=▁says\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=270 size=2340 all=36258 active=2020 piece=▁App\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=2360 all=36375 active=2137 piece=▁prochaine\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=265 size=2380 all=36530 active=2292 piece=▁lieu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=2400 all=36645 active=2407 piece=▁Tous\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=262 min_freq=117\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=260 size=2420 all=36775 active=1963 piece=▁patient\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=2440 all=36891 active=2079 piece=▁radio\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=254 size=2460 all=36971 active=2159 piece=▁cost\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=251 size=2480 all=37107 active=2295 piece=▁opinion\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=247 size=2500 all=37274 active=2462 piece=fect\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=247 min_freq=111\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=2520 all=37413 active=1989 piece=▁Votre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=242 size=2540 all=37448 active=2024 piece=▁Not\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=239 size=2560 all=37558 active=2134 piece=▁appreciate\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=2580 all=37669 active=2245 piece=▁hos\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=232 size=2600 all=37782 active=2358 piece=▁problèmes\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=231 min_freq=106\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=228 size=2620 all=37848 active=1956 piece=▁rac\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=226 size=2640 all=38031 active=2139 piece=▁lie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=2660 all=38144 active=2252 piece=▁short\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=2680 all=38256 active=2364 piece=▁œ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=218 size=2700 all=38389 active=2497 piece=▁team\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=218 min_freq=101\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=216 size=2720 all=38507 active=2036 piece=▁commencé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=213 size=2740 all=38624 active=2153 piece=▁lose\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=211 size=2760 all=38690 active=2219 piece=apeau\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=209 size=2780 all=38786 active=2315 piece=fois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=207 size=2800 all=38889 active=2418 piece=▁simple\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=207 min_freq=96\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=2820 all=38951 active=2007 piece=▁clothes\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=202 size=2840 all=39096 active=2152 piece=aig\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=2860 all=39183 active=2239 piece=tel\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=198 size=2880 all=39310 active=2366 piece=étud\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=2900 all=39403 active=2459 piece=▁plain\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=197 min_freq=92\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=2920 all=39474 active=2035 piece=ps\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=2940 all=39576 active=2137 piece=lent\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=190 size=2960 all=39751 active=2312 piece=▁Ton\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=2980 all=39857 active=2418 piece=▁lég\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=3000 all=39979 active=2540 piece=▁lucky\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=186 min_freq=87\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=184 size=3020 all=40047 active=2067 piece=hist\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=183 size=3040 all=40199 active=2219 piece=▁nois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=3060 all=40308 active=2328 piece=set\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=3080 all=40416 active=2436 piece=▁choc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=3100 all=40467 active=2487 piece=▁Here\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=179 min_freq=83\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=3120 all=40544 active=2101 piece=▁Everything\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=176 size=3140 all=40712 active=2269 piece=▁ble\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=174 size=3160 all=40870 active=2427 piece=ove\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=3180 all=40959 active=2516 piece=ned\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=3200 all=41062 active=2619 piece=▁rather\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=171 min_freq=80\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=3220 all=41127 active=2119 piece=ieurs\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=3240 all=41195 active=2187 piece=▁earth\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=165 size=3260 all=41315 active=2307 piece=ortes\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=3280 all=41401 active=2393 piece=▁sense\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=162 size=3300 all=41473 active=2465 piece=ayez\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=162 min_freq=76\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=3320 all=41558 active=2156 piece=▁pluie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=160 size=3340 all=41626 active=2224 piece=▁désormais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=3360 all=41698 active=2296 piece=▁manque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=3380 all=41712 active=2310 piece=▁embarrass\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=3400 all=41790 active=2388 piece=▁driv\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=73\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=3420 all=41893 active=2191 piece=mon\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=3440 all=42040 active=2338 piece=lu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=150 size=3460 all=42209 active=2507 piece=▁novel\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=148 size=3480 all=42268 active=2566 piece=▁mad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=147 size=3500 all=42362 active=2660 piece=▁tromp\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=147 min_freq=70\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=3520 all=42454 active=2195 piece=▁dent\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=3540 all=42504 active=2245 piece=▁recher\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=3560 all=42600 active=2341 piece=road\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=3580 all=42643 active=2384 piece=▁volé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=3600 all=42705 active=2446 piece=clock\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=140 min_freq=67\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=3620 all=42733 active=2164 piece=étranger\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=3640 all=42847 active=2278 piece=ived\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=3660 all=42884 active=2315 piece=écu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=3680 all=42951 active=2382 piece=▁danser\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=3700 all=43036 active=2467 piece=▁Put\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=133 min_freq=65\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=3720 all=43123 active=2239 piece=itch\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=131 size=3740 all=43211 active=2327 piece=ement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=3760 all=43302 active=2418 piece=▁ship\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=3780 all=43391 active=2507 piece=▁consc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=3800 all=43450 active=2566 piece=▁Lorsque\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=128 min_freq=62\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=127 size=3820 all=43502 active=2225 piece=▁medicine\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=3840 all=43602 active=2325 piece=ained\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3860 all=43670 active=2393 piece=ator\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3880 all=43763 active=2486 piece=▁college\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=3900 all=43839 active=2562 piece=▁excell\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=123 min_freq=60\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=3920 all=43878 active=2226 piece=▁sudden\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121 size=3940 all=43929 active=2277 piece=▁triste\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=3960 all=43987 active=2335 piece=▁garçons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=3980 all=44026 active=2374 piece=rom\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=4000 all=44110 active=2458 piece=▁ice\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=117 min_freq=58\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=4020 all=44143 active=2236 piece=dge\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=4040 all=44197 active=2290 piece=lax\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=4060 all=44263 active=2356 piece=uellement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=4080 all=44331 active=2424 piece=▁tempête\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=4100 all=44426 active=2519 piece=▁egg\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=112 min_freq=56\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=4120 all=44486 active=2279 piece=so\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=4140 all=44570 active=2363 piece=▁continuer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=4160 all=44705 active=2498 piece=▁volont\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=4180 all=44739 active=2532 piece=come\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=4200 all=44847 active=2640 piece=▁erreurs\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=108 min_freq=54\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=4220 all=44925 active=2321 piece=▁Quels\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=4240 all=44985 active=2381 piece=▁dès\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=4260 all=45029 active=2425 piece=▁anywhere\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=4280 all=45082 active=2478 piece=▁risque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=4300 all=45123 active=2519 piece=▁decide\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=104 min_freq=53\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=4320 all=45189 active=2323 piece=▁mail\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=4340 all=45223 active=2357 piece=adm\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=4360 all=45340 active=2474 piece=▁continue\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=4380 all=45402 active=2536 piece=▁rends\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=4400 all=45478 active=2612 piece=▁avions\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=100 min_freq=51\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=4420 all=45537 active=2333 piece=oie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=4440 all=45571 active=2367 piece=▁superm\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=4460 all=45608 active=2404 piece=▁œil\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=4480 all=45656 active=2452 piece=nez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=4500 all=45751 active=2547 piece=▁herself\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=97 min_freq=49\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=4520 all=45839 active=2376 piece=▁failed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=95 size=4540 all=45940 active=2477 piece=▁censé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=94 size=4560 all=46015 active=2552 piece=▁bird\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=4580 all=46055 active=2592 piece=alif\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=4600 all=46094 active=2631 piece=▁jealous\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=93 min_freq=48\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=4620 all=46152 active=2362 piece=▁seemed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=4640 all=46262 active=2472 piece=▁Didn\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=4660 all=46323 active=2533 piece=▁rom\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=4680 all=46387 active=2597 piece=▁montagne\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=4700 all=46500 active=2710 piece=▁pier\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=89 min_freq=46\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=4720 all=46540 active=2362 piece=apprendre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=4740 all=46674 active=2496 piece=▁prêts\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=4760 all=46718 active=2540 piece=▁loi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=4780 all=46752 active=2574 piece=▁ordered\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=4800 all=46827 active=2649 piece=▁fallu\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=86 min_freq=45\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=4820 all=46838 active=2353 piece=cts\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=4840 all=46907 active=2422 piece=▁project\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=4860 all=47054 active=2569 piece=ousand\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=4880 all=47062 active=2577 piece=▁pén\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=4900 all=47119 active=2634 piece=▁nerveux\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=83 min_freq=44\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=4920 all=47165 active=2402 piece=▁Sont\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=4940 all=47208 active=2445 piece=▁ouverte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=4960 all=47261 active=2498 piece=▁mille\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=4980 all=47340 active=2577 piece=omas\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=5000 all=47393 active=2630 piece=▁Thomas\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=80 min_freq=42\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=5020 all=47392 active=2369 piece=▁explanation\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=5040 all=47418 active=2395 piece=affaire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=5060 all=47493 active=2470 piece=uits\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=5080 all=47555 active=2532 piece=▁church\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=5100 all=47604 active=2581 piece=▁ant\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=41\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=5120 all=47657 active=2421 piece=▁guilty\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=5140 all=47693 active=2457 piece=ctobre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=5160 all=47736 active=2500 piece=ode\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=5180 all=47782 active=2546 piece=▁enemy\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=5200 all=47839 active=2603 piece=▁dom\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=74 min_freq=40\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=5220 all=47880 active=2427 piece=▁serons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5240 all=47925 active=2472 piece=âte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5260 all=48028 active=2575 piece=▁bored\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5280 all=48041 active=2588 piece=▁absolument\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=5300 all=48093 active=2640 piece=▁croit\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=72 min_freq=39\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=5320 all=48137 active=2449 piece=▁prisonn\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=5340 all=48248 active=2560 piece=▁rule\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=5360 all=48273 active=2585 piece=▁accepter\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=5380 all=48333 active=2645 piece=accep\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=5400 all=48368 active=2680 piece=▁cousin\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=70 min_freq=38\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=5420 all=48355 active=2406 piece=▁na\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=5440 all=48401 active=2452 piece=▁pencil\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=5460 all=48430 active=2481 piece=accomp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=5480 all=48463 active=2514 piece=▁lettres\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=5500 all=48499 active=2550 piece=over\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=67 min_freq=37\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=5520 all=48543 active=2451 piece=▁venus\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5540 all=48532 active=2440 piece=ss\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5560 all=48639 active=2547 piece=▁suivi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5580 all=48640 active=2548 piece=▁particip\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=5600 all=48729 active=2637 piece=riger\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=65 min_freq=36\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=5620 all=48749 active=2449 piece=▁échoué\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5640 all=48764 active=2464 piece=hor\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5660 all=48844 active=2544 piece=▁guest\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5680 all=48838 active=2538 piece=▁mistaken\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5700 all=48923 active=2623 piece=▁pil\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=63 min_freq=35\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5720 all=48991 active=2506 piece=▁règle\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5740 all=49012 active=2527 piece=▁expérience\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=5760 all=49131 active=2646 piece=ession\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=5780 all=49183 active=2698 piece=▁temper\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5800 all=49182 active=2697 piece=ix\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=61 min_freq=34\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5820 all=49296 active=2557 piece=▁Voyez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5840 all=49315 active=2576 piece=▁oiseaux\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5860 all=49353 active=2614 piece=chons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5880 all=49393 active=2654 piece=▁soupe\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5900 all=49421 active=2682 piece=embrasser\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=60 min_freq=33\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5920 all=49485 active=2536 piece=umée\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5940 all=49528 active=2579 piece=▁pénét\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5960 all=49537 active=2588 piece=▁hearing\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=5980 all=49575 active=2626 piece=▁dad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=6000 all=49647 active=2698 piece=▁ended\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=58 min_freq=32\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=6020 all=49650 active=2486 piece=▁responsib\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=6040 all=49695 active=2531 piece=▁ours\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=6060 all=49728 active=2564 piece=▁fassiez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6080 all=49748 active=2584 piece=arr\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6100 all=49851 active=2687 piece=▁step\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=56 min_freq=32\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6120 all=49887 active=2526 piece=▁intend\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6140 all=49889 active=2528 piece=▁préoccup\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6160 all=49963 active=2602 piece=▁diss\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6180 all=50027 active=2666 piece=▁distur\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6200 all=50023 active=2662 piece=▁incapable\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=55 min_freq=31\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6220 all=50115 active=2594 piece=▁sof\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6240 all=50161 active=2640 piece=▁hesit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6260 all=50171 active=2650 piece=▁pousser\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6280 all=50234 active=2713 piece=apped\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6300 all=50279 active=2758 piece=▁sépar\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=30\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6320 all=50307 active=2532 piece=▁argument\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6340 all=50392 active=2617 piece=▁wet\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6360 all=50419 active=2644 piece=▁hobby\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6380 all=50417 active=2642 piece=▁attitude\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6400 all=50452 active=2677 piece=assé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=51 min_freq=30\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6420 all=50524 active=2581 piece=▁birds\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6440 all=50520 active=2577 piece=▁punctu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6460 all=50517 active=2574 piece=▁perfectly\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6480 all=50620 active=2677 piece=▁paie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6500 all=50626 active=2683 piece=▁anyway\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=50 min_freq=29\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6520 all=50636 active=2542 piece=▁effrayé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6540 all=50638 active=2544 piece=ok\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6560 all=50803 active=2709 piece=agers\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6580 all=50836 active=2742 piece=▁sales\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6600 all=50857 active=2763 piece=▁répéter\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=49 min_freq=28\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6620 all=50846 active=2532 piece=▁impressionné\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6640 all=50951 active=2637 piece=▁Last\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6660 all=50988 active=2674 piece=▁accus\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6680 all=51018 active=2704 piece=▁England\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6700 all=51012 active=2698 piece=▁traversé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=48 min_freq=28\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6720 all=51107 active=2646 piece=ashed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6740 all=51161 active=2700 piece=▁fixed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6760 all=51152 active=2691 piece=▁suicide\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6780 all=51164 active=2703 piece=lit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6800 all=51242 active=2781 piece=▁fich\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=46 min_freq=27\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6820 all=51281 active=2595 piece=▁miles\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6840 all=51269 active=2583 piece=▁pleasure\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6860 all=51302 active=2616 piece=▁cla\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6880 all=51360 active=2674 piece=▁coupé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6900 all=51366 active=2680 piece=▁trompé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=45 min_freq=27\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6920 all=51360 active=2563 piece=▁proximité\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6940 all=51402 active=2605 piece=▁bud\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6960 all=51439 active=2642 piece=▁wore\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6980 all=51452 active=2655 piece=▁guests\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=7000 all=51445 active=2648 piece=▁treated\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=44 min_freq=26\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7020 all=51438 active=2566 piece=ram\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7040 all=51535 active=2663 piece=riage\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7060 all=51593 active=2721 piece=▁drown\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7080 all=51597 active=2725 piece=▁repair\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7100 all=51587 active=2715 piece=▁exciting\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43 min_freq=25\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7120 all=51606 active=2599 piece=anic\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7140 all=51716 active=2709 piece=▁Tour\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7160 all=51746 active=2739 piece=▁loyer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7180 all=51743 active=2736 piece=▁remain\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7200 all=51741 active=2734 piece=▁plantes\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=42 min_freq=25\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7220 all=51733 active=2580 piece=▁succeeded\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7240 all=51828 active=2675 piece=rise\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7260 all=51903 active=2750 piece=▁mett\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7280 all=51924 active=2771 piece=▁attach\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7300 all=51936 active=2783 piece=▁facture\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=24\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7320 all=51930 active=2590 piece=▁volontaire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7340 all=51992 active=2652 piece=▁Ind\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7360 all=52044 active=2704 piece=érence\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7380 all=52046 active=2706 piece=▁handed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7400 all=52043 active=2703 piece=▁parcour\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=40 min_freq=24\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7420 all=52037 active=2594 piece=mi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7440 all=52194 active=2751 piece=ining\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7460 all=52264 active=2821 piece=▁Jusqu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7480 all=52287 active=2844 piece=▁ruined\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7500 all=52275 active=2832 piece=enseigner\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39 min_freq=23\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7520 all=52263 active=2599 piece=tée\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7540 all=52317 active=2653 piece=▁loan\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7560 all=52366 active=2702 piece=▁forêt\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7580 all=52356 active=2692 piece=▁retire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7600 all=52362 active=2698 piece=▁rentrée\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=23\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7620 all=52374 active=2631 piece=▁continued\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7640 all=52421 active=2678 piece=réh\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7660 all=52495 active=2752 piece=▁bête\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7680 all=52515 active=2772 piece=▁crack\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7700 all=52526 active=2783 piece=▁poches\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=37 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7720 all=52513 active=2614 piece=▁cleaning\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7740 all=52523 active=2624 piece=▁cy\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7760 all=52585 active=2686 piece=▁Time\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7780 all=52623 active=2724 piece=▁round\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7800 all=52630 active=2731 piece=▁member\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7820 all=52631 active=2633 piece=▁teaches\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7840 all=52616 active=2618 piece=▁médicaments\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7860 all=52728 active=2730 piece=anced\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7880 all=52761 active=2763 piece=ricité\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7900 all=52769 active=2771 piece=▁pêche\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=35 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7920 all=52791 active=2659 piece=▁maître\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: spm_model.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_model.vocab\n"
          ]
        }
      ],
      "source": [
        "# Train the SentencePiece model (bpe-based)\n",
        "spm.SentencePieceTrainer.train(input='combined_data.txt', model_prefix='spm_model', vocab_size=vocab_size, model_type='bpe')\n",
        "\n",
        "sp_en = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "sp_fr = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "\n",
        "def tokenize(text, sp_processor):\n",
        "    return sp_processor.encode(text, out_type=str)  # Encode to subword tokens\n",
        "\n",
        "# Tokenize English and French sentences\n",
        "df['english_tokens'] = df['english'].apply(lambda x: tokenize(x, sp_en))\n",
        "df['french_tokens'] = df['french'].apply(lambda x: tokenize(x, sp_fr))\n",
        "\n",
        "def tokens_to_indices(tokens, sp_processor):\n",
        "    return sp_processor.encode(' '.join(tokens), out_type=int)  # Convert to indices\n",
        "\n",
        "df['english_indices'] = df['english_tokens'].apply(lambda x: tokens_to_indices(x, sp_en))\n",
        "df['french_indices'] = df['french_tokens'].apply(lambda x: tokens_to_indices(x, sp_fr))\n",
        "\n",
        "max_len = max(df['english_indices'].apply(len).max(), df['french_indices'].apply(len).max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2huGprHJCA4z",
        "outputId": "cad6e662-d71d-4de0-ae73-12a35c8297ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  english      french english_tokens      french_tokens  \\\n",
            "0     Hi.      Salut!     [▁H, i, .]    [▁S, al, ut, !]   \n",
            "1    Run!     Cours !    [▁R, un, !]     [▁C, ours, ▁!]   \n",
            "2    Run!    Courez !    [▁R, un, !]  [▁C, ou, rez, ▁!]   \n",
            "3    Who?       Qui ?      [▁Who, ?]         [▁Qui, ▁?]   \n",
            "4    Wow!  Ça alors !    [▁W, ow, !]  [▁Ça, ▁alors, ▁!]   \n",
            "\n",
            "           english_indices              french_indices  \n",
            "0  [100, 7636, 7926, 7938]  [118, 303, 7926, 190, 244]  \n",
            "1           [570, 68, 244]             [84, 6042, 244]  \n",
            "2           [570, 68, 244]    [84, 695, 70, 7961, 244]  \n",
            "3                [842, 60]                   [964, 60]  \n",
            "4          [79, 6855, 244]            [652, 2327, 244]  \n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "df = df[:10000]\n",
        "print(df.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvAbHD4_9Dbl",
        "outputId": "88786be9-167f-4a48-9bcc-26ec192991d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max len = 21\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour ajouter du padding\n",
        "def pad_sequence(seq, max_len, sp_model):\n",
        "    pad_id = sp_model.piece_to_id('<pad>')  # Obtenir l'ID du token PAD\n",
        "    return seq + [pad_id] * (max_len - len(seq))  # Ajouter le padding jusqu'à max_len\n",
        "\n",
        "# Calculer la longueur maximale des phrases dans les deux langues\n",
        "max_len = max(\n",
        "    df['english_indices'].apply(len).max(),  # Longueur maximale pour l'anglais\n",
        "    df['french_indices'].apply(len).max()   # Longueur maximale pour le français\n",
        ")\n",
        "\n",
        "# Dataset pour la traduction\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_sentences, french_sentences, sp_en, sp_fr, max_len):\n",
        "        # Tokenisation des phrases en anglais et français\n",
        "        self.english_sentences = [sp_en.encode(sent, out_type=int) for sent in english_sentences]\n",
        "        self.french_sentences = [sp_fr.encode(sent, out_type=int) for sent in french_sentences]\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Padding des séquences\n",
        "        self.english_sentences = [pad_sequence(sent, max_len, sp_en) for sent in self.english_sentences]\n",
        "        self.french_sentences = [pad_sequence(sent, max_len, sp_fr) for sent in self.french_sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.english_sentences[idx]), torch.tensor(self.french_sentences[idx])\n",
        "\n",
        "# Exemple : Charger les phrases depuis ton DataFrame\n",
        "english_sentences = df['english'].tolist()\n",
        "french_sentences = df['french'].tolist()\n",
        "\n",
        "print(f\"max len = {max_len}\")\n",
        "\n",
        "# Créer le dataset\n",
        "dataset = TranslationDataset(english_sentences, french_sentences, sp_en, sp_fr, max_len)\n",
        "\n",
        "# Créer un DataLoader pour charger les données en lots\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "mfDqnTbwthCq",
        "outputId": "05ed56f1-286a-489c-aafd-a731f8b90f61"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention (for encoder/decoder) \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, encoder_out=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Input tensor.\n",
        "            mask: Optional mask for attention.\n",
        "            encoder_out: Optional encoder output for cross-attention.\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape  # Get dimensions of the input tensor\n",
        "        print(f\"x shape = {x.shape}\")\n",
        "        print(f\"x = {x}\")\n",
        "        # If encoder_out is provided, use it for keys and values (cross-attention)\n",
        "        encoder_seq_len = []\n",
        "\n",
        "        if encoder_out is not None:\n",
        "            # Project encoder_out to embed_dim\n",
        "            #print(f\"encoder out = {encoder_out.shape}\")\n",
        "            _, encoder_seq_len, encoder_dim = encoder_out.shape  # Get encoder_out dimensions\n",
        "            #print(f\"encoder dim = {encoder_dim}\")\n",
        "            self.encoder_proj = nn.Linear(encoder_dim, self.embed_dim)  # Initialize encoder_proj\n",
        "            encoder_out = self.encoder_proj(encoder_out)  # Project to embed_dim\n",
        "\n",
        "            # Calculate keys and values from encoder_out\n",
        "            k = self.key(encoder_out)  # (B, T, head_size)\n",
        "            v = self.value(encoder_out)  # (B, T, head_size)\n",
        "        else:  # Otherwise, use x for keys and values (self-attention)\n",
        "            k = self.key(x)  # (B, T, head_size)\n",
        "            v = self.value(x)  # (B, T, head_size)\n",
        "\n",
        "        # Calculate query from input x\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        wei = torch.bmm(q, k.transpose(1, 2)) * (C ** -0.5)  # (B, T, T)\n",
        "        #print(\"Attention weights (wei):\", wei)\n",
        "        # Apply optional padding mask\n",
        "        if mask is not None:\n",
        "            # Reshape mask to match wei's shape for self-attention or cross-attention\n",
        "            # Assuming mask shape is (batch_size, 1, target_sequence_length, source_sequence_length)\n",
        "\n",
        "            # Apply mask to attention scores\n",
        "            if encoder_seq_len:\n",
        "                # If the mask has a dimension of size 256 in the third axis, we need to reshape it\n",
        "                if mask.shape[2] == 256:\n",
        "                    mask = mask[:, :, :max_len-1]  # Slice the mask to have shape [batch_size, seq_len, seq_len]\n",
        "                    \n",
        "                #mask = mask.unsqueeze(1)  # Shape: [batch_size, 1, seq_len]\n",
        "                \n",
        "                mask = mask.expand(-1, encoder_seq_len, encoder_seq_len)\n",
        "            wei = wei.masked_fill(mask == 0, float('-inf'))  # Apply mask\n",
        "            \n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        \n",
        "\n",
        "        # Apply dropout\n",
        "        #wei = self.dropout(wei)\n",
        "\n",
        "        # Expand v to match the sequence length T\n",
        "        v = v.expand(-1, T, -1)  # Expands the second dimension (sequence length)\n",
        "        #print(f\"wei shape = {wei.shape}\")\n",
        "        #print(f\"v shape = {v.shape}\")\n",
        "        \n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "        #print(f\"out shape = {out.shape}\")\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "svscC5PItjMB",
        "outputId": "d160d871-6abf-4ac0-d99d-044a3d7cecc6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head attention mechanism \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, head_size, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size, embed_dim=embed_dim) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, embed_dim)  # Projection layer with correct input/output dimensions\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, encoder_out=None):\n",
        "        # Apply each head to the input and concatenate the results\n",
        "        \n",
        "        out = torch.cat([h(x, mask, encoder_out) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Project the concatenated outputs to the original embedding dimension\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ZuezIDxftliz",
        "outputId": "2eab5ec0-93f7-4808-ac4b-1846c49b7f1f"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non linearity\"\"\"\n",
        "\n",
        "    def __init__(self, embd_dim, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embd_dim, 4*ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*ff_dim, embd_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "M-3gvxbFzo6T",
        "outputId": "ac26e05b-11e1-40ac-acbb-18c74c6c0b59"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder Layer\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # Self-Attention + Add & Norm\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # Feedforward + Add & Norm\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        #print(x.shape)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-ip1MeSvzo3B",
        "outputId": "9571d00a-0202-4b32-ff4c-026047b6c2e5"
      },
      "outputs": [],
      "source": [
        "# Transformer Decoder Layer\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads # Calculate head_size here\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout=dropout)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_out, src_mask=None, tgt_mask=None):\n",
        "\n",
        "        # Self-Attention + Add & Norm\n",
        "        print(\"self att\")\n",
        "        self_attn_out = self.self_attn(x, tgt_mask)\n",
        "        x = self.norm1(x + self_attn_out)\n",
        "        x = self.dropout(x)\n",
        "        # Cross-Attention (Encoder-Decoder)\n",
        "        if src_mask is not None:\n",
        "            src_mask = src_mask.expand(-1, max_len-1, -1)\n",
        "        print(\"cross att\")\n",
        "        cross_attn_out = self.cross_attn(x, encoder_out, src_mask)\n",
        "        x = self.norm2(x + cross_attn_out)\n",
        "        x = self.dropout(x)\n",
        "        # Feedforward + Add & Norm\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-AFhwDJgzo0a",
        "outputId": "32410707-18d6-48e7-ab8c-bca717f8a8a7"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "tcE2faABzoyA",
        "outputId": "cfc55696-8101-43f4-aa52-57b6d4450835"
      },
      "outputs": [],
      "source": [
        "# Transformer Decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "    def forward(self, x, encoder_out, src_mask=None, tgt_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_out, src_mask, tgt_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UWXCZ1lDzovD",
        "outputId": "ef9eb9dc-ab1e-4761-9b45-21bc786e9a18"
      },
      "outputs": [],
      "source": [
        "# Full Transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, ff_dim, dropout=dropout_rate):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "      self.encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "      self.decoder = TransformerDecoder(num_layers, embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "      self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "      # Dropout layer\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src_emb = self.dropout(self.embedding(src))\n",
        "        tgt_emb = self.dropout(self.embedding(tgt))\n",
        "\n",
        "        # Generate masks if not provided\n",
        "        #if src_mask is None:\n",
        "            # Generate mask based on src length of tgt for alignment\n",
        "            #src_mask = self.generate_mask(src[:, :tgt.shape[1]])\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = self.generate_decoder_mask(tgt)\n",
        "\n",
        "        # Removing the sequence length adjustment\n",
        "        #src_mask = src_mask[:, :, :tgt.shape[1]] # Adjust src_mask's sequence length\n",
        "\n",
        "        #src_mask = src_mask.unsqueeze(1)  # Add a dimension for heads (if necessary)\n",
        "        #tgt_mask = tgt_mask.unsqueeze(1)  # Add a dimension for heads (if necessary)\n",
        "\n",
        "        # Align src_emb with target sequence length before passing to encoder\n",
        "        src_emb = src_emb[:, :tgt.shape[1], :]\n",
        "\n",
        "        encoder_out = self.encoder(src_emb, src_mask)\n",
        "        decoder_out = self.decoder(tgt_emb, encoder_out, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc_out(decoder_out)\n",
        "\n",
        "    def generate_mask(self, sequence):\n",
        "      \"\"\"print(\"GENERATE MASK\")\n",
        "      # Get the padding token ID\n",
        "      pad_id = sp_en.piece_to_id('<pad>')\n",
        "      print(f\"pad id = {pad_id}\")\n",
        "      # Create a mask where padding tokens are 0, others are 1\n",
        "      mask = (sequence != pad_id)  # shape: (batch_size, sequence_length)\n",
        "      # Add dimensions for broadcasting (unsqueeze once)\n",
        "      print(\"MASK generate mask\")\n",
        "      print(mask)\n",
        "      mask = mask.unsqueeze(1)  # shape: (batch_size, 1, sequence_length)\n",
        "\n",
        "      return mask.type(torch.float32)  # or torch.float32, depending on your requirements\"\"\"\n",
        "      pass\n",
        "\n",
        "    def generate_decoder_mask(self, tgt):\n",
        "        \"\"\"print(\"GENERATE DECODER MASK\")\n",
        "        # Get the padding token ID\n",
        "        pad_id = sp_en.piece_to_id('<pad>')\n",
        "        # Create a mask where padding tokens are 0, others are 1\n",
        "        mask = (tgt != pad_id) \n",
        "        print(\"MASK PADDING BOOL\")\n",
        "        print(mask)\n",
        "        mask = mask.type(torch.float32)\n",
        "        # shape: (batch_size, sequence_length)\n",
        "        print(\"MASK PADDING\")\n",
        "        print(mask)\n",
        "        # Add dimensions for broadcasting (unsqueeze once)\n",
        "        mask = mask.unsqueeze(1)  # shape: (batch_size, 1, sequence_length)\"\"\"\n",
        "\n",
        "        # Changed the mask type to float32 instead of bool\n",
        "        #mask = mask.type(torch.float32)\n",
        "        # Create a subsequent mask (triangular mask)\n",
        "        seq_len = tgt.size(1)  # Get the target sequence length\n",
        "        subsequent_mask = torch.tril(torch.ones(seq_len, seq_len)).type(torch.float32) # shape: (sequence_length, sequence_length)\n",
        "        # Expand dimensions of subsequent_mask to match the padding mask\n",
        "        print(\"MASK TRIL\")\n",
        "        print(subsequent_mask)\n",
        "        subsequent_mask = subsequent_mask.unsqueeze(0)\n",
        "        # Combine padding mask and subsequent mask\n",
        "        #mask = mask * subsequent_mask\n",
        "        mask = subsequent_mask\n",
        "        print(\"FULL MASK\")\n",
        "        print(mask)\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ywlHKYEEDOG9",
        "outputId": "ab0e238e-706d-4bcd-eb7b-f40a14cfd1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_size = 8000\n",
            "MASK TRIL\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1.]])\n",
            "FULL MASK\n",
            "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 1.]]])\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.0509, -0.1354, -0.0639,  ..., -1.8004,  0.6388,  1.1529],\n",
            "         [ 0.3483, -0.4034,  0.6226,  ..., -0.1731,  0.2962, -1.2488],\n",
            "         [-0.6738, -0.7622,  0.0000,  ...,  1.5384, -1.7544,  0.4979],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-1.1951,  0.1241, -0.1025,  ...,  2.0336,  0.3440,  2.0215],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.0000,  1.6517, -0.0209],\n",
            "         [ 0.0860, -0.9027,  0.9477,  ..., -0.0000, -0.7534, -0.0000],\n",
            "         ...,\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.3775,  0.0000,  0.7804,  ...,  1.9337, -0.0902, -2.0822],\n",
            "         [-0.4769, -0.1862, -0.8745,  ...,  0.0000,  0.8036,  1.4376],\n",
            "         [ 0.1689,  0.4192,  1.6793,  ..., -1.5438,  0.0465, -0.5610],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3775,  2.6115,  0.7804,  ...,  0.0000, -0.0902, -2.0822],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  1.5911,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-2.0328,  1.1927,  0.6978,  ...,  0.6202, -0.0000,  0.4077],\n",
            "         [ 0.4286, -0.8855,  0.8172,  ..., -0.8355,  2.7927, -0.5247],\n",
            "         [ 0.0736,  0.3007,  1.3371,  ...,  1.4378, -0.0000, -1.9376],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 0.2807,  0.3232,  0.8898,  ..., -0.1286, -3.1231,  1.6981],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.7835,  0.1394,  0.5546,  ..., -0.7604,  0.0000,  2.1594],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-2.4026e-01, -1.4849e-01, -6.8077e-01,  ..., -2.1685e+00,\n",
            "           1.0129e+00,  1.7174e+00],\n",
            "         [ 1.5488e-01, -0.0000e+00, -7.2288e-02,  ..., -3.5443e-01,\n",
            "           4.2017e-01, -6.9479e-01],\n",
            "         [-7.6489e-01, -1.3658e+00, -1.1765e+00,  ...,  1.3069e+00,\n",
            "          -1.6418e+00,  1.3913e+00],\n",
            "         ...,\n",
            "         [-2.2643e+00, -2.6600e+00, -4.3154e-02,  ..., -3.7319e-01,\n",
            "           4.1418e-01,  2.4866e+00],\n",
            "         [-0.0000e+00, -2.5546e+00, -8.9351e-01,  ..., -3.1460e-01,\n",
            "           2.3122e-01,  1.8990e+00],\n",
            "         [-2.3432e+00, -2.6058e+00, -8.6660e-01,  ..., -3.7346e-01,\n",
            "          -1.8444e-01,  2.5528e+00]],\n",
            "\n",
            "        [[-1.1578e+00, -1.8412e-01, -1.0805e+00,  ...,  2.3167e+00,\n",
            "           9.4436e-01,  0.0000e+00],\n",
            "         [-1.9015e-01, -1.4489e+00, -0.0000e+00,  ..., -1.1650e-01,\n",
            "           2.1699e+00,  1.0511e+00],\n",
            "         [ 0.0000e+00, -1.4558e+00,  2.0141e-01,  ...,  6.2664e-02,\n",
            "          -5.5552e-01,  4.1878e-01],\n",
            "         ...,\n",
            "         [-2.1813e+00, -5.0953e-01, -4.8734e-01,  ..., -4.7370e-02,\n",
            "           4.3723e-01,  2.5003e+00],\n",
            "         [-2.3849e+00, -2.7179e-01, -0.0000e+00,  ..., -1.0830e-01,\n",
            "           3.8245e-01,  2.4110e+00],\n",
            "         [-2.2371e+00, -2.5587e+00, -0.0000e+00,  ..., -3.4031e-02,\n",
            "           0.0000e+00,  2.2978e+00]],\n",
            "\n",
            "        [[ 3.5531e-01, -2.2416e-01,  4.3880e-01,  ...,  1.1912e+00,\n",
            "           0.0000e+00, -1.1586e+00],\n",
            "         [-7.1341e-01, -5.4957e-01, -1.7040e+00,  ..., -9.3755e-02,\n",
            "           0.0000e+00,  2.1385e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  1.2452e+00,  ..., -1.6353e+00,\n",
            "          -1.2397e-01,  3.7701e-01],\n",
            "         ...,\n",
            "         [-2.2954e+00, -2.5960e+00, -7.0214e-01,  ..., -2.9238e-01,\n",
            "           0.0000e+00,  2.2602e+00],\n",
            "         [-1.8582e-01, -2.5827e+00, -6.8349e-01,  ..., -0.0000e+00,\n",
            "           4.7739e-01,  0.0000e+00],\n",
            "         [-2.3218e+00, -2.6241e+00, -4.3947e-01,  ..., -1.9594e-01,\n",
            "           3.4563e-01,  2.3781e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  2.3178e+00, -1.5451e-02,  ..., -4.1229e-01,\n",
            "          -9.5405e-02, -1.5092e+00],\n",
            "         [-2.2050e-01, -1.4941e+00, -1.8969e+00,  ..., -7.7312e-02,\n",
            "           1.8886e+00,  0.0000e+00],\n",
            "         [-1.7346e+00, -8.7527e-02, -3.2598e-01,  ..., -0.0000e+00,\n",
            "           1.3422e+00,  2.2450e+00],\n",
            "         ...,\n",
            "         [-2.1546e+00, -2.2048e+00, -6.2589e-01,  ..., -0.0000e+00,\n",
            "          -3.4805e-01,  2.3544e+00],\n",
            "         [-2.1947e+00, -2.1435e+00, -4.5397e-01,  ..., -4.5787e-01,\n",
            "           3.1530e-02,  2.3896e+00],\n",
            "         [-2.1364e+00, -2.6083e+00, -4.7651e-01,  ..., -2.6280e-01,\n",
            "           6.9326e-01,  2.0712e+00]],\n",
            "\n",
            "        [[-2.1366e+00,  9.3743e-01, -1.9589e-01,  ...,  3.1722e-01,\n",
            "           1.3747e-01,  1.1179e+00],\n",
            "         [ 8.8087e-01, -9.9547e-01, -2.4625e-01,  ..., -0.0000e+00,\n",
            "           0.0000e+00, -4.4354e-01],\n",
            "         [ 5.4654e-03,  6.5716e-02,  6.1967e-01,  ...,  1.4528e+00,\n",
            "           2.5708e-01, -1.7124e+00],\n",
            "         ...,\n",
            "         [-2.0906e+00, -2.6920e+00, -4.2303e-01,  ..., -3.0418e-01,\n",
            "           4.0972e-01,  0.0000e+00],\n",
            "         [-2.2446e+00, -2.4780e+00, -5.1356e-01,  ..., -5.9460e-02,\n",
            "           5.5354e-01,  2.6195e+00],\n",
            "         [-0.0000e+00, -2.6883e+00, -7.0120e-01,  ..., -3.9731e-01,\n",
            "           3.8158e-01,  2.3940e+00]],\n",
            "\n",
            "        [[ 1.1883e-02, -1.6485e-01,  5.9628e-01,  ...,  1.3708e-01,\n",
            "          -3.3680e+00,  0.0000e+00],\n",
            "         [-2.2840e-01, -1.5697e+00, -2.2058e+00,  ...,  0.0000e+00,\n",
            "           2.1916e+00,  8.8722e-01],\n",
            "         [-1.7339e+00, -2.6772e-01, -2.5588e-01,  ..., -5.8028e-01,\n",
            "          -3.0496e-03,  2.3368e+00],\n",
            "         ...,\n",
            "         [-2.1226e+00, -2.6974e+00, -9.0279e-01,  ..., -1.0518e-01,\n",
            "           3.7084e-01,  0.0000e+00],\n",
            "         [-2.3247e+00, -7.5438e-01, -6.8755e-01,  ..., -5.2492e-02,\n",
            "           3.4346e-01,  2.2403e+00],\n",
            "         [-2.1072e+00, -2.6452e+00, -0.0000e+00,  ..., -9.8560e-02,\n",
            "           5.0964e-01,  2.6617e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[ 0.2881,  0.0000, -1.1158,  ..., -1.8882,  0.7109,  2.1549],\n",
            "         [ 0.0000,  0.2300,  0.1907,  ..., -0.2696,  0.1401, -0.3683],\n",
            "         [-0.1631, -1.6818, -1.5677,  ...,  1.7976, -1.5959,  1.6947],\n",
            "         ...,\n",
            "         [-1.9643, -2.4516, -0.5513,  ...,  0.1708, -0.0247,  3.2626],\n",
            "         [ 0.4040, -2.3107, -1.3563,  ...,  0.0642,  0.1090,  2.5294],\n",
            "         [-1.8602, -2.1881, -0.0000,  ...,  0.4015, -0.2596,  3.0348]],\n",
            "\n",
            "        [[-0.6469, -0.0000, -0.8948,  ...,  2.7651,  0.0000,  0.1562],\n",
            "         [-0.0881, -1.1771,  0.2843,  ..., -0.1861,  0.0000,  1.2909],\n",
            "         [ 0.1747, -0.0000, -0.4399,  ...,  0.4313, -0.8334,  0.3867],\n",
            "         ...,\n",
            "         [-1.8425, -0.4131, -0.9142,  ...,  0.2067,  0.1299,  2.6424],\n",
            "         [-2.3012, -0.0365, -0.2893,  ...,  0.1397,  0.1731,  2.3988],\n",
            "         [-0.0000, -2.3326, -0.3276,  ...,  0.6347, -0.1987,  2.4169]],\n",
            "\n",
            "        [[ 0.4271,  0.3260,  0.7900,  ...,  1.5180, -0.2376, -0.0000],\n",
            "         [-0.4395, -0.4162, -1.8508,  ...,  0.1458, -0.2993,  2.0453],\n",
            "         [ 0.3739, -0.0000,  1.6677,  ..., -1.3626, -0.2277,  0.9704],\n",
            "         ...,\n",
            "         [-2.1140, -2.4585, -0.7284,  ...,  0.0000, -0.2125,  2.7085],\n",
            "         [-0.1570, -2.4576, -0.9807,  ...,  0.4465, -0.0415,  0.1721],\n",
            "         [-2.1941, -2.4402, -0.7880,  ...,  0.1965, -0.0958,  2.7633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0343,  0.0000,  0.2536,  ..., -0.1151, -0.5402, -1.1899],\n",
            "         [-0.0920, -1.0616, -1.6306,  ...,  0.0000,  1.4717,  0.3566],\n",
            "         [-1.8958,  0.0586,  0.1295,  ...,  0.6282,  0.9104,  2.2534],\n",
            "         ...,\n",
            "         [-1.8587, -1.8789, -0.7843,  ...,  0.3368, -0.8054,  2.4371],\n",
            "         [-0.0000, -2.1110, -0.7406,  ..., -0.0146,  0.1920,  2.5919],\n",
            "         [-2.1609, -2.3566, -0.7810,  ...,  0.0000,  0.2962,  2.1274]],\n",
            "\n",
            "        [[-2.2072,  1.1410,  0.1808,  ...,  0.7798, -0.1510,  1.5036],\n",
            "         [ 0.8831, -0.6359,  0.0145,  ...,  0.7758,  0.1973,  0.0942],\n",
            "         [ 0.3540,  0.0000,  0.8396,  ...,  1.7524,  0.1709, -1.2678],\n",
            "         ...,\n",
            "         [-1.8139, -2.5811, -0.6179,  ...,  0.0390, -0.0000,  0.4305],\n",
            "         [-2.0406, -2.0220, -0.6164,  ...,  0.0000,  0.0000,  3.0402],\n",
            "         [ 0.3118, -0.0000, -0.9264,  ...,  0.1731,  0.2328,  2.8997]],\n",
            "\n",
            "        [[ 0.1431, -0.4908,  0.1843,  ...,  0.0536, -3.6146,  0.2840],\n",
            "         [-0.0470, -1.3598, -2.0325,  ...,  0.4162,  0.0000,  1.5082],\n",
            "         [-1.6006, -0.0412,  0.0909,  ..., -0.0541, -0.1559,  2.7505],\n",
            "         ...,\n",
            "         [-1.7154, -2.6072, -1.2104,  ...,  0.5734, -0.0908,  0.2096],\n",
            "         [-1.9902, -0.0000, -1.1209,  ...,  0.5661, -0.1263,  2.4723],\n",
            "         [-1.6248, -2.5289, -0.2711,  ...,  0.7162,  0.1187,  0.5535]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-4.3107e-01,  1.4896e-01, -1.0334e+00,  ..., -1.7749e+00,\n",
            "           1.0160e+00,  2.1248e+00],\n",
            "         [-7.6268e-01,  2.8573e-01,  8.3046e-02,  ...,  4.0711e-02,\n",
            "          -1.9102e-01, -1.7360e-01],\n",
            "         [-1.4896e+00, -1.7237e+00, -2.1163e+00,  ...,  1.7530e+00,\n",
            "          -1.3468e+00,  1.4627e+00],\n",
            "         ...,\n",
            "         [-2.8796e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0330e-01,\n",
            "          -3.6332e-01,  2.7350e+00],\n",
            "         [-6.3519e-01, -1.9536e+00, -1.2557e+00,  ...,  4.0172e-01,\n",
            "           1.4372e-01,  2.1665e+00],\n",
            "         [-2.5368e+00, -1.9959e+00, -1.1258e-01,  ...,  7.6741e-01,\n",
            "          -8.0506e-01,  2.7110e+00]],\n",
            "\n",
            "        [[-1.1448e+00, -4.1590e-01, -8.7760e-01,  ...,  3.0183e+00,\n",
            "           4.2617e-01, -5.2920e-02],\n",
            "         [-8.1747e-01, -1.5611e+00,  4.3020e-01,  ..., -0.0000e+00,\n",
            "           2.9090e-01,  0.0000e+00],\n",
            "         [-0.0000e+00, -1.2770e-01, -7.9600e-01,  ...,  5.3409e-01,\n",
            "          -7.6579e-01,  5.9976e-01],\n",
            "         ...,\n",
            "         [-2.3823e+00, -4.9447e-01, -1.0144e+00,  ...,  4.8481e-01,\n",
            "          -1.5673e-02,  2.3816e+00],\n",
            "         [-0.0000e+00, -8.0709e-02, -3.8433e-01,  ...,  3.7513e-01,\n",
            "           2.5718e-02,  2.1769e+00],\n",
            "         [-7.3478e-01, -2.2044e+00, -3.0543e-01,  ...,  8.0933e-01,\n",
            "          -2.2094e-01,  2.0333e+00]],\n",
            "\n",
            "        [[-8.6547e-01,  1.1308e-01,  6.2568e-01,  ...,  1.7033e+00,\n",
            "          -3.0288e-01, -5.7374e-02],\n",
            "         [-1.3358e+00, -6.2006e-01, -2.1746e+00,  ...,  4.1249e-02,\n",
            "          -5.8819e-03,  1.9251e+00],\n",
            "         [-6.1462e-01,  5.2686e-02,  1.2406e+00,  ..., -1.3355e+00,\n",
            "          -1.1830e-01,  1.2355e+00],\n",
            "         ...,\n",
            "         [-3.0330e+00, -2.4507e+00, -6.1593e-01,  ...,  2.2074e-01,\n",
            "          -3.9808e-01,  2.5038e+00],\n",
            "         [-9.1510e-01, -2.1945e+00, -8.2877e-01,  ...,  4.0619e-01,\n",
            "          -1.4962e-01,  1.2214e-01],\n",
            "         [-2.8793e+00, -2.3734e+00, -0.0000e+00,  ...,  4.4804e-01,\n",
            "           7.5141e-02,  2.8157e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.7559e-01, -1.8383e-01,  1.8995e-01,  ...,  3.0039e-01,\n",
            "          -5.4056e-01, -1.1881e+00],\n",
            "         [-1.1331e+00, -1.0867e+00, -1.9404e+00,  ...,  1.3421e-01,\n",
            "           2.1214e+00, -6.2623e-02],\n",
            "         [-2.5973e+00,  5.5993e-02, -4.9973e-02,  ...,  8.4071e-01,\n",
            "           1.0524e+00,  2.4102e+00],\n",
            "         ...,\n",
            "         [-2.4626e+00, -1.7373e+00, -0.0000e+00,  ...,  5.9194e-01,\n",
            "          -1.1889e+00,  1.8908e+00],\n",
            "         [-0.0000e+00, -1.9606e+00, -9.7491e-01,  ...,  2.8936e-01,\n",
            "          -0.0000e+00,  2.7497e+00],\n",
            "         [-0.0000e+00, -2.3765e+00, -8.3352e-01,  ...,  2.0156e-01,\n",
            "           3.9495e-01,  1.5949e+00]],\n",
            "\n",
            "        [[-3.0894e+00,  1.0100e+00, -1.7597e-01,  ...,  8.9187e-01,\n",
            "          -4.0464e-01,  1.0459e+00],\n",
            "         [ 1.2866e-01, -8.2854e-01, -2.2014e-01,  ...,  8.0272e-01,\n",
            "           3.8461e-01, -1.8634e-01],\n",
            "         [-3.9569e-01, -1.7770e-03,  5.2012e-01,  ...,  1.9801e+00,\n",
            "           3.3619e-01, -1.1441e+00],\n",
            "         ...,\n",
            "         [-2.0756e+00, -2.9472e+00, -0.0000e+00,  ...,  7.7006e-02,\n",
            "          -1.4546e-01,  3.5050e-01],\n",
            "         [-2.9295e+00, -1.8261e+00, -6.6380e-01,  ..., -1.0741e-01,\n",
            "          -4.6567e-02,  2.7361e+00],\n",
            "         [-4.9535e-01, -0.0000e+00, -7.8250e-01,  ...,  2.4027e-01,\n",
            "          -1.7017e-02,  2.5918e+00]],\n",
            "\n",
            "        [[-9.5389e-01, -4.8836e-01, -2.4142e-01,  ...,  5.2569e-01,\n",
            "          -3.6763e+00,  5.7975e-01],\n",
            "         [-6.9685e-01, -1.6277e+00, -1.8309e+00,  ...,  4.3699e-01,\n",
            "           3.9426e-01,  1.2088e+00],\n",
            "         [-2.7730e+00, -1.3353e-01,  8.5660e-02,  ...,  0.0000e+00,\n",
            "          -2.2206e-01,  2.7150e+00],\n",
            "         ...,\n",
            "         [-2.6852e+00, -2.5053e+00, -1.1694e+00,  ...,  7.1203e-01,\n",
            "          -6.8621e-01,  9.6810e-02],\n",
            "         [-2.5945e+00, -2.9337e-02, -1.0409e+00,  ...,  9.2365e-01,\n",
            "          -3.5976e-01,  2.3890e+00],\n",
            "         [-2.2629e+00, -2.5746e+00, -1.5739e-01,  ...,  7.3937e-01,\n",
            "           1.0003e-01,  2.6346e-01]]], grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4569,  0.2821, -1.1907,  ..., -2.1307,  0.9041,  1.9517],\n",
            "         [-0.3038,  0.5278, -0.2020,  ...,  0.5205,  0.0270, -0.2997],\n",
            "         [-1.5464, -1.8520, -2.6556,  ...,  1.5710, -1.0977,  1.2488],\n",
            "         ...,\n",
            "         [-0.0000, -0.2108, -0.3345,  ...,  0.5934, -0.5079,  2.5475],\n",
            "         [-0.0000, -2.3729, -1.8453,  ...,  0.8707,  0.1132,  2.1359],\n",
            "         [-2.3204, -2.3680, -0.4164,  ...,  1.3396, -0.7216,  2.5143]],\n",
            "\n",
            "        [[-1.1634, -0.4485, -0.6303,  ...,  3.3201,  0.1828, -0.0000],\n",
            "         [-0.8411, -2.1193,  0.5170,  ...,  0.1336,  0.3424, -0.0000],\n",
            "         [-0.0110, -0.1311, -1.2303,  ...,  0.8077, -0.5880,  0.4934],\n",
            "         ...,\n",
            "         [-2.4057, -0.8804, -0.9166,  ...,  0.8541,  0.0187,  2.0105],\n",
            "         [-0.0341, -0.3814, -0.6740,  ...,  0.2401, -0.2991,  1.9095],\n",
            "         [-0.7188, -2.5691, -0.3458,  ...,  1.4724, -0.2523,  1.9942]],\n",
            "\n",
            "        [[-0.6568, -0.1439,  0.6278,  ...,  1.8913, -0.6087,  0.1269],\n",
            "         [-1.4115, -1.1894, -2.4702,  ...,  0.4212, -0.3534,  1.6494],\n",
            "         [-0.6322, -0.2409,  1.2610,  ..., -1.1434, -0.3009,  1.2974],\n",
            "         ...,\n",
            "         [-3.3715, -2.5780, -0.9076,  ...,  0.6351, -0.7394,  2.6192],\n",
            "         [-1.2245, -0.0000, -0.9835,  ...,  0.0000, -0.4383, -0.0037],\n",
            "         [-2.7164, -2.7424, -0.3486,  ...,  0.0000, -0.1630,  2.7734]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8177, -0.4309, -0.0000,  ...,  0.5007, -0.4061, -1.0213],\n",
            "         [-0.0000, -1.2788, -2.0338,  ..., -0.0655,  2.3387, -0.3004],\n",
            "         [-2.6067, -0.1897, -0.3800,  ...,  1.1081,  1.0434,  2.4928],\n",
            "         ...,\n",
            "         [-2.4486, -1.8441, -0.5280,  ...,  0.8370, -1.2119,  1.8954],\n",
            "         [ 0.1540, -2.2749, -1.3187,  ...,  0.7981, -0.2494,  2.6273],\n",
            "         [ 0.4112, -2.7837, -1.2327,  ...,  0.6256,  0.4432,  0.0000]],\n",
            "\n",
            "        [[-3.0845,  0.7243, -0.3545,  ...,  1.2617, -0.1584,  0.9294],\n",
            "         [ 0.2127, -0.0000, -0.3601,  ...,  1.1756,  0.2508, -0.1718],\n",
            "         [-0.0000, -0.3408,  0.0000,  ...,  2.3196,  0.5800, -1.8565],\n",
            "         ...,\n",
            "         [-2.2769, -3.2151, -0.0000,  ...,  0.4678, -0.1715,  0.0926],\n",
            "         [-2.8829, -1.8830, -0.0000,  ...,  0.0042, -0.1906,  2.4817],\n",
            "         [-0.0000,  0.1411, -1.0498,  ...,  0.8882,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.2660, -0.7754, -0.5211,  ...,  0.7347, -3.6205,  0.5077],\n",
            "         [-0.8882, -1.9143, -2.0255,  ...,  0.6304,  0.2795,  0.9116],\n",
            "         [-2.2359, -0.1687, -0.4558,  ...,  0.0593, -0.0000,  2.6876],\n",
            "         ...,\n",
            "         [-2.4880, -2.9732, -1.4239,  ...,  1.2759, -0.8374,  0.1177],\n",
            "         [-0.0000, -0.2314, -1.0294,  ...,  1.5073, -0.5674,  2.2043],\n",
            "         [-2.0809, -3.1721, -0.3198,  ...,  1.3986,  0.2025,  0.2325]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7884, -0.0062, -1.1998,  ..., -2.3616,  1.1823,  2.2056],\n",
            "         [-0.7824,  0.5773,  0.1883,  ...,  0.4477, -0.2102,  0.0231],\n",
            "         [-1.6722, -1.8533, -0.0000,  ...,  1.8040, -1.9164,  1.6577],\n",
            "         ...,\n",
            "         [-0.7416, -0.1518, -0.4282,  ...,  0.6689, -0.7426,  0.0000],\n",
            "         [-0.3883, -2.2842, -1.9110,  ...,  1.0069, -0.0569,  2.5313],\n",
            "         [-0.0000, -1.7016, -0.4578,  ...,  1.3172, -0.7891,  2.8685]],\n",
            "\n",
            "        [[-1.6478, -0.3293, -0.6197,  ...,  2.9240, -0.2774,  0.1129],\n",
            "         [-1.0041, -2.2112,  0.7314,  ...,  0.1321, -0.0000,  0.0000],\n",
            "         [-0.0903, -0.3714, -0.8821,  ...,  1.0239, -0.6520,  0.9266],\n",
            "         ...,\n",
            "         [-3.0077, -0.3109, -1.1177,  ...,  1.0015, -0.2874,  2.6759],\n",
            "         [-0.2727, -0.0000, -0.8283,  ...,  0.5289, -0.5204,  0.0000],\n",
            "         [-1.1340, -2.3843, -0.0000,  ...,  1.6195, -0.5255,  0.0000]],\n",
            "\n",
            "        [[-1.0053, -0.2848,  0.9337,  ...,  2.4329, -0.5325,  0.5106],\n",
            "         [-1.8039, -1.0606, -0.0000,  ...,  0.7886, -0.8651,  1.6131],\n",
            "         [-0.6967,  0.1040,  1.2139,  ..., -0.0000, -0.3363,  1.2275],\n",
            "         ...,\n",
            "         [-3.5234, -1.9006, -1.0966,  ...,  0.6756, -0.0000,  2.7654],\n",
            "         [-1.1227,  0.0000, -1.2952,  ...,  0.0452, -0.5129,  0.6526],\n",
            "         [-0.0000, -2.6060, -0.6073,  ...,  0.4395, -0.2656,  2.6834]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8431, -0.4994,  0.2697,  ...,  1.1893, -0.4855, -0.9916],\n",
            "         [-0.3065, -1.3794, -1.8546,  ..., -0.1610,  2.3006, -0.4151],\n",
            "         [-3.4677, -0.0000, -0.4308,  ...,  1.4712,  0.7266,  2.3628],\n",
            "         ...,\n",
            "         [-0.0000, -1.4340, -0.6013,  ...,  1.0385, -0.0000,  2.3190],\n",
            "         [-0.2290, -1.8099, -0.0000,  ...,  0.0000, -0.5306,  3.3576],\n",
            "         [-0.0728, -2.8654, -1.3908,  ...,  0.4581,  0.2438,  0.6773]],\n",
            "\n",
            "        [[-3.3075,  0.6424,  0.1239,  ...,  1.3350, -0.2711,  1.1331],\n",
            "         [-0.0595, -0.3513, -0.3354,  ...,  1.1121, -0.1298, -0.1924],\n",
            "         [-0.5349, -0.2572,  0.3230,  ...,  2.1987,  0.5001, -1.7171],\n",
            "         ...,\n",
            "         [-2.6956, -0.0000,  0.0050,  ...,  0.4718, -0.8456,  0.0000],\n",
            "         [-3.0739, -1.8450,  0.0734,  ...,  0.0405, -0.5102,  2.9366],\n",
            "         [-0.3749,  0.4244, -1.3147,  ...,  0.7835, -0.2448,  0.5551]],\n",
            "\n",
            "        [[-1.6387, -0.7146, -0.6005,  ...,  0.4520, -3.5188,  0.7514],\n",
            "         [-1.2427, -1.8327, -1.8045,  ...,  0.7194, -0.1504,  0.8347],\n",
            "         [-2.8409, -0.0000, -0.5429,  ...,  0.2591, -0.1726,  2.7541],\n",
            "         ...,\n",
            "         [-3.0576, -2.6993, -1.7864,  ...,  1.3782, -1.0810,  0.5144],\n",
            "         [-0.3782,  0.0060, -1.1718,  ...,  1.6561, -1.1652,  2.7577],\n",
            "         [-0.0000, -2.6537, -0.5338,  ...,  1.4375, -0.1379,  0.8912]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "self att\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-1.0273e-01, -1.8016e-01,  4.1263e-02,  ..., -4.2398e-02,\n",
            "          -1.1806e-01, -2.8944e-02],\n",
            "         [-4.4866e-02, -2.1699e-01,  7.7144e-02,  ...,  1.9452e-01,\n",
            "           1.8269e-01,  2.8214e-01],\n",
            "         [ 1.5605e-01, -2.6544e-01,  5.3564e-02,  ...,  1.4171e-01,\n",
            "           1.2147e-01,  6.7075e-02],\n",
            "         ...,\n",
            "         [ 6.6354e-03, -1.4730e-02,  9.7167e-02,  ...,  1.1710e-01,\n",
            "           6.0332e-02,  1.0275e-01],\n",
            "         [ 4.8414e-02,  4.9750e-03,  1.3065e-01,  ...,  1.3249e-01,\n",
            "           6.1333e-02,  1.0271e-01],\n",
            "         [ 7.6745e-03,  3.5684e-02,  1.6064e-01,  ...,  1.1400e-01,\n",
            "           4.7840e-02,  6.0026e-02]],\n",
            "\n",
            "        [[-1.9517e-01, -1.0060e-01, -1.0869e-01,  ...,  2.8069e-01,\n",
            "           2.3384e-01,  2.2913e-01],\n",
            "         [-1.7759e-01,  1.7458e-01,  7.2544e-02,  ..., -8.6130e-03,\n",
            "           2.2313e-02,  1.4919e-01],\n",
            "         [-5.3496e-02,  4.4484e-02, -2.2026e-01,  ...,  8.1706e-02,\n",
            "           1.7460e-02,  4.7131e-02],\n",
            "         ...,\n",
            "         [ 1.4894e-01, -2.8325e-03,  2.2800e-01,  ...,  1.8389e-02,\n",
            "           4.5514e-02,  1.1218e-01],\n",
            "         [ 1.4816e-01,  1.2113e-01,  1.2615e-01,  ...,  1.0245e-02,\n",
            "          -9.9557e-03,  1.0820e-01],\n",
            "         [ 8.7196e-02,  5.9157e-02,  1.7203e-01,  ...,  8.8572e-03,\n",
            "           2.2271e-02,  1.0325e-01]],\n",
            "\n",
            "        [[-5.3522e-02,  8.1838e-02, -4.2453e-05,  ...,  9.0446e-03,\n",
            "           4.1111e-02,  6.8215e-02],\n",
            "         [ 1.0330e-01, -6.4056e-02,  5.0816e-03,  ..., -3.3248e-01,\n",
            "          -3.6790e-01, -3.7720e-01],\n",
            "         [-1.1215e-01,  3.0792e-01,  2.4159e-01,  ...,  5.5842e-02,\n",
            "           1.8594e-01,  1.5071e-01],\n",
            "         ...,\n",
            "         [-5.1727e-02, -3.7422e-02, -1.2053e-01,  ...,  1.2878e-01,\n",
            "           8.0105e-02,  1.0566e-01],\n",
            "         [-2.6609e-02, -6.1880e-02, -6.1590e-02,  ...,  1.8286e-02,\n",
            "          -1.9608e-02, -3.3714e-02],\n",
            "         [ 2.3057e-02, -1.1873e-02, -5.8141e-02,  ...,  3.7615e-02,\n",
            "          -1.0772e-02, -7.4447e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.7017e-02, -1.7651e-01,  2.0149e-01,  ..., -1.4206e-02,\n",
            "           6.5873e-02,  1.1275e-01],\n",
            "         [-8.8595e-02, -5.0888e-02,  1.5391e-01,  ..., -8.6943e-02,\n",
            "          -1.4272e-01, -7.5601e-02],\n",
            "         [ 1.4220e-01,  1.0988e-01, -5.4538e-02,  ...,  2.8680e-02,\n",
            "           1.2275e-02,  1.1665e-01],\n",
            "         ...,\n",
            "         [-1.3232e-03, -7.2842e-03, -3.5550e-02,  ...,  7.9466e-02,\n",
            "           1.5769e-01,  3.4520e-02],\n",
            "         [-2.0578e-02, -9.5776e-02, -1.7645e-01,  ...,  4.3895e-02,\n",
            "           1.0942e-01,  8.6158e-03],\n",
            "         [ 5.9066e-03, -1.0338e-01, -5.4490e-02,  ...,  1.7431e-01,\n",
            "           2.6823e-01,  1.3306e-01]],\n",
            "\n",
            "        [[ 3.8503e-02, -1.3655e-01,  5.2535e-03,  ...,  2.2360e-01,\n",
            "           2.4409e-01,  1.6625e-01],\n",
            "         [ 7.0257e-02, -7.2737e-02,  1.1492e-01,  ...,  2.9046e-01,\n",
            "           3.4197e-01,  3.7248e-01],\n",
            "         [ 1.6768e-01,  7.4417e-02, -3.7056e-02,  ..., -1.4764e-01,\n",
            "          -1.2653e-01, -7.7909e-02],\n",
            "         ...,\n",
            "         [-3.3707e-01, -4.0507e-02,  5.2215e-02,  ...,  5.1778e-02,\n",
            "           4.0116e-02,  3.7597e-02],\n",
            "         [-2.8505e-01, -2.3471e-03,  3.3057e-02,  ...,  1.0763e-01,\n",
            "           1.1586e-01,  1.1301e-01],\n",
            "         [-2.3102e-01,  3.1185e-02,  6.7803e-02,  ...,  1.5279e-01,\n",
            "           1.7614e-01,  1.6032e-01]],\n",
            "\n",
            "        [[-1.1007e-01,  1.7919e-02,  5.6239e-02,  ...,  4.8377e-02,\n",
            "           2.7792e-02,  3.5107e-02],\n",
            "         [ 3.0790e-01,  2.3519e-01, -1.8733e-01,  ...,  9.4015e-02,\n",
            "           1.7112e-01,  1.5015e-01],\n",
            "         [ 2.6230e-02,  9.0698e-03,  2.0141e-01,  ..., -1.3475e-01,\n",
            "          -6.0152e-02, -9.9874e-02],\n",
            "         ...,\n",
            "         [ 6.5881e-02,  5.2292e-02,  1.3026e-01,  ...,  7.1225e-02,\n",
            "           1.2391e-01,  5.2694e-02],\n",
            "         [ 2.4097e-01,  5.1596e-02,  1.8919e-01,  ...,  1.5175e-01,\n",
            "           1.9808e-01,  1.3681e-01],\n",
            "         [ 6.6373e-02,  8.2127e-03,  1.8274e-01,  ...,  6.0833e-02,\n",
            "           1.3504e-01,  7.0471e-02]]], grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[-0.1027,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0449, -0.2170,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1561, -0.2654,  0.0536,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0066, -0.0147,  0.0972,  ...,  0.1171,    -inf,    -inf],\n",
            "         [ 0.0484,  0.0050,  0.1306,  ...,  0.1325,  0.0613,    -inf],\n",
            "         [ 0.0077,  0.0357,  0.1606,  ...,  0.1140,  0.0478,  0.0600]],\n",
            "\n",
            "        [[-0.1952,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1776,  0.1746,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0535,  0.0445, -0.2203,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1489, -0.0028,  0.2280,  ...,  0.0184,    -inf,    -inf],\n",
            "         [ 0.1482,  0.1211,  0.1262,  ...,  0.0102, -0.0100,    -inf],\n",
            "         [ 0.0872,  0.0592,  0.1720,  ...,  0.0089,  0.0223,  0.1032]],\n",
            "\n",
            "        [[-0.0535,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1033, -0.0641,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1121,  0.3079,  0.2416,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0517, -0.0374, -0.1205,  ...,  0.1288,    -inf,    -inf],\n",
            "         [-0.0266, -0.0619, -0.0616,  ...,  0.0183, -0.0196,    -inf],\n",
            "         [ 0.0231, -0.0119, -0.0581,  ...,  0.0376, -0.0108, -0.0074]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0970,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0886, -0.0509,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1422,  0.1099, -0.0545,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0013, -0.0073, -0.0355,  ...,  0.0795,    -inf,    -inf],\n",
            "         [-0.0206, -0.0958, -0.1765,  ...,  0.0439,  0.1094,    -inf],\n",
            "         [ 0.0059, -0.1034, -0.0545,  ...,  0.1743,  0.2682,  0.1331]],\n",
            "\n",
            "        [[ 0.0385,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0703, -0.0727,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1677,  0.0744, -0.0371,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.3371, -0.0405,  0.0522,  ...,  0.0518,    -inf,    -inf],\n",
            "         [-0.2850, -0.0023,  0.0331,  ...,  0.1076,  0.1159,    -inf],\n",
            "         [-0.2310,  0.0312,  0.0678,  ...,  0.1528,  0.1761,  0.1603]],\n",
            "\n",
            "        [[-0.1101,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.3079,  0.2352,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0262,  0.0091,  0.2014,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0659,  0.0523,  0.1303,  ...,  0.0712,    -inf,    -inf],\n",
            "         [ 0.2410,  0.0516,  0.1892,  ...,  0.1517,  0.1981,    -inf],\n",
            "         [ 0.0664,  0.0082,  0.1827,  ...,  0.0608,  0.1350,  0.0705]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[ 0.0950, -0.1292,  0.0077,  ..., -0.0289, -0.0509, -0.0335],\n",
            "         [ 0.0846, -0.2399,  0.0017,  ...,  0.1089,  0.0869,  0.1021],\n",
            "         [-0.0744, -0.0459, -0.1504,  ..., -0.1101, -0.0947, -0.0917],\n",
            "         ...,\n",
            "         [-0.1386, -0.1725, -0.2079,  ...,  0.2036,  0.1334,  0.2259],\n",
            "         [-0.1309, -0.1089, -0.1125,  ...,  0.1834,  0.1149,  0.2117],\n",
            "         [-0.1183, -0.1478, -0.1469,  ...,  0.1756,  0.1190,  0.2120]],\n",
            "\n",
            "        [[-0.0168, -0.1295, -0.0730,  ...,  0.1051,  0.0870,  0.1864],\n",
            "         [-0.1033, -0.1751,  0.0130,  ..., -0.0644, -0.0245, -0.0361],\n",
            "         [ 0.1037, -0.1346,  0.2047,  ...,  0.0255, -0.0683, -0.0424],\n",
            "         ...,\n",
            "         [ 0.1752,  0.0429,  0.0761,  ...,  0.0292,  0.0686,  0.1480],\n",
            "         [ 0.0954,  0.0320,  0.1184,  ...,  0.0669,  0.1422,  0.2413],\n",
            "         [ 0.0994, -0.0276,  0.0976,  ...,  0.0913,  0.1648,  0.2217]],\n",
            "\n",
            "        [[-0.1085, -0.0190, -0.0857,  ..., -0.0679, -0.1036, -0.0869],\n",
            "         [-0.0037, -0.0668, -0.0567,  ...,  0.1419, -0.0827,  0.0163],\n",
            "         [-0.0604,  0.0675, -0.0471,  ..., -0.1238, -0.0887, -0.0948],\n",
            "         ...,\n",
            "         [ 0.0906,  0.1243,  0.0214,  ...,  0.1858,  0.2228,  0.1726],\n",
            "         [-0.0105,  0.1320,  0.0257,  ...,  0.1650,  0.1925,  0.1491],\n",
            "         [ 0.1279,  0.1481,  0.0336,  ...,  0.1210,  0.1434,  0.0923]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0339,  0.2474, -0.1734,  ..., -0.0015,  0.0219,  0.0244],\n",
            "         [-0.2539,  0.1578, -0.2654,  ..., -0.0405,  0.0253, -0.0408],\n",
            "         [-0.2399, -0.2520,  0.1876,  ..., -0.0353, -0.0296, -0.0550],\n",
            "         ...,\n",
            "         [ 0.0655, -0.1467,  0.1421,  ...,  0.2700,  0.1885,  0.2394],\n",
            "         [ 0.0145, -0.2084,  0.0430,  ...,  0.2520,  0.1680,  0.1971],\n",
            "         [ 0.0349, -0.0614,  0.1470,  ...,  0.2659,  0.1545,  0.1741]],\n",
            "\n",
            "        [[ 0.0421, -0.0932,  0.0994,  ...,  0.0808,  0.0447, -0.0277],\n",
            "         [-0.1145, -0.1014, -0.0141,  ..., -0.1995, -0.2088, -0.1371],\n",
            "         [ 0.2199, -0.0944,  0.0194,  ..., -0.0899, -0.0679, -0.0994],\n",
            "         ...,\n",
            "         [ 0.3475, -0.0241,  0.0372,  ...,  0.2258,  0.1483,  0.0712],\n",
            "         [ 0.3801, -0.0568, -0.0567,  ...,  0.1961,  0.1123,  0.0684],\n",
            "         [ 0.4969, -0.1046, -0.0557,  ...,  0.2123,  0.1313,  0.1164]],\n",
            "\n",
            "        [[-0.0775, -0.0904, -0.3309,  ...,  0.0418, -0.0787, -0.0498],\n",
            "         [ 0.0817,  0.4526, -0.2903,  ..., -0.0051, -0.2396, -0.0282],\n",
            "         [-0.1120, -0.2991, -0.3161,  ...,  0.1440,  0.0991,  0.1365],\n",
            "         ...,\n",
            "         [ 0.0490,  0.1348, -0.1148,  ...,  0.2110,  0.2310,  0.2074],\n",
            "         [ 0.0829,  0.2657, -0.1791,  ...,  0.1695,  0.1551,  0.1683],\n",
            "         [ 0.0378,  0.1022, -0.0487,  ...,  0.1849,  0.2358,  0.2054]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[ 0.0950,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0846, -0.2399,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0744, -0.0459, -0.1504,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.1386, -0.1725, -0.2079,  ...,  0.2036,    -inf,    -inf],\n",
            "         [-0.1309, -0.1089, -0.1125,  ...,  0.1834,  0.1149,    -inf],\n",
            "         [-0.1183, -0.1478, -0.1469,  ...,  0.1756,  0.1190,  0.2120]],\n",
            "\n",
            "        [[-0.0168,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1033, -0.1751,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1037, -0.1346,  0.2047,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1752,  0.0429,  0.0761,  ...,  0.0292,    -inf,    -inf],\n",
            "         [ 0.0954,  0.0320,  0.1184,  ...,  0.0669,  0.1422,    -inf],\n",
            "         [ 0.0994, -0.0276,  0.0976,  ...,  0.0913,  0.1648,  0.2217]],\n",
            "\n",
            "        [[-0.1085,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0037, -0.0668,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0604,  0.0675, -0.0471,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0906,  0.1243,  0.0214,  ...,  0.1858,    -inf,    -inf],\n",
            "         [-0.0105,  0.1320,  0.0257,  ...,  0.1650,  0.1925,    -inf],\n",
            "         [ 0.1279,  0.1481,  0.0336,  ...,  0.1210,  0.1434,  0.0923]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0339,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2539,  0.1578,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2399, -0.2520,  0.1876,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0655, -0.1467,  0.1421,  ...,  0.2700,    -inf,    -inf],\n",
            "         [ 0.0145, -0.2084,  0.0430,  ...,  0.2520,  0.1680,    -inf],\n",
            "         [ 0.0349, -0.0614,  0.1470,  ...,  0.2659,  0.1545,  0.1741]],\n",
            "\n",
            "        [[ 0.0421,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1145, -0.1014,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2199, -0.0944,  0.0194,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.3475, -0.0241,  0.0372,  ...,  0.2258,    -inf,    -inf],\n",
            "         [ 0.3801, -0.0568, -0.0567,  ...,  0.1961,  0.1123,    -inf],\n",
            "         [ 0.4969, -0.1046, -0.0557,  ...,  0.2123,  0.1313,  0.1164]],\n",
            "\n",
            "        [[-0.0775,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0817,  0.4526,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1120, -0.2991, -0.3161,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0490,  0.1348, -0.1148,  ...,  0.2110,    -inf,    -inf],\n",
            "         [ 0.0829,  0.2657, -0.1791,  ...,  0.1695,  0.1551,    -inf],\n",
            "         [ 0.0378,  0.1022, -0.0487,  ...,  0.1849,  0.2358,  0.2054]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[ 0.0567, -0.0612,  0.0318,  ..., -0.1583, -0.2246, -0.2031],\n",
            "         [ 0.1295,  0.0404, -0.0406,  ..., -0.1420, -0.2086, -0.1140],\n",
            "         [-0.0947,  0.0168, -0.0216,  ...,  0.0314,  0.0455,  0.1267],\n",
            "         ...,\n",
            "         [ 0.2029, -0.2658, -0.0148,  ...,  0.0294, -0.0321, -0.0537],\n",
            "         [ 0.2019, -0.2757, -0.0220,  ..., -0.0444, -0.0963, -0.1288],\n",
            "         [ 0.1407, -0.2631, -0.0857,  ..., -0.0169, -0.0488, -0.1030]],\n",
            "\n",
            "        [[ 0.0374, -0.0947, -0.0115,  ..., -0.0858, -0.0738, -0.0652],\n",
            "         [-0.0881, -0.0116, -0.1145,  ..., -0.0105,  0.0202, -0.0078],\n",
            "         [-0.2810, -0.1879, -0.1217,  ..., -0.0621, -0.0131, -0.0208],\n",
            "         ...,\n",
            "         [-0.1683,  0.0871, -0.0250,  ..., -0.0850, -0.0731, -0.0815],\n",
            "         [-0.1629,  0.1411,  0.0526,  ..., -0.0771, -0.0496, -0.0655],\n",
            "         [-0.1404,  0.1066, -0.0143,  ..., -0.1272, -0.0980, -0.0918]],\n",
            "\n",
            "        [[-0.0832, -0.0421,  0.0466,  ..., -0.0784, -0.0583, -0.0944],\n",
            "         [-0.2399,  0.0253, -0.0611,  ...,  0.2364,  0.2673,  0.3374],\n",
            "         [-0.0512, -0.0692, -0.1288,  ..., -0.1621, -0.0958, -0.1350],\n",
            "         ...,\n",
            "         [-0.0803,  0.2189, -0.2361,  ...,  0.0495, -0.0263,  0.0624],\n",
            "         [-0.1205,  0.2184, -0.2618,  ...,  0.0921, -0.0005,  0.0861],\n",
            "         [-0.1633,  0.2250, -0.1212,  ...,  0.0509, -0.0401,  0.0518]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0816,  0.0220, -0.0671,  ..., -0.1322, -0.0341, -0.1040],\n",
            "         [ 0.2109, -0.0810,  0.0978,  ...,  0.0862,  0.0818,  0.0911],\n",
            "         [-0.4260,  0.1981,  0.0561,  ...,  0.0622,  0.2019,  0.0548],\n",
            "         ...,\n",
            "         [ 0.0984, -0.2257, -0.0542,  ..., -0.0304, -0.0478, -0.0521],\n",
            "         [ 0.0691, -0.2342,  0.0598,  ...,  0.0327, -0.0274, -0.0129],\n",
            "         [-0.0015, -0.2188,  0.0072,  ...,  0.0720,  0.0106,  0.0085]],\n",
            "\n",
            "        [[-0.1524,  0.0624,  0.1662,  ...,  0.0835,  0.0668,  0.0926],\n",
            "         [ 0.0640, -0.0655, -0.1014,  ...,  0.0165,  0.0409,  0.0965],\n",
            "         [-0.0673,  0.3207, -0.1755,  ..., -0.2067, -0.2117, -0.1734],\n",
            "         ...,\n",
            "         [ 0.1944, -0.2794,  0.2220,  ..., -0.1339, -0.1378, -0.1652],\n",
            "         [ 0.0752, -0.2508,  0.2536,  ..., -0.0418, -0.0624, -0.1079],\n",
            "         [ 0.1192, -0.2806,  0.2117,  ..., -0.0322, -0.0480, -0.0981]],\n",
            "\n",
            "        [[-0.1166,  0.0679, -0.1107,  ..., -0.0566, -0.0768, -0.1088],\n",
            "         [ 0.0755,  0.1841, -0.0106,  ...,  0.0764,  0.1604,  0.0819],\n",
            "         [-0.0380,  0.0069,  0.2144,  ..., -0.0445, -0.0464, -0.0935],\n",
            "         ...,\n",
            "         [-0.0263, -0.0576, -0.0563,  ..., -0.0567, -0.0778,  0.0014],\n",
            "         [ 0.0284, -0.0675, -0.0833,  ..., -0.0775, -0.0977,  0.0079],\n",
            "         [-0.0666,  0.0269, -0.1546,  ..., -0.0827, -0.0987, -0.0427]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[ 0.0567,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1295,  0.0404,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0947,  0.0168, -0.0216,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.2029, -0.2658, -0.0148,  ...,  0.0294,    -inf,    -inf],\n",
            "         [ 0.2019, -0.2757, -0.0220,  ..., -0.0444, -0.0963,    -inf],\n",
            "         [ 0.1407, -0.2631, -0.0857,  ..., -0.0169, -0.0488, -0.1030]],\n",
            "\n",
            "        [[ 0.0374,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0881, -0.0116,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2810, -0.1879, -0.1217,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.1683,  0.0871, -0.0250,  ..., -0.0850,    -inf,    -inf],\n",
            "         [-0.1629,  0.1411,  0.0526,  ..., -0.0771, -0.0496,    -inf],\n",
            "         [-0.1404,  0.1066, -0.0143,  ..., -0.1272, -0.0980, -0.0918]],\n",
            "\n",
            "        [[-0.0832,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2399,  0.0253,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0512, -0.0692, -0.1288,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0803,  0.2189, -0.2361,  ...,  0.0495,    -inf,    -inf],\n",
            "         [-0.1205,  0.2184, -0.2618,  ...,  0.0921, -0.0005,    -inf],\n",
            "         [-0.1633,  0.2250, -0.1212,  ...,  0.0509, -0.0401,  0.0518]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0816,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2109, -0.0810,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.4260,  0.1981,  0.0561,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0984, -0.2257, -0.0542,  ..., -0.0304,    -inf,    -inf],\n",
            "         [ 0.0691, -0.2342,  0.0598,  ...,  0.0327, -0.0274,    -inf],\n",
            "         [-0.0015, -0.2188,  0.0072,  ...,  0.0720,  0.0106,  0.0085]],\n",
            "\n",
            "        [[-0.1524,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0640, -0.0655,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0673,  0.3207, -0.1755,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1944, -0.2794,  0.2220,  ..., -0.1339,    -inf,    -inf],\n",
            "         [ 0.0752, -0.2508,  0.2536,  ..., -0.0418, -0.0624,    -inf],\n",
            "         [ 0.1192, -0.2806,  0.2117,  ..., -0.0322, -0.0480, -0.0981]],\n",
            "\n",
            "        [[-0.1166,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0755,  0.1841,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0380,  0.0069,  0.2144,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0263, -0.0576, -0.0563,  ..., -0.0567,    -inf,    -inf],\n",
            "         [ 0.0284, -0.0675, -0.0833,  ..., -0.0775, -0.0977,    -inf],\n",
            "         [-0.0666,  0.0269, -0.1546,  ..., -0.0827, -0.0987, -0.0427]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-0.0259,  0.0914,  0.0310,  ...,  0.0303,  0.0528,  0.0383],\n",
            "         [-0.2029, -0.1471, -0.1200,  ..., -0.0993, -0.0776, -0.1618],\n",
            "         [ 0.2426,  0.1055,  0.0241,  ..., -0.0202, -0.0548, -0.0990],\n",
            "         ...,\n",
            "         [ 0.0045,  0.0820,  0.0678,  ...,  0.0350, -0.0226, -0.0307],\n",
            "         [ 0.0326,  0.1217,  0.1149,  ...,  0.0082, -0.0333, -0.0284],\n",
            "         [ 0.0188,  0.0531,  0.0992,  ..., -0.0280, -0.0838, -0.0937]],\n",
            "\n",
            "        [[ 0.0134, -0.0986,  0.1286,  ..., -0.0454, -0.0281, -0.0580],\n",
            "         [-0.0928, -0.1721, -0.0293,  ..., -0.1306, -0.1025, -0.1142],\n",
            "         [ 0.1814, -0.0745, -0.1594,  ...,  0.0391,  0.0863,  0.1144],\n",
            "         ...,\n",
            "         [ 0.0834,  0.0824, -0.0408,  ...,  0.0497, -0.0519, -0.0005],\n",
            "         [ 0.0428,  0.0787, -0.0590,  ...,  0.0025, -0.0705, -0.0520],\n",
            "         [-0.0165,  0.0340, -0.0442,  ...,  0.0486, -0.0265, -0.0010]],\n",
            "\n",
            "        [[ 0.1031,  0.0702, -0.0629,  ..., -0.0573, -0.0698, -0.0831],\n",
            "         [ 0.2200, -0.0018, -0.0034,  ..., -0.1437, -0.1094, -0.0158],\n",
            "         [ 0.2313,  0.2027,  0.2714,  ...,  0.1287,  0.1315,  0.1626],\n",
            "         ...,\n",
            "         [-0.0660,  0.0179,  0.0129,  ..., -0.0138, -0.0144,  0.0431],\n",
            "         [-0.0944,  0.1346, -0.0699,  ..., -0.0225, -0.0441,  0.0234],\n",
            "         [-0.0859, -0.0292, -0.1370,  ...,  0.0317,  0.0139,  0.0509]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2371,  0.3272, -0.1984,  ...,  0.0320,  0.0050, -0.0091],\n",
            "         [-0.0114,  0.0562, -0.1680,  ...,  0.1931,  0.2041,  0.2145],\n",
            "         [ 0.1250, -0.1082, -0.1096,  ..., -0.0605, -0.0288, -0.0669],\n",
            "         ...,\n",
            "         [ 0.0321, -0.1203,  0.0385,  ..., -0.0964, -0.0950, -0.0293],\n",
            "         [-0.0479, -0.1027,  0.0950,  ..., -0.0390, -0.0708, -0.0164],\n",
            "         [-0.0255, -0.0447,  0.0497,  ..., -0.0906, -0.1102, -0.0630]],\n",
            "\n",
            "        [[-0.1834,  0.0654, -0.0866,  ...,  0.1394,  0.1411,  0.0965],\n",
            "         [-0.0074, -0.1682,  0.0402,  ..., -0.1302, -0.0995, -0.1473],\n",
            "         [-0.1196, -0.0312, -0.0545,  ...,  0.1388,  0.0618,  0.0976],\n",
            "         ...,\n",
            "         [ 0.0753,  0.0111,  0.0766,  ..., -0.1695, -0.0871,  0.0126],\n",
            "         [ 0.0928, -0.0012,  0.0540,  ..., -0.1813, -0.0736,  0.0297],\n",
            "         [ 0.1246,  0.0154,  0.1197,  ..., -0.2125, -0.0954, -0.0049]],\n",
            "\n",
            "        [[-0.1174,  0.0617,  0.0903,  ..., -0.2281, -0.1711, -0.2306],\n",
            "         [-0.0542,  0.0296, -0.0049,  ..., -0.0343,  0.0114,  0.0104],\n",
            "         [-0.0006,  0.0662, -0.0774,  ...,  0.1549,  0.1662,  0.1079],\n",
            "         ...,\n",
            "         [ 0.1133, -0.0697,  0.0441,  ...,  0.0095,  0.0475, -0.0364],\n",
            "         [ 0.1148, -0.0711,  0.0326,  ...,  0.0174,  0.0599, -0.0231],\n",
            "         [ 0.1396,  0.0349,  0.0559,  ..., -0.0383,  0.0119, -0.0929]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[-0.0259,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2029, -0.1471,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2426,  0.1055,  0.0241,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0045,  0.0820,  0.0678,  ...,  0.0350,    -inf,    -inf],\n",
            "         [ 0.0326,  0.1217,  0.1149,  ...,  0.0082, -0.0333,    -inf],\n",
            "         [ 0.0188,  0.0531,  0.0992,  ..., -0.0280, -0.0838, -0.0937]],\n",
            "\n",
            "        [[ 0.0134,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0928, -0.1721,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1814, -0.0745, -0.1594,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0834,  0.0824, -0.0408,  ...,  0.0497,    -inf,    -inf],\n",
            "         [ 0.0428,  0.0787, -0.0590,  ...,  0.0025, -0.0705,    -inf],\n",
            "         [-0.0165,  0.0340, -0.0442,  ...,  0.0486, -0.0265, -0.0010]],\n",
            "\n",
            "        [[ 0.1031,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2200, -0.0018,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2313,  0.2027,  0.2714,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0660,  0.0179,  0.0129,  ..., -0.0138,    -inf,    -inf],\n",
            "         [-0.0944,  0.1346, -0.0699,  ..., -0.0225, -0.0441,    -inf],\n",
            "         [-0.0859, -0.0292, -0.1370,  ...,  0.0317,  0.0139,  0.0509]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2371,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0114,  0.0562,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1250, -0.1082, -0.1096,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0321, -0.1203,  0.0385,  ..., -0.0964,    -inf,    -inf],\n",
            "         [-0.0479, -0.1027,  0.0950,  ..., -0.0390, -0.0708,    -inf],\n",
            "         [-0.0255, -0.0447,  0.0497,  ..., -0.0906, -0.1102, -0.0630]],\n",
            "\n",
            "        [[-0.1834,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0074, -0.1682,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1196, -0.0312, -0.0545,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0753,  0.0111,  0.0766,  ..., -0.1695,    -inf,    -inf],\n",
            "         [ 0.0928, -0.0012,  0.0540,  ..., -0.1813, -0.0736,    -inf],\n",
            "         [ 0.1246,  0.0154,  0.1197,  ..., -0.2125, -0.0954, -0.0049]],\n",
            "\n",
            "        [[-0.1174,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0542,  0.0296,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0006,  0.0662, -0.0774,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1133, -0.0697,  0.0441,  ...,  0.0095,    -inf,    -inf],\n",
            "         [ 0.1148, -0.0711,  0.0326,  ...,  0.0174,  0.0599,    -inf],\n",
            "         [ 0.1396,  0.0349,  0.0559,  ..., -0.0383,  0.0119, -0.0929]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[ 0.0801,  0.1579,  0.1765,  ...,  0.0312,  0.0604,  0.0572],\n",
            "         [ 0.0138, -0.1659,  0.3809,  ..., -0.1143, -0.0871, -0.0265],\n",
            "         [-0.1098, -0.1503, -0.0656,  ...,  0.0016, -0.0178, -0.0718],\n",
            "         ...,\n",
            "         [ 0.1639,  0.1445,  0.1249,  ...,  0.1343,  0.1619,  0.2665],\n",
            "         [ 0.1012,  0.1252,  0.0714,  ...,  0.1540,  0.1795,  0.2842],\n",
            "         [ 0.1814,  0.1476,  0.1417,  ...,  0.1838,  0.1856,  0.2845]],\n",
            "\n",
            "        [[-0.0198,  0.1792,  0.1184,  ...,  0.0792,  0.1161,  0.1724],\n",
            "         [-0.0170,  0.3522,  0.1244,  ...,  0.0291, -0.0272, -0.0192],\n",
            "         [ 0.1074,  0.0112, -0.2138,  ...,  0.0575,  0.0503,  0.1068],\n",
            "         ...,\n",
            "         [ 0.1119,  0.0642, -0.0583,  ...,  0.1587,  0.2308,  0.1837],\n",
            "         [ 0.1435,  0.0184, -0.0149,  ...,  0.1099,  0.1990,  0.1586],\n",
            "         [ 0.2492, -0.0311,  0.0079,  ...,  0.1190,  0.2195,  0.1626]],\n",
            "\n",
            "        [[ 0.1127, -0.2411, -0.0314,  ...,  0.0475,  0.0429,  0.1251],\n",
            "         [-0.0087,  0.0290, -0.0400,  ...,  0.1472,  0.0973,  0.1151],\n",
            "         [-0.1007,  0.0023,  0.1506,  ..., -0.0940, -0.0617, -0.0909],\n",
            "         ...,\n",
            "         [-0.0589,  0.0340, -0.1190,  ...,  0.1618,  0.1019,  0.0917],\n",
            "         [ 0.0260,  0.0139, -0.1619,  ...,  0.1460,  0.1200,  0.1330],\n",
            "         [ 0.0279, -0.0072, -0.1507,  ...,  0.1964,  0.1608,  0.1373]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1469, -0.1508, -0.0604,  ...,  0.0265,  0.0131,  0.0806],\n",
            "         [ 0.0700,  0.0630,  0.0093,  ...,  0.0151,  0.0071, -0.0349],\n",
            "         [ 0.1600, -0.0227,  0.0487,  ...,  0.0396,  0.0327,  0.0870],\n",
            "         ...,\n",
            "         [-0.0948, -0.1291,  0.0053,  ...,  0.1684,  0.2214,  0.1573],\n",
            "         [-0.0817, -0.1199,  0.0247,  ...,  0.1044,  0.1597,  0.1374],\n",
            "         [ 0.0109, -0.1049,  0.0416,  ...,  0.1687,  0.2335,  0.1798]],\n",
            "\n",
            "        [[ 0.0142,  0.0265,  0.1210,  ...,  0.2370,  0.1948,  0.2039],\n",
            "         [ 0.0486,  0.0908, -0.2110,  ...,  0.0177,  0.0106,  0.1049],\n",
            "         [-0.0649, -0.0062, -0.0325,  ...,  0.0825,  0.0560,  0.0236],\n",
            "         ...,\n",
            "         [ 0.0377, -0.0070, -0.0350,  ...,  0.1613,  0.1186,  0.1338],\n",
            "         [ 0.0201,  0.0144,  0.0077,  ...,  0.2184,  0.1991,  0.2176],\n",
            "         [-0.0095,  0.0410, -0.0503,  ...,  0.1670,  0.1906,  0.1899]],\n",
            "\n",
            "        [[-0.0124,  0.2298, -0.0376,  ..., -0.0989, -0.1014,  0.0564],\n",
            "         [-0.1603,  0.0264, -0.0224,  ...,  0.0195,  0.0363,  0.0513],\n",
            "         [ 0.0371,  0.0723,  0.1061,  ...,  0.1497,  0.2479,  0.1728],\n",
            "         ...,\n",
            "         [ 0.1356,  0.1223,  0.0411,  ...,  0.2376,  0.1596,  0.1254],\n",
            "         [ 0.1842,  0.1378, -0.0367,  ...,  0.2105,  0.1505,  0.1219],\n",
            "         [ 0.1739,  0.1342,  0.0228,  ...,  0.2706,  0.2115,  0.1539]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[ 0.0801,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0138, -0.1659,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1098, -0.1503, -0.0656,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1639,  0.1445,  0.1249,  ...,  0.1343,    -inf,    -inf],\n",
            "         [ 0.1012,  0.1252,  0.0714,  ...,  0.1540,  0.1795,    -inf],\n",
            "         [ 0.1814,  0.1476,  0.1417,  ...,  0.1838,  0.1856,  0.2845]],\n",
            "\n",
            "        [[-0.0198,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0170,  0.3522,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1074,  0.0112, -0.2138,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1119,  0.0642, -0.0583,  ...,  0.1587,    -inf,    -inf],\n",
            "         [ 0.1435,  0.0184, -0.0149,  ...,  0.1099,  0.1990,    -inf],\n",
            "         [ 0.2492, -0.0311,  0.0079,  ...,  0.1190,  0.2195,  0.1626]],\n",
            "\n",
            "        [[ 0.1127,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0087,  0.0290,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1007,  0.0023,  0.1506,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0589,  0.0340, -0.1190,  ...,  0.1618,    -inf,    -inf],\n",
            "         [ 0.0260,  0.0139, -0.1619,  ...,  0.1460,  0.1200,    -inf],\n",
            "         [ 0.0279, -0.0072, -0.1507,  ...,  0.1964,  0.1608,  0.1373]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1469,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0700,  0.0630,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1600, -0.0227,  0.0487,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0948, -0.1291,  0.0053,  ...,  0.1684,    -inf,    -inf],\n",
            "         [-0.0817, -0.1199,  0.0247,  ...,  0.1044,  0.1597,    -inf],\n",
            "         [ 0.0109, -0.1049,  0.0416,  ...,  0.1687,  0.2335,  0.1798]],\n",
            "\n",
            "        [[ 0.0142,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0486,  0.0908,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0649, -0.0062, -0.0325,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0377, -0.0070, -0.0350,  ...,  0.1613,    -inf,    -inf],\n",
            "         [ 0.0201,  0.0144,  0.0077,  ...,  0.2184,  0.1991,    -inf],\n",
            "         [-0.0095,  0.0410, -0.0503,  ...,  0.1670,  0.1906,  0.1899]],\n",
            "\n",
            "        [[-0.0124,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1603,  0.0264,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0371,  0.0723,  0.1061,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1356,  0.1223,  0.0411,  ...,  0.2376,    -inf,    -inf],\n",
            "         [ 0.1842,  0.1378, -0.0367,  ...,  0.2105,  0.1505,    -inf],\n",
            "         [ 0.1739,  0.1342,  0.0228,  ...,  0.2706,  0.2115,  0.1539]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-0.2165,  0.1113, -0.0320,  ..., -0.0837, -0.0194, -0.0744],\n",
            "         [ 0.0515,  0.1441, -0.1690,  ..., -0.1627, -0.1155, -0.1887],\n",
            "         [-0.2122,  0.0592,  0.1218,  ...,  0.3314,  0.3502,  0.3676],\n",
            "         ...,\n",
            "         [ 0.1606,  0.1942, -0.1969,  ..., -0.0237,  0.0038, -0.0150],\n",
            "         [ 0.2363,  0.1507, -0.2521,  ..., -0.1320, -0.1151, -0.1250],\n",
            "         [ 0.1014,  0.1123, -0.2073,  ..., -0.1216, -0.1244, -0.1009]],\n",
            "\n",
            "        [[-0.1017,  0.3123, -0.0714,  ..., -0.1359, -0.2036, -0.1331],\n",
            "         [ 0.0211,  0.0026,  0.0552,  ..., -0.0542,  0.0026, -0.0853],\n",
            "         [-0.0152,  0.3193, -0.3781,  ..., -0.0338,  0.0636, -0.0116],\n",
            "         ...,\n",
            "         [-0.1204, -0.0891,  0.2195,  ..., -0.0030,  0.0992,  0.0452],\n",
            "         [-0.1631, -0.0943,  0.2115,  ..., -0.2078, -0.1196, -0.2111],\n",
            "         [-0.0609, -0.1174,  0.3556,  ..., -0.1857, -0.0495, -0.1545]],\n",
            "\n",
            "        [[ 0.0412,  0.2150,  0.0363,  ..., -0.1468, -0.2098, -0.2011],\n",
            "         [-0.1201, -0.1688,  0.0311,  ...,  0.0286,  0.0645,  0.0654],\n",
            "         [-0.0657, -0.1034, -0.3616,  ...,  0.2580,  0.2205,  0.2994],\n",
            "         ...,\n",
            "         [ 0.2993,  0.1514, -0.1067,  ..., -0.1107, -0.0866, -0.1113],\n",
            "         [ 0.3629,  0.1265, -0.0557,  ..., -0.1595, -0.1225, -0.1409],\n",
            "         [ 0.4088,  0.1587, -0.1253,  ..., -0.0840, -0.0454, -0.0521]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0162,  0.1546, -0.1240,  ..., -0.3619, -0.2917, -0.1991],\n",
            "         [-0.0361, -0.0298, -0.0152,  ..., -0.1185, -0.1411, -0.2102],\n",
            "         [ 0.0966,  0.0311, -0.0271,  ...,  0.1509,  0.1664,  0.1971],\n",
            "         ...,\n",
            "         [ 0.0275,  0.0609,  0.0867,  ..., -0.1933, -0.2043, -0.1565],\n",
            "         [ 0.0827,  0.0470,  0.0335,  ..., -0.0923, -0.1219, -0.1039],\n",
            "         [ 0.0139,  0.0465, -0.0427,  ..., -0.0617, -0.0523, -0.0381]],\n",
            "\n",
            "        [[ 0.1456, -0.0634,  0.2223,  ...,  0.0241,  0.0144,  0.0629],\n",
            "         [-0.2536, -0.0233, -0.0694,  ...,  0.0125, -0.0302,  0.0331],\n",
            "         [-0.1675,  0.1001, -0.0931,  ...,  0.1908,  0.1714,  0.2052],\n",
            "         ...,\n",
            "         [ 0.0909,  0.0172, -0.0404,  ...,  0.0262, -0.0293, -0.0091],\n",
            "         [ 0.0556,  0.0258,  0.0031,  ...,  0.0141, -0.0593, -0.0634],\n",
            "         [ 0.1051, -0.0325,  0.0321,  ..., -0.0159, -0.0606, -0.0554]],\n",
            "\n",
            "        [[ 0.0125,  0.1599, -0.0983,  ..., -0.0849, -0.1285, -0.1109],\n",
            "         [ 0.2501, -0.1753,  0.2310,  ..., -0.0510, -0.0274, -0.1580],\n",
            "         [-0.0942,  0.0762, -0.1215,  ..., -0.0692, -0.1440, -0.0956],\n",
            "         ...,\n",
            "         [ 0.0490, -0.1498,  0.1986,  ..., -0.1811, -0.1756, -0.0387],\n",
            "         [ 0.0926, -0.2278,  0.2390,  ..., -0.1171, -0.0900, -0.0101],\n",
            "         [ 0.0662, -0.2142,  0.2874,  ..., -0.2317, -0.2135, -0.1086]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[-0.2165,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0515,  0.1441,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2122,  0.0592,  0.1218,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1606,  0.1942, -0.1969,  ..., -0.0237,    -inf,    -inf],\n",
            "         [ 0.2363,  0.1507, -0.2521,  ..., -0.1320, -0.1151,    -inf],\n",
            "         [ 0.1014,  0.1123, -0.2073,  ..., -0.1216, -0.1244, -0.1009]],\n",
            "\n",
            "        [[-0.1017,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0211,  0.0026,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0152,  0.3193, -0.3781,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.1204, -0.0891,  0.2195,  ..., -0.0030,    -inf,    -inf],\n",
            "         [-0.1631, -0.0943,  0.2115,  ..., -0.2078, -0.1196,    -inf],\n",
            "         [-0.0609, -0.1174,  0.3556,  ..., -0.1857, -0.0495, -0.1545]],\n",
            "\n",
            "        [[ 0.0412,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1201, -0.1688,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0657, -0.1034, -0.3616,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.2993,  0.1514, -0.1067,  ..., -0.1107,    -inf,    -inf],\n",
            "         [ 0.3629,  0.1265, -0.0557,  ..., -0.1595, -0.1225,    -inf],\n",
            "         [ 0.4088,  0.1587, -0.1253,  ..., -0.0840, -0.0454, -0.0521]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0162,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0361, -0.0298,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0966,  0.0311, -0.0271,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0275,  0.0609,  0.0867,  ..., -0.1933,    -inf,    -inf],\n",
            "         [ 0.0827,  0.0470,  0.0335,  ..., -0.0923, -0.1219,    -inf],\n",
            "         [ 0.0139,  0.0465, -0.0427,  ..., -0.0617, -0.0523, -0.0381]],\n",
            "\n",
            "        [[ 0.1456,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.2536, -0.0233,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1675,  0.1001, -0.0931,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0909,  0.0172, -0.0404,  ...,  0.0262,    -inf,    -inf],\n",
            "         [ 0.0556,  0.0258,  0.0031,  ...,  0.0141, -0.0593,    -inf],\n",
            "         [ 0.1051, -0.0325,  0.0321,  ..., -0.0159, -0.0606, -0.0554]],\n",
            "\n",
            "        [[ 0.0125,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2501, -0.1753,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0942,  0.0762, -0.1215,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0490, -0.1498,  0.1986,  ..., -0.1811,    -inf,    -inf],\n",
            "         [ 0.0926, -0.2278,  0.2390,  ..., -0.1171, -0.0900,    -inf],\n",
            "         [ 0.0662, -0.2142,  0.2874,  ..., -0.2317, -0.2135, -0.1086]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-0.1296,  0.1593, -0.4095,  ..., -0.1518, -0.1733, -0.2070],\n",
            "         [ 0.0618,  0.0065, -0.0100,  ...,  0.2384,  0.2631,  0.2747],\n",
            "         [ 0.0666,  0.0399, -0.1403,  ...,  0.1693,  0.1633,  0.2438],\n",
            "         ...,\n",
            "         [-0.0747,  0.1330, -0.1401,  ..., -0.3300, -0.4108, -0.3365],\n",
            "         [-0.0795,  0.1174, -0.1815,  ..., -0.2658, -0.3408, -0.2480],\n",
            "         [-0.0199,  0.1541, -0.2782,  ..., -0.2928, -0.3755, -0.3077]],\n",
            "\n",
            "        [[-0.0908,  0.0387, -0.1680,  ..., -0.0222,  0.0383,  0.0109],\n",
            "         [ 0.1896, -0.0105, -0.0626,  ..., -0.1896, -0.1285, -0.1249],\n",
            "         [-0.1379, -0.0090,  0.1998,  ...,  0.0179,  0.0342,  0.0766],\n",
            "         ...,\n",
            "         [ 0.0542, -0.0422,  0.0791,  ..., -0.1507, -0.2244, -0.2136],\n",
            "         [ 0.0280, -0.0053,  0.0942,  ..., -0.2669, -0.3150, -0.3171],\n",
            "         [ 0.0198,  0.0375,  0.0993,  ..., -0.2524, -0.3268, -0.2894]],\n",
            "\n",
            "        [[ 0.0710,  0.0950, -0.0061,  ...,  0.0606,  0.0633,  0.0512],\n",
            "         [-0.0405,  0.0069,  0.1705,  ..., -0.1166, -0.0993, -0.1130],\n",
            "         [-0.1982, -0.0702,  0.0536,  ..., -0.0384, -0.0663, -0.0456],\n",
            "         ...,\n",
            "         [ 0.0380,  0.1385,  0.2092,  ..., -0.3431, -0.2843, -0.2438],\n",
            "         [ 0.0637,  0.0608,  0.1444,  ..., -0.3789, -0.3328, -0.2761],\n",
            "         [ 0.0803,  0.0372,  0.1469,  ..., -0.3136, -0.2663, -0.2113]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0399,  0.1752,  0.1295,  ..., -0.0829, -0.0679, -0.1050],\n",
            "         [-0.1071, -0.0645, -0.0056,  ...,  0.0533,  0.0642,  0.0441],\n",
            "         [-0.0021, -0.0508,  0.0215,  ...,  0.0125,  0.0337,  0.0269],\n",
            "         ...,\n",
            "         [ 0.0168, -0.0482,  0.1233,  ..., -0.3393, -0.3071, -0.2851],\n",
            "         [-0.0984,  0.0362,  0.0414,  ..., -0.3323, -0.3012, -0.2806],\n",
            "         [-0.1055, -0.0113, -0.0312,  ..., -0.3440, -0.3143, -0.3637]],\n",
            "\n",
            "        [[ 0.1709, -0.2595, -0.0298,  ..., -0.1479, -0.1649, -0.1645],\n",
            "         [ 0.0575,  0.0018,  0.0476,  ...,  0.1079,  0.1708,  0.0905],\n",
            "         [-0.0735,  0.2500,  0.0548,  ...,  0.0936,  0.2388,  0.0955],\n",
            "         ...,\n",
            "         [ 0.1925, -0.1892, -0.1223,  ..., -0.3431, -0.3655, -0.4128],\n",
            "         [ 0.2128, -0.0830, -0.0179,  ..., -0.2298, -0.2263, -0.2914],\n",
            "         [ 0.2758, -0.1065, -0.0770,  ..., -0.2958, -0.3512, -0.3720]],\n",
            "\n",
            "        [[ 0.0358,  0.0012,  0.1852,  ...,  0.4605,  0.4406,  0.4083],\n",
            "         [ 0.0747, -0.0889,  0.0880,  ...,  0.0540, -0.0178,  0.0560],\n",
            "         [ 0.1705,  0.0199, -0.0300,  ...,  0.1312,  0.0759,  0.1263],\n",
            "         ...,\n",
            "         [ 0.1504,  0.1096, -0.3264,  ..., -0.3006, -0.2626, -0.3204],\n",
            "         [ 0.1485,  0.0364, -0.1932,  ..., -0.2851, -0.2605, -0.3068],\n",
            "         [ 0.1681,  0.0531, -0.2743,  ..., -0.2916, -0.2701, -0.3205]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[-0.1296,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0618,  0.0065,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0666,  0.0399, -0.1403,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0747,  0.1330, -0.1401,  ..., -0.3300,    -inf,    -inf],\n",
            "         [-0.0795,  0.1174, -0.1815,  ..., -0.2658, -0.3408,    -inf],\n",
            "         [-0.0199,  0.1541, -0.2782,  ..., -0.2928, -0.3755, -0.3077]],\n",
            "\n",
            "        [[-0.0908,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1896, -0.0105,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1379, -0.0090,  0.1998,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0542, -0.0422,  0.0791,  ..., -0.1507,    -inf,    -inf],\n",
            "         [ 0.0280, -0.0053,  0.0942,  ..., -0.2669, -0.3150,    -inf],\n",
            "         [ 0.0198,  0.0375,  0.0993,  ..., -0.2524, -0.3268, -0.2894]],\n",
            "\n",
            "        [[ 0.0710,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0405,  0.0069,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1982, -0.0702,  0.0536,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0380,  0.1385,  0.2092,  ..., -0.3431,    -inf,    -inf],\n",
            "         [ 0.0637,  0.0608,  0.1444,  ..., -0.3789, -0.3328,    -inf],\n",
            "         [ 0.0803,  0.0372,  0.1469,  ..., -0.3136, -0.2663, -0.2113]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0399,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1071, -0.0645,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0021, -0.0508,  0.0215,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0168, -0.0482,  0.1233,  ..., -0.3393,    -inf,    -inf],\n",
            "         [-0.0984,  0.0362,  0.0414,  ..., -0.3323, -0.3012,    -inf],\n",
            "         [-0.1055, -0.0113, -0.0312,  ..., -0.3440, -0.3143, -0.3637]],\n",
            "\n",
            "        [[ 0.1709,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0575,  0.0018,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0735,  0.2500,  0.0548,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1925, -0.1892, -0.1223,  ..., -0.3431,    -inf,    -inf],\n",
            "         [ 0.2128, -0.0830, -0.0179,  ..., -0.2298, -0.2263,    -inf],\n",
            "         [ 0.2758, -0.1065, -0.0770,  ..., -0.2958, -0.3512, -0.3720]],\n",
            "\n",
            "        [[ 0.0358,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0747, -0.0889,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1705,  0.0199, -0.0300,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.1504,  0.1096, -0.3264,  ..., -0.3006,    -inf,    -inf],\n",
            "         [ 0.1485,  0.0364, -0.1932,  ..., -0.2851, -0.2605,    -inf],\n",
            "         [ 0.1681,  0.0531, -0.2743,  ..., -0.2916, -0.2701, -0.3205]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.4386,  0.5303,  0.6405,  ...,  0.2399, -1.4687, -0.5983],\n",
            "         [ 1.1515, -0.3848, -0.0000,  ..., -1.5527,  0.2448, -0.3095],\n",
            "         [ 3.5953,  1.2245, -0.2271,  ...,  1.1249, -0.6132,  0.7021],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.0000,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.3630, -0.0000, -1.1992,  ..., -0.4825,  2.7803, -0.5993],\n",
            "         [-1.2217, -0.0421, -0.0822,  ...,  0.4250,  1.5771, -1.0303],\n",
            "         [-1.4790,  1.0127, -3.0224,  ..., -0.8060, -1.0719, -0.8004],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.0000,  2.0575]],\n",
            "\n",
            "        [[-0.4986,  0.0000, -1.7183,  ...,  1.6167,  0.5873,  0.6898],\n",
            "         [-0.7198,  0.7641,  0.3288,  ..., -1.3275, -1.2690, -1.0568],\n",
            "         [ 1.6301,  0.4369, -2.4252,  ...,  0.6177, -0.0000, -0.3944],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-0.0000, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5860,  1.1312, -1.0814,  ...,  0.4406,  0.3651,  0.7031],\n",
            "         [-0.1775, -0.9814, -1.2993,  ...,  0.1609,  1.6517, -0.0209],\n",
            "         [-1.4397, -0.0000,  0.6600,  ...,  0.2279,  2.9070,  0.5085],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  0.0000],\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[-0.8267,  0.7046,  0.9928,  ..., -0.0000, -0.5183,  2.2207],\n",
            "         [ 1.2231, -2.5945,  0.4160,  ...,  2.6846,  0.2909,  0.9155],\n",
            "         [-1.5960, -0.8619,  0.0149,  ...,  0.9160,  0.5782, -0.7331],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -0.0000,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575]],\n",
            "\n",
            "        [[ 1.1235, -0.8659, -0.7298,  ...,  1.4217,  1.9598, -0.5040],\n",
            "         [-2.0815,  0.5463, -0.0622,  ..., -2.3253, -2.7379, -0.7186],\n",
            "         [-0.2242,  1.5276, -1.0323,  ...,  0.6732,  2.1657,  0.0181],\n",
            "         ...,\n",
            "         [-1.7087, -1.9359,  0.0000,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.1018,  0.6107,  2.0575],\n",
            "         [-1.7087, -1.9359,  0.1945,  ...,  0.0000,  0.6107,  2.0575]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-0.1826,  0.1814, -0.0336,  ...,  0.0625,  0.0175,  0.0228],\n",
            "         [-0.0308, -0.0324, -0.0281,  ..., -0.0928,  0.0033, -0.0079],\n",
            "         [-0.0384,  0.0530, -0.0412,  ...,  0.2128,  0.1997,  0.1831],\n",
            "         ...,\n",
            "         [-0.3290,  0.1791,  0.2152,  ..., -0.1047, -0.0512,  0.0018],\n",
            "         [-0.3214,  0.1434,  0.1911,  ..., -0.0899, -0.0293, -0.0033],\n",
            "         [-0.3080,  0.1263,  0.1644,  ..., -0.1097, -0.0658, -0.0421]],\n",
            "\n",
            "        [[ 0.1839,  0.0561,  0.0714,  ...,  0.0040, -0.0402, -0.0853],\n",
            "         [-0.0760,  0.3383, -0.1582,  ...,  0.0192,  0.0458,  0.1340],\n",
            "         [ 0.1105,  0.0970, -0.1909,  ...,  0.1115,  0.1773,  0.1458],\n",
            "         ...,\n",
            "         [ 0.0927,  0.1321, -0.1098,  ..., -0.0477, -0.0704, -0.1185],\n",
            "         [ 0.1813,  0.1167, -0.0912,  ...,  0.0084,  0.0158, -0.0093],\n",
            "         [ 0.1427,  0.1174, -0.0957,  ..., -0.0027,  0.0348, -0.0071]],\n",
            "\n",
            "        [[ 0.0273, -0.1536,  0.1443,  ...,  0.1139,  0.2208,  0.1686],\n",
            "         [ 0.1039, -0.0642, -0.0207,  ..., -0.1144, -0.0648, -0.0718],\n",
            "         [-0.1083,  0.1163,  0.0576,  ..., -0.1268, -0.1264, -0.1703],\n",
            "         ...,\n",
            "         [ 0.0607, -0.1167, -0.0665,  ..., -0.0520, -0.1149, -0.0383],\n",
            "         [ 0.1117, -0.0946, -0.0501,  ..., -0.0155, -0.0864,  0.0028],\n",
            "         [ 0.1539, -0.0947, -0.1150,  ..., -0.0322, -0.0933, -0.0226]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1502, -0.0083,  0.2056,  ...,  0.1414,  0.0665,  0.1042],\n",
            "         [-0.0572,  0.0778, -0.0282,  ...,  0.1865,  0.1990,  0.1978],\n",
            "         [ 0.0831, -0.2428,  0.1344,  ...,  0.0601,  0.1396,  0.1004],\n",
            "         ...,\n",
            "         [ 0.0451, -0.1620, -0.0942,  ..., -0.0899, -0.1222, -0.0252],\n",
            "         [ 0.0934, -0.0725, -0.0767,  ..., -0.0263, -0.0850,  0.0612],\n",
            "         [ 0.0922, -0.1901, -0.0159,  ..., -0.0121, -0.0051,  0.0478]],\n",
            "\n",
            "        [[ 0.0177, -0.1060,  0.1134,  ...,  0.1325,  0.0514,  0.1551],\n",
            "         [ 0.1094, -0.2398, -0.1312,  ..., -0.2537, -0.2903, -0.3141],\n",
            "         [ 0.0447, -0.1028, -0.0521,  ..., -0.1026, -0.1157, -0.1600],\n",
            "         ...,\n",
            "         [-0.3378,  0.0253, -0.1801,  ..., -0.0943,  0.0169, -0.1652],\n",
            "         [-0.2664, -0.0716, -0.1450,  ..., -0.1307, -0.0468, -0.2182],\n",
            "         [-0.2878,  0.0230, -0.1025,  ..., -0.0362,  0.0620, -0.0614]],\n",
            "\n",
            "        [[ 0.1586,  0.1709, -0.2980,  ...,  0.1109,  0.1672,  0.0714],\n",
            "         [ 0.2188,  0.2937, -0.2243,  ..., -0.2324, -0.1619, -0.2466],\n",
            "         [-0.0393,  0.2665, -0.3206,  ..., -0.1381, -0.0682, -0.0354],\n",
            "         ...,\n",
            "         [-0.0068, -0.0391, -0.0682,  ..., -0.0232, -0.0808, -0.0638],\n",
            "         [-0.0732,  0.0380, -0.0243,  ..., -0.0132, -0.0573, -0.0248],\n",
            "         [-0.0289, -0.0726, -0.0775,  ..., -0.0523, -0.1519, -0.1027]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores with mask\n",
            "tensor([[[-0.1826,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0308, -0.0324,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0384,  0.0530, -0.0412,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.3290,  0.1791,  0.2152,  ..., -0.1047,    -inf,    -inf],\n",
            "         [-0.3214,  0.1434,  0.1911,  ..., -0.0899, -0.0293,    -inf],\n",
            "         [-0.3080,  0.1263,  0.1644,  ..., -0.1097, -0.0658, -0.0421]],\n",
            "\n",
            "        [[ 0.1839,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0760,  0.3383,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1105,  0.0970, -0.1909,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0927,  0.1321, -0.1098,  ..., -0.0477,    -inf,    -inf],\n",
            "         [ 0.1813,  0.1167, -0.0912,  ...,  0.0084,  0.0158,    -inf],\n",
            "         [ 0.1427,  0.1174, -0.0957,  ..., -0.0027,  0.0348, -0.0071]],\n",
            "\n",
            "        [[ 0.0273,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1039, -0.0642,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.1083,  0.1163,  0.0576,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0607, -0.1167, -0.0665,  ..., -0.0520,    -inf,    -inf],\n",
            "         [ 0.1117, -0.0946, -0.0501,  ..., -0.0155, -0.0864,    -inf],\n",
            "         [ 0.1539, -0.0947, -0.1150,  ..., -0.0322, -0.0933, -0.0226]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1502,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0572,  0.0778,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0831, -0.2428,  0.1344,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [ 0.0451, -0.1620, -0.0942,  ..., -0.0899,    -inf,    -inf],\n",
            "         [ 0.0934, -0.0725, -0.0767,  ..., -0.0263, -0.0850,    -inf],\n",
            "         [ 0.0922, -0.1901, -0.0159,  ..., -0.0121, -0.0051,  0.0478]],\n",
            "\n",
            "        [[ 0.0177,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.1094, -0.2398,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.0447, -0.1028, -0.0521,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.3378,  0.0253, -0.1801,  ..., -0.0943,    -inf,    -inf],\n",
            "         [-0.2664, -0.0716, -0.1450,  ..., -0.1307, -0.0468,    -inf],\n",
            "         [-0.2878,  0.0230, -0.1025,  ..., -0.0362,  0.0620, -0.0614]],\n",
            "\n",
            "        [[ 0.1586,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [ 0.2188,  0.2937,    -inf,  ...,    -inf,    -inf,    -inf],\n",
            "         [-0.0393,  0.2665, -0.3206,  ...,    -inf,    -inf,    -inf],\n",
            "         ...,\n",
            "         [-0.0068, -0.0391, -0.0682,  ..., -0.0232,    -inf,    -inf],\n",
            "         [-0.0732,  0.0380, -0.0243,  ..., -0.0132, -0.0573,    -inf],\n",
            "         [-0.0289, -0.0726, -0.0775,  ..., -0.0523, -0.1519, -0.1027]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "cross att\n",
            "x shape = torch.Size([32, 20, 256])\n",
            "x = tensor([[[-0.7582,  0.4434,  1.0275,  ...,  0.3027, -1.5120, -0.4764],\n",
            "         [ 0.7888, -0.3148, -0.0000,  ..., -1.5491, -0.3028,  0.0501],\n",
            "         [ 3.5863,  1.2917,  0.0421,  ...,  1.3544, -0.8862,  1.1153],\n",
            "         ...,\n",
            "         [-1.8408, -1.9825, -0.3418,  ..., -0.3728, -0.0000,  2.6018],\n",
            "         [-1.7912, -1.9435, -0.3399,  ..., -0.2716,  0.4167,  0.5706],\n",
            "         [-1.8050, -1.9696, -0.0000,  ..., -0.0000,  0.0000,  2.6490]],\n",
            "\n",
            "        [[-0.0000, -0.4498, -1.5763,  ..., -0.0000,  3.3500, -0.0585],\n",
            "         [-1.2255, -0.2090, -0.0000,  ...,  0.5402,  2.0096, -0.7952],\n",
            "         [-1.5021,  0.8536, -2.9040,  ..., -0.8206, -1.0369, -0.7770],\n",
            "         ...,\n",
            "         [-1.7862, -2.0668, -0.5411,  ..., -0.3025,  0.5670,  2.6678],\n",
            "         [-1.7904, -2.0508, -0.1513,  ..., -0.0000,  0.5365,  2.6148],\n",
            "         [-1.8314, -2.0259, -0.5635,  ..., -0.3129, -0.0810,  1.8595]],\n",
            "\n",
            "        [[ 0.1940, -0.4792, -0.0000,  ...,  1.7830,  1.4780,  0.8557],\n",
            "         [-0.2704,  0.7233,  0.4519,  ..., -1.3748, -1.0715, -1.1796],\n",
            "         [ 0.0000,  0.4730, -2.1745,  ...,  0.6455,  0.2837, -0.2584],\n",
            "         ...,\n",
            "         [-1.6140, -1.9790, -0.4799,  ..., -0.3269,  0.5291,  2.6457],\n",
            "         [-0.0000, -0.1343, -0.5431,  ..., -0.0000,  0.4803,  2.6059],\n",
            "         [ 0.0278, -1.9865, -0.5200,  ..., -0.3395,  0.5205,  2.6408]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1885,  1.2720, -0.7203,  ...,  0.8704,  0.5803,  0.1606],\n",
            "         [-0.0913, -1.2308, -1.4010,  ...,  0.0067,  1.7893, -0.9090],\n",
            "         [-1.4062,  0.0119,  0.5340,  ...,  0.0216,  2.7590, -0.1478],\n",
            "         ...,\n",
            "         [-1.7234, -2.0947, -0.1603,  ..., -0.3593,  0.4779,  0.4402],\n",
            "         [-1.7474, -2.0672, -0.5950,  ..., -0.3546,  0.5074,  2.5881],\n",
            "         [-1.7814, -2.0928, -0.4442,  ..., -0.3763,  0.0000,  2.6234]],\n",
            "\n",
            "        [[-0.7381,  1.2367,  1.0202,  ...,  0.4472, -0.6616,  2.2454],\n",
            "         [ 1.7815, -2.4521,  0.5221,  ...,  2.7653,  0.3171,  1.2882],\n",
            "         [-1.5791, -0.7731,  0.2621,  ...,  1.2089,  0.4948, -0.0000],\n",
            "         ...,\n",
            "         [-1.7623, -2.0387, -0.3526,  ..., -0.2084,  0.4323,  2.6223],\n",
            "         [-1.7697, -0.1160, -0.3890,  ..., -0.2497,  0.4079,  1.8763],\n",
            "         [-1.8374, -2.1105, -0.0213,  ..., -0.3031,  0.0000,  2.5716]],\n",
            "\n",
            "        [[ 1.3737, -0.0000, -0.2200,  ...,  1.8628,  1.7809, -0.3644],\n",
            "         [-1.9098,  0.9917,  0.0000,  ..., -1.7507, -2.2920, -0.2313],\n",
            "         [-0.2043,  0.0000, -0.8071,  ...,  0.7320,  2.2857,  0.2873],\n",
            "         ...,\n",
            "         [-1.7258, -2.0230, -0.5537,  ..., -0.3304,  0.5253,  0.0000],\n",
            "         [-1.8007, -2.1024, -0.3996,  ..., -0.3504,  0.4524,  1.9297],\n",
            "         [-1.7547, -2.0487, -0.3904,  ..., -0.4453,  0.0000,  2.6418]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Att scores\n",
            "tensor([[[-1.5939e-02, -7.2182e-02,  2.1541e-01,  ..., -1.1354e-01,\n",
            "           3.4760e-02, -3.2053e-02],\n",
            "         [-4.9923e-02, -8.3746e-02,  2.3080e-02,  ..., -2.2041e-02,\n",
            "           7.2034e-02, -1.3579e-01],\n",
            "         [-2.9162e-02, -1.7200e-01, -2.6842e-01,  ...,  3.9103e-02,\n",
            "          -1.2446e-01, -3.3648e-03],\n",
            "         ...,\n",
            "         [ 1.1635e-02,  8.2210e-02,  2.2656e-01,  ..., -1.1056e-01,\n",
            "           1.1760e-01, -7.7680e-02],\n",
            "         [-3.9459e-02,  9.5607e-02,  2.2099e-01,  ..., -9.0253e-02,\n",
            "           4.7612e-02, -1.4986e-01],\n",
            "         [ 1.5234e-02,  1.9261e-01,  2.2788e-01,  ..., -9.9976e-02,\n",
            "           4.6064e-02, -1.4107e-01]],\n",
            "\n",
            "        [[ 2.7979e-01,  4.4624e-02,  2.3631e-01,  ..., -9.7610e-02,\n",
            "          -6.0459e-02, -1.3211e-01],\n",
            "         [-2.2367e-01,  2.0800e-01, -2.2372e-02,  ..., -1.6822e-01,\n",
            "          -2.3124e-01, -2.0445e-01],\n",
            "         [ 7.2391e-02, -1.5361e-01,  1.0443e-01,  ...,  4.6467e-02,\n",
            "           1.8439e-01,  9.8893e-02],\n",
            "         ...,\n",
            "         [-1.2882e-01,  4.2569e-02,  4.1821e-01,  ..., -2.3179e-02,\n",
            "           4.3173e-02,  2.7040e-02],\n",
            "         [-1.2438e-01, -6.9284e-02,  2.6096e-01,  ..., -3.2601e-02,\n",
            "          -4.1838e-02, -2.2687e-02],\n",
            "         [-2.3933e-02,  7.4077e-02,  4.5611e-01,  ..., -1.7212e-01,\n",
            "          -9.9488e-02, -6.7846e-02]],\n",
            "\n",
            "        [[-1.4610e-01,  5.5519e-02, -2.2541e-03,  ...,  3.2552e-02,\n",
            "          -8.1733e-02,  6.1428e-03],\n",
            "         [ 6.2811e-02, -1.2385e-01,  1.0300e-01,  ..., -1.2490e-01,\n",
            "          -1.0831e-01, -1.3548e-01],\n",
            "         [ 6.1323e-02, -8.5341e-02, -7.9435e-02,  ...,  1.3913e-01,\n",
            "           6.7165e-02,  2.7256e-02],\n",
            "         ...,\n",
            "         [ 2.7333e-02, -1.9440e-02,  1.0620e-02,  ..., -9.8985e-02,\n",
            "          -7.2036e-02, -1.7765e-01],\n",
            "         [-1.2255e-02, -4.5171e-02,  6.4496e-02,  ..., -5.7930e-02,\n",
            "          -2.5829e-02, -1.5486e-01],\n",
            "         [-1.0186e-01, -6.6159e-02,  8.1843e-02,  ..., -1.7279e-01,\n",
            "          -1.4095e-01, -2.7766e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.1516e-01,  6.4869e-02,  8.6719e-02,  ...,  4.4557e-02,\n",
            "           1.6029e-01,  9.2115e-02],\n",
            "         [-7.5188e-02,  5.9363e-02,  9.3056e-02,  ..., -2.9944e-02,\n",
            "           3.5378e-02,  7.1129e-02],\n",
            "         [ 3.3767e-02,  1.4224e-01,  5.0870e-02,  ..., -1.4285e-01,\n",
            "           6.5254e-03, -5.7579e-02],\n",
            "         ...,\n",
            "         [-6.4865e-02, -4.0653e-02, -7.2966e-02,  ..., -8.3522e-02,\n",
            "          -1.4810e-01, -1.0706e-01],\n",
            "         [-5.7936e-02,  7.2684e-02,  8.3211e-02,  ..., -1.3445e-01,\n",
            "          -1.6531e-01, -1.1499e-01],\n",
            "         [-1.2030e-02,  1.3642e-01,  1.0986e-02,  ..., -4.2597e-02,\n",
            "          -1.1123e-01, -5.3717e-02]],\n",
            "\n",
            "        [[-4.5052e-02, -2.0294e-01, -3.7373e-01,  ..., -2.0691e-01,\n",
            "          -3.5172e-01, -2.1001e-01],\n",
            "         [-1.5646e-01, -1.8374e-01, -1.9477e-01,  ...,  8.3141e-02,\n",
            "           9.6470e-02,  1.4384e-01],\n",
            "         [-2.2054e-01,  5.6689e-02, -9.1565e-02,  ..., -2.0798e-01,\n",
            "          -1.0116e-01, -1.3685e-01],\n",
            "         ...,\n",
            "         [ 7.7611e-02,  7.2282e-02,  1.2207e-02,  ..., -3.9095e-02,\n",
            "          -1.7222e-01, -5.3432e-02],\n",
            "         [ 1.2621e-01,  6.0924e-02,  2.4293e-02,  ..., -8.5772e-02,\n",
            "          -1.5905e-01, -5.5424e-02],\n",
            "         [ 9.9063e-02,  7.2257e-02,  7.3818e-02,  ..., -8.8935e-02,\n",
            "          -1.5773e-01, -7.4857e-02]],\n",
            "\n",
            "        [[-5.8190e-02,  4.5991e-02,  6.1775e-02,  ..., -3.1169e-02,\n",
            "          -1.1672e-04, -1.1858e-01],\n",
            "         [-3.4248e-01, -5.6704e-03, -1.0279e-01,  ...,  5.6314e-02,\n",
            "           1.6487e-01,  8.4814e-02],\n",
            "         [-2.2096e-01,  4.6458e-02, -8.7891e-02,  ..., -2.5330e-01,\n",
            "          -4.2375e-01, -1.7589e-01],\n",
            "         ...,\n",
            "         [ 1.2788e-01, -4.0146e-02,  1.9223e-01,  ..., -1.0376e-01,\n",
            "          -1.5939e-01, -3.5602e-02],\n",
            "         [-2.8136e-02, -4.7593e-03,  3.0052e-02,  ..., -2.0448e-01,\n",
            "          -2.3210e-01, -1.0512e-01],\n",
            "         [ 6.2517e-02, -1.6109e-02,  7.3276e-02,  ..., -1.1787e-01,\n",
            "          -1.6993e-01, -5.7826e-02]]], grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[451]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Remettre à zéro les gradients\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#print(f\"src shape: {src.shape}\")   # Should be (batch_size, seq_len)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#print(f\"tgt shape: {tgt.shape}\")   # Should be (batch_size, seq_len)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#print(f\"tgt[:, :-1] shape: {tgt[:, :-1].shape}\")  # Should be (batch_size, seq_len - 1)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#print(f\"tgt[:, 1:] shape: {tgt[:, 1:].shape}\")    # Should also be (batch_size, seq_len - 1)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Passer les entrées à travers le modèle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Entrée : src, sortie : tgt décalé d'une position (pour prédire le mot suivant)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Calculer la perte\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Utilisation de la dernière colonne de la sortie (cible) pour le calcul de la perte\u001b[39;00m\n\u001b[32m     29\u001b[39m loss = criterion(output.view(-\u001b[32m1\u001b[39m, output.shape[-\u001b[32m1\u001b[39m]), tgt[:, \u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m))  \u001b[38;5;66;03m# La sortie sans le token de début, et le target sans le token de début\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[450]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     31\u001b[39m src_emb = src_emb[:, :tgt.shape[\u001b[32m1\u001b[39m], :]\n\u001b[32m     33\u001b[39m encoder_out = \u001b[38;5;28mself\u001b[39m.encoder(src_emb, src_mask)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m decoder_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc_out(decoder_out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[449]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_out, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[447]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     24\u001b[39m     src_mask = src_mask.expand(-\u001b[32m1\u001b[39m, max_len-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcross att\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m cross_attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x + cross_attn_out)\n\u001b[32m     28\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[415]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m wei = \u001b[43mwei\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply mask\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores with mask\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n",
            "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "\n",
        "# Supposons que tu as déjà défini le modèle Transformer (comme montré précédemment)\n",
        "model = Transformer(vocab_size=len(sp_en), embed_dim=256, num_layers=6, num_heads=8, ff_dim=512, dropout=dropout_rate)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "# Définir un optimiseur (par exemple Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Définir une fonction de perte (par exemple CrossEntropy pour la traduction)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=sp_en.pad_id())  # Ignorer le PAD token pendant le calcul de la perte\n",
        "\n",
        "# Mettre le modèle en mode entraînement\n",
        "model.train()\n",
        "\n",
        "# Boucle d'entraînement\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0  # Variable pour suivre la perte totale sur un epoch\n",
        "\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()  # Remettre à zéro les gradients\n",
        "        #print(f\"src shape: {src.shape}\")   # Should be (batch_size, seq_len)\n",
        "        #print(f\"tgt shape: {tgt.shape}\")   # Should be (batch_size, seq_len)\n",
        "        #print(f\"tgt[:, :-1] shape: {tgt[:, :-1].shape}\")  # Should be (batch_size, seq_len - 1)\n",
        "        #print(f\"tgt[:, 1:] shape: {tgt[:, 1:].shape}\")    # Should also be (batch_size, seq_len - 1)\n",
        "        # Passer les entrées à travers le modèle\n",
        "        output = model(src, tgt[:, :-1])  # Entrée : src, sortie : tgt décalé d'une position (pour prédire le mot suivant)\n",
        "\n",
        "        # Calculer la perte\n",
        "        # Utilisation de la dernière colonne de la sortie (cible) pour le calcul de la perte\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))  # La sortie sans le token de début, et le target sans le token de début\n",
        "        \n",
        "        # Calculer les gradients et mettre à jour les poids\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:  # Afficher la perte tous les 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {total_loss/100:.4f}\")\n",
        "            total_loss = 0  # Réinitialiser la perte\n",
        "            #print(f\"Last output shape = {output.shape}\")\n",
        "            #print(f\"Last output = {output}\")\n",
        "\n",
        "    # Affichage de la perte à la fin de chaque époque\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncVi1QIZECK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer(\n",
            "  (embedding): Embedding(8000, 256)\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (cross_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (encoder_proj): Linear(in_features=20, out_features=256, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=256, out_features=8000, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if torch.isnan(param).any():\n",
        "        print(f\"⚠️ NaN found in {name}\")\n",
        "    if torch.max(torch.abs(param)) > 1e6:  # Too large?\n",
        "        print(f\"⚠️ Large weights in {name}: {torch.max(torch.abs(param))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding.weight | grad mean: 1.0910219394588694e-09 | grad max: 0.0014179187128320336\n",
            "decoder.layers.0.self_attn.heads.0.key.weight | grad mean: -1.7163264374175924e-06 | grad max: 0.00046530787949450314\n",
            "decoder.layers.0.self_attn.heads.0.query.weight | grad mean: 5.403609293352929e-07 | grad max: 0.0008278907043859363\n",
            "decoder.layers.0.self_attn.heads.0.value.weight | grad mean: -1.4911188372934703e-07 | grad max: 0.00284948805347085\n",
            "decoder.layers.0.self_attn.heads.1.key.weight | grad mean: -1.6824567694584402e-07 | grad max: 0.00024645341909490526\n",
            "decoder.layers.0.self_attn.heads.1.query.weight | grad mean: 8.843265959512792e-07 | grad max: 0.000500938855111599\n",
            "decoder.layers.0.self_attn.heads.1.value.weight | grad mean: 5.182630047784187e-06 | grad max: 0.0029646572656929493\n",
            "decoder.layers.0.self_attn.heads.2.key.weight | grad mean: 1.4277583204602706e-07 | grad max: 0.00034390174550935626\n",
            "decoder.layers.0.self_attn.heads.2.query.weight | grad mean: 3.7206081060503493e-07 | grad max: 0.0007505752728320658\n",
            "decoder.layers.0.self_attn.heads.2.value.weight | grad mean: -4.962373168382328e-06 | grad max: 0.003029735991731286\n",
            "decoder.layers.0.self_attn.heads.3.key.weight | grad mean: -4.6065224523772486e-08 | grad max: 0.0009043036843650043\n",
            "decoder.layers.0.self_attn.heads.3.query.weight | grad mean: 1.5551382830381044e-06 | grad max: 0.0008624177426099777\n",
            "decoder.layers.0.self_attn.heads.3.value.weight | grad mean: 5.2208902161510196e-06 | grad max: 0.003509114496409893\n",
            "decoder.layers.0.self_attn.heads.4.key.weight | grad mean: 2.417490918560361e-07 | grad max: 0.0009551250259391963\n",
            "decoder.layers.0.self_attn.heads.4.query.weight | grad mean: -8.56033784657484e-06 | grad max: 0.0014031463069841266\n",
            "decoder.layers.0.self_attn.heads.4.value.weight | grad mean: -6.497680828942975e-07 | grad max: 0.002511376980692148\n",
            "decoder.layers.0.self_attn.heads.5.key.weight | grad mean: -1.246977490154677e-07 | grad max: 0.00036405707942321897\n",
            "decoder.layers.0.self_attn.heads.5.query.weight | grad mean: 4.745064359212847e-07 | grad max: 0.0004710362700279802\n",
            "decoder.layers.0.self_attn.heads.5.value.weight | grad mean: 5.405594492913224e-06 | grad max: 0.0025798024144023657\n",
            "decoder.layers.0.self_attn.heads.6.key.weight | grad mean: -9.70461201177386e-07 | grad max: 0.00039202504558488727\n",
            "decoder.layers.0.self_attn.heads.6.query.weight | grad mean: -3.690483083573781e-07 | grad max: 0.0004831029218621552\n",
            "decoder.layers.0.self_attn.heads.6.value.weight | grad mean: 5.899428288103081e-06 | grad max: 0.0027538363356143236\n",
            "decoder.layers.0.self_attn.heads.7.key.weight | grad mean: 2.2742048599866393e-07 | grad max: 0.0006908522336743772\n",
            "decoder.layers.0.self_attn.heads.7.query.weight | grad mean: 1.605199031473603e-06 | grad max: 0.001718347892165184\n",
            "decoder.layers.0.self_attn.heads.7.value.weight | grad mean: 2.3785405574017204e-06 | grad max: 0.002634248696267605\n",
            "decoder.layers.0.self_attn.proj.weight | grad mean: -7.384783202724066e-07 | grad max: 0.002702199388295412\n",
            "decoder.layers.0.self_attn.proj.bias | grad mean: -5.165302354726009e-06 | grad max: 0.002136469818651676\n",
            "decoder.layers.0.norm1.weight | grad mean: -5.453703124658205e-07 | grad max: 0.002391712972894311\n",
            "decoder.layers.0.norm1.bias | grad mean: -8.351344149559736e-06 | grad max: 0.0019793263636529446\n",
            "decoder.layers.0.cross_attn.heads.0.key.weight | grad mean: -7.220635696888848e-15 | grad max: 1.679591253404833e-12\n",
            "decoder.layers.0.cross_attn.heads.0.query.weight | grad mean: 2.520010103656433e-16 | grad max: 1.7458914088747113e-12\n",
            "decoder.layers.0.cross_attn.heads.0.value.weight | grad mean: -1.4266117887018481e-06 | grad max: 0.0007963706739246845\n",
            "decoder.layers.0.cross_attn.heads.1.key.weight | grad mean: -1.107968725845378e-17 | grad max: 7.949066209954336e-13\n",
            "decoder.layers.0.cross_attn.heads.1.query.weight | grad mean: 5.590612269456251e-16 | grad max: 1.542691972430954e-12\n",
            "decoder.layers.0.cross_attn.heads.1.value.weight | grad mean: 1.5751408000141964e-07 | grad max: 0.0007338040159083903\n",
            "decoder.layers.0.cross_attn.heads.2.key.weight | grad mean: 5.305191812384705e-16 | grad max: 1.2615944530378065e-12\n",
            "decoder.layers.0.cross_attn.heads.2.query.weight | grad mean: 1.818034460295188e-17 | grad max: 1.6816840888583817e-12\n",
            "decoder.layers.0.cross_attn.heads.2.value.weight | grad mean: -2.8720387490466237e-07 | grad max: 0.0008075898513197899\n",
            "decoder.layers.0.cross_attn.heads.3.key.weight | grad mean: 1.100664046057155e-15 | grad max: 3.7880611733313863e-13\n",
            "decoder.layers.0.cross_attn.heads.3.query.weight | grad mean: 1.4789716713718237e-16 | grad max: 5.217941963925332e-13\n",
            "decoder.layers.0.cross_attn.heads.3.value.weight | grad mean: 2.1392554572230438e-07 | grad max: 0.0003856416733469814\n",
            "decoder.layers.0.cross_attn.heads.4.key.weight | grad mean: 2.2100724367452897e-15 | grad max: 1.075555877000367e-12\n",
            "decoder.layers.0.cross_attn.heads.4.query.weight | grad mean: 4.9756615851893e-16 | grad max: 1.6358975805930154e-12\n",
            "decoder.layers.0.cross_attn.heads.4.value.weight | grad mean: -4.4930402509635314e-08 | grad max: 0.0006070734816603363\n",
            "decoder.layers.0.cross_attn.heads.5.key.weight | grad mean: -4.2083482025570226e-17 | grad max: 1.1552323767732853e-12\n",
            "decoder.layers.0.cross_attn.heads.5.query.weight | grad mean: 2.5476273482038635e-16 | grad max: 9.523973406797004e-13\n",
            "decoder.layers.0.cross_attn.heads.5.value.weight | grad mean: -1.8640573529182802e-08 | grad max: 0.0007708438788540661\n",
            "decoder.layers.0.cross_attn.heads.6.key.weight | grad mean: 1.477111004684502e-15 | grad max: 1.1906788489196574e-12\n",
            "decoder.layers.0.cross_attn.heads.6.query.weight | grad mean: 3.3150026701204093e-16 | grad max: 8.862321683802965e-13\n",
            "decoder.layers.0.cross_attn.heads.6.value.weight | grad mean: -2.9950567181913357e-07 | grad max: 0.0009156465530395508\n",
            "decoder.layers.0.cross_attn.heads.7.key.weight | grad mean: 3.5821451091022725e-15 | grad max: 1.479254761217741e-12\n",
            "decoder.layers.0.cross_attn.heads.7.query.weight | grad mean: 9.193932514025611e-17 | grad max: 1.6483210196066578e-12\n",
            "decoder.layers.0.cross_attn.heads.7.value.weight | grad mean: -2.3614206838828977e-06 | grad max: 0.0009351811022497714\n",
            "decoder.layers.0.cross_attn.proj.weight | grad mean: 1.6127972912727273e-07 | grad max: 0.000995903043076396\n",
            "decoder.layers.0.cross_attn.proj.bias | grad mean: -1.1021067621186376e-05 | grad max: 0.0019609173759818077\n",
            "decoder.layers.0.norm2.weight | grad mean: -1.4656793609901797e-06 | grad max: 0.002411504043266177\n",
            "decoder.layers.0.norm2.bias | grad mean: 7.940627256175503e-06 | grad max: 0.0018450797069817781\n",
            "decoder.layers.0.ff.net.0.weight | grad mean: -8.936360984534986e-08 | grad max: 0.0023909439332783222\n",
            "decoder.layers.0.ff.net.0.bias | grad mean: -8.112856448860839e-06 | grad max: 0.000753016909584403\n",
            "decoder.layers.0.ff.net.2.weight | grad mean: -1.2766960253429716e-06 | grad max: 0.0016854631248861551\n",
            "decoder.layers.0.ff.net.2.bias | grad mean: -6.1905338952783495e-06 | grad max: 0.001180208520963788\n",
            "decoder.layers.0.norm3.weight | grad mean: -1.0933824341918807e-05 | grad max: 0.0014772180002182722\n",
            "decoder.layers.0.norm3.bias | grad mean: 4.8504200094612315e-05 | grad max: 0.0016644158167764544\n",
            "decoder.layers.1.self_attn.heads.0.key.weight | grad mean: -4.5778847379551735e-09 | grad max: 0.00041809037793427706\n",
            "decoder.layers.1.self_attn.heads.0.query.weight | grad mean: 2.48365523702887e-07 | grad max: 0.0005818491918034852\n",
            "decoder.layers.1.self_attn.heads.0.value.weight | grad mean: -3.3918462349902256e-07 | grad max: 0.0015818229876458645\n",
            "decoder.layers.1.self_attn.heads.1.key.weight | grad mean: 4.6556593247260025e-08 | grad max: 0.0002500014961697161\n",
            "decoder.layers.1.self_attn.heads.1.query.weight | grad mean: -4.022907660328201e-07 | grad max: 0.0003315315116196871\n",
            "decoder.layers.1.self_attn.heads.1.value.weight | grad mean: 5.548251920117764e-07 | grad max: 0.00149614829570055\n",
            "decoder.layers.1.self_attn.heads.2.key.weight | grad mean: -2.8814454822168045e-07 | grad max: 0.00024062342708930373\n",
            "decoder.layers.1.self_attn.heads.2.query.weight | grad mean: -3.8041093830543105e-07 | grad max: 0.00033477821853011847\n",
            "decoder.layers.1.self_attn.heads.2.value.weight | grad mean: -3.916015430149855e-06 | grad max: 0.0012493334943428636\n",
            "decoder.layers.1.self_attn.heads.3.key.weight | grad mean: 1.9840666709569632e-07 | grad max: 0.0002490546612534672\n",
            "decoder.layers.1.self_attn.heads.3.query.weight | grad mean: 2.445787572469271e-07 | grad max: 0.0003947452059946954\n",
            "decoder.layers.1.self_attn.heads.3.value.weight | grad mean: 1.5778880424477393e-06 | grad max: 0.0014417333295568824\n",
            "decoder.layers.1.self_attn.heads.4.key.weight | grad mean: 2.5243195977964206e-07 | grad max: 0.00042739565833471715\n",
            "decoder.layers.1.self_attn.heads.4.query.weight | grad mean: -2.3049273067954346e-07 | grad max: 0.0006130394176580012\n",
            "decoder.layers.1.self_attn.heads.4.value.weight | grad mean: 8.653780696477043e-07 | grad max: 0.0012118982849642634\n",
            "decoder.layers.1.self_attn.heads.5.key.weight | grad mean: 1.775814126858677e-07 | grad max: 0.00030069041531533003\n",
            "decoder.layers.1.self_attn.heads.5.query.weight | grad mean: -3.070685181683075e-07 | grad max: 0.00030864670407027006\n",
            "decoder.layers.1.self_attn.heads.5.value.weight | grad mean: 2.580631189630367e-06 | grad max: 0.0018612123094499111\n",
            "decoder.layers.1.self_attn.heads.6.key.weight | grad mean: -1.1208953765162732e-08 | grad max: 0.0002312395954504609\n",
            "decoder.layers.1.self_attn.heads.6.query.weight | grad mean: 1.5323996649385663e-07 | grad max: 0.0002934351796284318\n",
            "decoder.layers.1.self_attn.heads.6.value.weight | grad mean: -1.295744823437417e-06 | grad max: 0.002211215440183878\n",
            "decoder.layers.1.self_attn.heads.7.key.weight | grad mean: -1.9102543546978268e-07 | grad max: 0.0003946857468690723\n",
            "decoder.layers.1.self_attn.heads.7.query.weight | grad mean: -4.7275375436584e-07 | grad max: 0.00048124202294275165\n",
            "decoder.layers.1.self_attn.heads.7.value.weight | grad mean: -1.7485523358118371e-06 | grad max: 0.0018684235401451588\n",
            "decoder.layers.1.self_attn.proj.weight | grad mean: 7.277927238646953e-07 | grad max: 0.0016857075970619917\n",
            "decoder.layers.1.self_attn.proj.bias | grad mean: 6.0337815739330836e-06 | grad max: 0.001195121556520462\n",
            "decoder.layers.1.norm1.weight | grad mean: -3.454767011135118e-06 | grad max: 0.0012887460179626942\n",
            "decoder.layers.1.norm1.bias | grad mean: 6.686587312287884e-06 | grad max: 0.0014382812660187483\n",
            "decoder.layers.1.cross_attn.heads.0.key.weight | grad mean: 4.556700560631602e-16 | grad max: 8.997085845438568e-13\n",
            "decoder.layers.1.cross_attn.heads.0.query.weight | grad mean: 2.7100001231211647e-16 | grad max: 1.6776618072186777e-12\n",
            "decoder.layers.1.cross_attn.heads.0.value.weight | grad mean: 1.4126595715424628e-07 | grad max: 0.0005216080462560058\n",
            "decoder.layers.1.cross_attn.heads.1.key.weight | grad mean: 5.819036938298238e-17 | grad max: 3.6333116419998146e-13\n",
            "decoder.layers.1.cross_attn.heads.1.query.weight | grad mean: 4.552652801934911e-16 | grad max: 6.686093193854215e-13\n",
            "decoder.layers.1.cross_attn.heads.1.value.weight | grad mean: 8.888947604646091e-07 | grad max: 0.0006791269988752902\n",
            "decoder.layers.1.cross_attn.heads.2.key.weight | grad mean: 2.021354131371742e-16 | grad max: 4.96846812823859e-13\n",
            "decoder.layers.1.cross_attn.heads.2.query.weight | grad mean: -1.9340276814877838e-18 | grad max: 5.728664070892009e-13\n",
            "decoder.layers.1.cross_attn.heads.2.value.weight | grad mean: -2.3905383272904146e-07 | grad max: 0.0007069580024108291\n",
            "decoder.layers.1.cross_attn.heads.3.key.weight | grad mean: 4.960506842273941e-17 | grad max: 4.886146825686111e-13\n",
            "decoder.layers.1.cross_attn.heads.3.query.weight | grad mean: 3.891243419302246e-16 | grad max: 5.556400066615563e-13\n",
            "decoder.layers.1.cross_attn.heads.3.value.weight | grad mean: 5.705851435777731e-07 | grad max: 0.0005780550418421626\n",
            "decoder.layers.1.cross_attn.heads.4.key.weight | grad mean: -1.0613214303000016e-16 | grad max: 8.252945310656401e-13\n",
            "decoder.layers.1.cross_attn.heads.4.query.weight | grad mean: -9.598358982604017e-17 | grad max: 1.5444365621467004e-12\n",
            "decoder.layers.1.cross_attn.heads.4.value.weight | grad mean: 4.422296058237407e-07 | grad max: 0.00046332646161317825\n",
            "decoder.layers.1.cross_attn.heads.5.key.weight | grad mean: -8.208795344584029e-16 | grad max: 6.295301194399194e-13\n",
            "decoder.layers.1.cross_attn.heads.5.query.weight | grad mean: 3.83190029101761e-16 | grad max: 5.641458437552482e-13\n",
            "decoder.layers.1.cross_attn.heads.5.value.weight | grad mean: 4.930694217364362e-07 | grad max: 0.0004372294351924211\n",
            "decoder.layers.1.cross_attn.heads.6.key.weight | grad mean: -1.6093051074218758e-15 | grad max: 2.4172979156550367e-12\n",
            "decoder.layers.1.cross_attn.heads.6.query.weight | grad mean: -1.6627611449648578e-16 | grad max: 2.0814606548763548e-12\n",
            "decoder.layers.1.cross_attn.heads.6.value.weight | grad mean: -9.996506378229242e-07 | grad max: 0.0006091052782721817\n",
            "decoder.layers.1.cross_attn.heads.7.key.weight | grad mean: -4.3358981295237194e-16 | grad max: 8.653262026196706e-13\n",
            "decoder.layers.1.cross_attn.heads.7.query.weight | grad mean: -6.934254280938643e-16 | grad max: 8.72198851770839e-13\n",
            "decoder.layers.1.cross_attn.heads.7.value.weight | grad mean: -4.661282559936808e-07 | grad max: 0.0005676813307218254\n",
            "decoder.layers.1.cross_attn.proj.weight | grad mean: -3.6227149280421145e-08 | grad max: 0.000873554206918925\n",
            "decoder.layers.1.cross_attn.proj.bias | grad mean: 7.5160460255574435e-06 | grad max: 0.0016212302725762129\n",
            "decoder.layers.1.norm2.weight | grad mean: -1.444041117792949e-07 | grad max: 0.0013055651215836406\n",
            "decoder.layers.1.norm2.bias | grad mean: -1.065155811375007e-05 | grad max: 0.0014706449583172798\n",
            "decoder.layers.1.ff.net.0.weight | grad mean: 1.5986906021225877e-08 | grad max: 0.0008050905889831483\n",
            "decoder.layers.1.ff.net.0.bias | grad mean: -3.18315755976073e-07 | grad max: 0.0004438667674548924\n",
            "decoder.layers.1.ff.net.2.weight | grad mean: -1.7084647652154672e-07 | grad max: 0.0014871266903355718\n",
            "decoder.layers.1.ff.net.2.bias | grad mean: -1.910184437292628e-07 | grad max: 0.0009473562822677195\n",
            "decoder.layers.1.norm3.weight | grad mean: -2.6303459890186787e-06 | grad max: 0.0013608381850644946\n",
            "decoder.layers.1.norm3.bias | grad mean: -9.025598046719097e-06 | grad max: 0.0012832803186029196\n",
            "decoder.layers.2.self_attn.heads.0.key.weight | grad mean: -5.5650247077210224e-08 | grad max: 0.00019081291975453496\n",
            "decoder.layers.2.self_attn.heads.0.query.weight | grad mean: 1.5838978129067982e-07 | grad max: 0.0001594458008185029\n",
            "decoder.layers.2.self_attn.heads.0.value.weight | grad mean: -1.934304691530997e-06 | grad max: 0.0012786593288183212\n",
            "decoder.layers.2.self_attn.heads.1.key.weight | grad mean: -1.9796536321337044e-07 | grad max: 0.00021205376833677292\n",
            "decoder.layers.2.self_attn.heads.1.query.weight | grad mean: 6.681563036181615e-08 | grad max: 0.0002919751568697393\n",
            "decoder.layers.2.self_attn.heads.1.value.weight | grad mean: 3.933150765078608e-07 | grad max: 0.0011545688612386584\n",
            "decoder.layers.2.self_attn.heads.2.key.weight | grad mean: -1.0561603858150193e-07 | grad max: 0.00016266154125332832\n",
            "decoder.layers.2.self_attn.heads.2.query.weight | grad mean: -1.8567023474247435e-08 | grad max: 0.00017889933951664716\n",
            "decoder.layers.2.self_attn.heads.2.value.weight | grad mean: -1.0821561318152817e-06 | grad max: 0.0016231619520112872\n",
            "decoder.layers.2.self_attn.heads.3.key.weight | grad mean: -2.106442309468548e-08 | grad max: 0.00013210513861849904\n",
            "decoder.layers.2.self_attn.heads.3.query.weight | grad mean: -6.218465387064498e-08 | grad max: 0.00012322174734435976\n",
            "decoder.layers.2.self_attn.heads.3.value.weight | grad mean: 3.913673367605952e-07 | grad max: 0.0010055231396108866\n",
            "decoder.layers.2.self_attn.heads.4.key.weight | grad mean: 1.1487740891880094e-07 | grad max: 0.00015986338257789612\n",
            "decoder.layers.2.self_attn.heads.4.query.weight | grad mean: -1.5819988163912058e-07 | grad max: 0.00015586700465064496\n",
            "decoder.layers.2.self_attn.heads.4.value.weight | grad mean: 1.1986685422016308e-06 | grad max: 0.0011881699319928885\n",
            "decoder.layers.2.self_attn.heads.5.key.weight | grad mean: 3.728678166226018e-08 | grad max: 0.00018747508875094354\n",
            "decoder.layers.2.self_attn.heads.5.query.weight | grad mean: 4.825000132768764e-08 | grad max: 0.00024964805925264955\n",
            "decoder.layers.2.self_attn.heads.5.value.weight | grad mean: -7.969724151735136e-07 | grad max: 0.001181675703264773\n",
            "decoder.layers.2.self_attn.heads.6.key.weight | grad mean: -1.1860322501888731e-08 | grad max: 0.0001903039519675076\n",
            "decoder.layers.2.self_attn.heads.6.query.weight | grad mean: 6.8094081484559865e-09 | grad max: 0.0002207463257946074\n",
            "decoder.layers.2.self_attn.heads.6.value.weight | grad mean: -2.379379111516755e-07 | grad max: 0.0012998658930882812\n",
            "decoder.layers.2.self_attn.heads.7.key.weight | grad mean: -7.601441609494941e-08 | grad max: 0.0001450685813324526\n",
            "decoder.layers.2.self_attn.heads.7.query.weight | grad mean: 1.9593159095165902e-08 | grad max: 0.00019580914522521198\n",
            "decoder.layers.2.self_attn.heads.7.value.weight | grad mean: 6.218978683136811e-07 | grad max: 0.0009121999028138816\n",
            "decoder.layers.2.self_attn.proj.weight | grad mean: 2.506310181615845e-07 | grad max: 0.001559887663461268\n",
            "decoder.layers.2.self_attn.proj.bias | grad mean: 1.0129478141607251e-05 | grad max: 0.0013567989226430655\n",
            "decoder.layers.2.norm1.weight | grad mean: 1.4297802408691496e-06 | grad max: 0.0014056203654035926\n",
            "decoder.layers.2.norm1.bias | grad mean: -4.6719901547476184e-07 | grad max: 0.0015153916319832206\n",
            "decoder.layers.2.cross_attn.heads.0.key.weight | grad mean: -4.4014928903546844e-16 | grad max: 5.653926762536066e-13\n",
            "decoder.layers.2.cross_attn.heads.0.query.weight | grad mean: 3.7032497506150745e-16 | grad max: 9.92966884931501e-13\n",
            "decoder.layers.2.cross_attn.heads.0.value.weight | grad mean: -1.3855685665475903e-07 | grad max: 0.0006563778151758015\n",
            "decoder.layers.2.cross_attn.heads.1.key.weight | grad mean: 2.6829758016368123e-16 | grad max: 7.915669530435265e-13\n",
            "decoder.layers.2.cross_attn.heads.1.query.weight | grad mean: -1.279291859688299e-16 | grad max: 6.45071019170218e-13\n",
            "decoder.layers.2.cross_attn.heads.1.value.weight | grad mean: 2.815074253703642e-07 | grad max: 0.0006704565021209419\n",
            "decoder.layers.2.cross_attn.heads.2.key.weight | grad mean: 8.785723249244326e-16 | grad max: 6.318617504219581e-13\n",
            "decoder.layers.2.cross_attn.heads.2.query.weight | grad mean: -3.0164833779151683e-16 | grad max: 6.803946512104475e-13\n",
            "decoder.layers.2.cross_attn.heads.2.value.weight | grad mean: 6.637997103098314e-07 | grad max: 0.0004546923446469009\n",
            "decoder.layers.2.cross_attn.heads.3.key.weight | grad mean: -1.0194844083923028e-15 | grad max: 3.505994322985928e-13\n",
            "decoder.layers.2.cross_attn.heads.3.query.weight | grad mean: -2.5291244428666854e-16 | grad max: 6.283279018609589e-13\n",
            "decoder.layers.2.cross_attn.heads.3.value.weight | grad mean: -7.221548514735332e-08 | grad max: 0.0005250498070381582\n",
            "decoder.layers.2.cross_attn.heads.4.key.weight | grad mean: -1.2106588975546511e-16 | grad max: 1.0692348699145593e-12\n",
            "decoder.layers.2.cross_attn.heads.4.query.weight | grad mean: 1.4659887738727834e-16 | grad max: 2.1705172381647486e-12\n",
            "decoder.layers.2.cross_attn.heads.4.value.weight | grad mean: 3.681565772239992e-07 | grad max: 0.0006146710366010666\n",
            "decoder.layers.2.cross_attn.heads.5.key.weight | grad mean: 1.6434607581078398e-15 | grad max: 1.1493344253751814e-12\n",
            "decoder.layers.2.cross_attn.heads.5.query.weight | grad mean: -2.0487858227641645e-16 | grad max: 1.509470608403174e-12\n",
            "decoder.layers.2.cross_attn.heads.5.value.weight | grad mean: -8.874175705386733e-07 | grad max: 0.0004499476926866919\n",
            "decoder.layers.2.cross_attn.heads.6.key.weight | grad mean: -8.983154982546279e-16 | grad max: 8.882922067181276e-13\n",
            "decoder.layers.2.cross_attn.heads.6.query.weight | grad mean: 1.1060712926341896e-15 | grad max: 1.0378345318556859e-12\n",
            "decoder.layers.2.cross_attn.heads.6.value.weight | grad mean: 1.8800471934810048e-06 | grad max: 0.00048438861267641187\n",
            "decoder.layers.2.cross_attn.heads.7.key.weight | grad mean: 3.0552660519546273e-15 | grad max: 1.5267729571930011e-12\n",
            "decoder.layers.2.cross_attn.heads.7.query.weight | grad mean: -5.72831653327375e-16 | grad max: 2.749231181251033e-12\n",
            "decoder.layers.2.cross_attn.heads.7.value.weight | grad mean: -6.258933638036979e-08 | grad max: 0.0005193384713493288\n",
            "decoder.layers.2.cross_attn.proj.weight | grad mean: -4.190377467239159e-08 | grad max: 0.0007448457181453705\n",
            "decoder.layers.2.cross_attn.proj.bias | grad mean: 7.1186705099535175e-06 | grad max: 0.0016736615216359496\n",
            "decoder.layers.2.norm2.weight | grad mean: 1.381442416459322e-06 | grad max: 0.0012179521145299077\n",
            "decoder.layers.2.norm2.bias | grad mean: -1.6880447219591588e-05 | grad max: 0.0016882915515452623\n",
            "decoder.layers.2.ff.net.0.weight | grad mean: 9.63682111887465e-08 | grad max: 0.0008272810373455286\n",
            "decoder.layers.2.ff.net.0.bias | grad mean: -1.4387251212610863e-06 | grad max: 0.0003157559549435973\n",
            "decoder.layers.2.ff.net.2.weight | grad mean: -5.994239700157777e-07 | grad max: 0.0013511477736756206\n",
            "decoder.layers.2.ff.net.2.bias | grad mean: -2.405622126389062e-06 | grad max: 0.0010823823977261782\n",
            "decoder.layers.2.norm3.weight | grad mean: -4.3230102164670825e-06 | grad max: 0.0011734955478459597\n",
            "decoder.layers.2.norm3.bias | grad mean: 5.756418886448955e-06 | grad max: 0.0015724904369562864\n",
            "decoder.layers.3.self_attn.heads.0.key.weight | grad mean: -6.63703829673068e-08 | grad max: 0.00014456592907663435\n",
            "decoder.layers.3.self_attn.heads.0.query.weight | grad mean: -2.4606782744740485e-07 | grad max: 0.00021634524455294013\n",
            "decoder.layers.3.self_attn.heads.0.value.weight | grad mean: 9.274820058635669e-07 | grad max: 0.0012514725094661117\n",
            "decoder.layers.3.self_attn.heads.1.key.weight | grad mean: -1.0365209135443365e-07 | grad max: 0.00011148315388709307\n",
            "decoder.layers.3.self_attn.heads.1.query.weight | grad mean: -1.4364403000399761e-09 | grad max: 0.00011960849224124104\n",
            "decoder.layers.3.self_attn.heads.1.value.weight | grad mean: -2.1933408334007254e-07 | grad max: 0.00123024289496243\n",
            "decoder.layers.3.self_attn.heads.2.key.weight | grad mean: 1.2514971103883e-07 | grad max: 0.0001552232715766877\n",
            "decoder.layers.3.self_attn.heads.2.query.weight | grad mean: 1.0683100271080548e-07 | grad max: 0.00016199439414776862\n",
            "decoder.layers.3.self_attn.heads.2.value.weight | grad mean: -1.3879755442758324e-06 | grad max: 0.001431800308637321\n",
            "decoder.layers.3.self_attn.heads.3.key.weight | grad mean: -5.776470857199456e-09 | grad max: 9.480024891672656e-05\n",
            "decoder.layers.3.self_attn.heads.3.query.weight | grad mean: -8.175074128757842e-08 | grad max: 0.00018028297927230597\n",
            "decoder.layers.3.self_attn.heads.3.value.weight | grad mean: 3.624722353379184e-07 | grad max: 0.0009931762469932437\n",
            "decoder.layers.3.self_attn.heads.4.key.weight | grad mean: -7.292668158243032e-08 | grad max: 0.00021100250887684524\n",
            "decoder.layers.3.self_attn.heads.4.query.weight | grad mean: 2.316209020136739e-07 | grad max: 0.00028298282995820045\n",
            "decoder.layers.3.self_attn.heads.4.value.weight | grad mean: 1.4877413150315988e-06 | grad max: 0.0012996188597753644\n",
            "decoder.layers.3.self_attn.heads.5.key.weight | grad mean: 1.9407806917115522e-08 | grad max: 0.00017613137606531382\n",
            "decoder.layers.3.self_attn.heads.5.query.weight | grad mean: -7.252532441270887e-08 | grad max: 0.0001669951161602512\n",
            "decoder.layers.3.self_attn.heads.5.value.weight | grad mean: -3.024348188773729e-06 | grad max: 0.0010239507537335157\n",
            "decoder.layers.3.self_attn.heads.6.key.weight | grad mean: 3.768359135847277e-08 | grad max: 0.00010342076711822301\n",
            "decoder.layers.3.self_attn.heads.6.query.weight | grad mean: -1.510718590225224e-07 | grad max: 0.00012537547445390373\n",
            "decoder.layers.3.self_attn.heads.6.value.weight | grad mean: 9.68443828242016e-07 | grad max: 0.0011099579278379679\n",
            "decoder.layers.3.self_attn.heads.7.key.weight | grad mean: -2.015044913150632e-07 | grad max: 0.00028816514532081783\n",
            "decoder.layers.3.self_attn.heads.7.query.weight | grad mean: 1.6127293633871886e-07 | grad max: 0.00020562206918839365\n",
            "decoder.layers.3.self_attn.heads.7.value.weight | grad mean: -1.5259495285135927e-06 | grad max: 0.0013136467896401882\n",
            "decoder.layers.3.self_attn.proj.weight | grad mean: 2.270710126595077e-07 | grad max: 0.0012749653542414308\n",
            "decoder.layers.3.self_attn.proj.bias | grad mean: 8.175735274562612e-06 | grad max: 0.001486693974584341\n",
            "decoder.layers.3.norm1.weight | grad mean: 9.849845810094848e-07 | grad max: 0.0012897023698315024\n",
            "decoder.layers.3.norm1.bias | grad mean: -1.025709934765473e-06 | grad max: 0.0016570283332839608\n",
            "decoder.layers.3.cross_attn.heads.0.key.weight | grad mean: -1.834468170021342e-15 | grad max: 5.973905181297368e-13\n",
            "decoder.layers.3.cross_attn.heads.0.query.weight | grad mean: 3.174514579582199e-16 | grad max: 6.719591246276413e-13\n",
            "decoder.layers.3.cross_attn.heads.0.value.weight | grad mean: -6.023628884577192e-07 | grad max: 0.0004471490974538028\n",
            "decoder.layers.3.cross_attn.heads.1.key.weight | grad mean: 2.689647774283216e-16 | grad max: 5.941570477806246e-13\n",
            "decoder.layers.3.cross_attn.heads.1.query.weight | grad mean: -1.0099994393280676e-17 | grad max: 1.1629990590358852e-12\n",
            "decoder.layers.3.cross_attn.heads.1.value.weight | grad mean: -2.2298159763067815e-07 | grad max: 0.0005101742572151124\n",
            "decoder.layers.3.cross_attn.heads.2.key.weight | grad mean: 3.141555204115554e-16 | grad max: 3.4750839900989094e-13\n",
            "decoder.layers.3.cross_attn.heads.2.query.weight | grad mean: 9.925406344472783e-17 | grad max: 3.989336801440285e-13\n",
            "decoder.layers.3.cross_attn.heads.2.value.weight | grad mean: 5.345734734873986e-07 | grad max: 0.0005617317510768771\n",
            "decoder.layers.3.cross_attn.heads.3.key.weight | grad mean: 8.325798123170634e-16 | grad max: 1.0026593270928696e-12\n",
            "decoder.layers.3.cross_attn.heads.3.query.weight | grad mean: 4.5851746319447396e-17 | grad max: 8.189820349668864e-13\n",
            "decoder.layers.3.cross_attn.heads.3.value.weight | grad mean: -1.2196070429126848e-06 | grad max: 0.00052583625074476\n",
            "decoder.layers.3.cross_attn.heads.4.key.weight | grad mean: 3.0539534685237383e-17 | grad max: 5.893773967133309e-13\n",
            "decoder.layers.3.cross_attn.heads.4.query.weight | grad mean: 3.851167643589685e-17 | grad max: 6.730380142094816e-13\n",
            "decoder.layers.3.cross_attn.heads.4.value.weight | grad mean: -1.1648842246358981e-06 | grad max: 0.00041295646224170923\n",
            "decoder.layers.3.cross_attn.heads.5.key.weight | grad mean: -2.492506149765698e-16 | grad max: 8.82060050000355e-13\n",
            "decoder.layers.3.cross_attn.heads.5.query.weight | grad mean: -1.7298063438388775e-15 | grad max: 1.1727778036901837e-12\n",
            "decoder.layers.3.cross_attn.heads.5.value.weight | grad mean: -3.93983100366313e-07 | grad max: 0.0004180590040050447\n",
            "decoder.layers.3.cross_attn.heads.6.key.weight | grad mean: 1.7356380068620457e-17 | grad max: 5.332821315616465e-13\n",
            "decoder.layers.3.cross_attn.heads.6.query.weight | grad mean: 1.1840787389720152e-16 | grad max: 5.319932320189957e-13\n",
            "decoder.layers.3.cross_attn.heads.6.value.weight | grad mean: -3.936840187179769e-08 | grad max: 0.0005346532561816275\n",
            "decoder.layers.3.cross_attn.heads.7.key.weight | grad mean: -2.901856526745612e-15 | grad max: 8.777778308897977e-13\n",
            "decoder.layers.3.cross_attn.heads.7.query.weight | grad mean: -3.440339723512404e-16 | grad max: 8.078330213968921e-13\n",
            "decoder.layers.3.cross_attn.heads.7.value.weight | grad mean: -3.146223832573014e-07 | grad max: 0.000458411785075441\n",
            "decoder.layers.3.cross_attn.proj.weight | grad mean: 1.4630319356001564e-07 | grad max: 0.0007023068028502166\n",
            "decoder.layers.3.cross_attn.proj.bias | grad mean: 1.1489924872876145e-05 | grad max: 0.0018214117735624313\n",
            "decoder.layers.3.norm2.weight | grad mean: 1.1167558113811538e-06 | grad max: 0.001288978150114417\n",
            "decoder.layers.3.norm2.bias | grad mean: -1.2495715054683387e-05 | grad max: 0.00171602051705122\n",
            "decoder.layers.3.ff.net.0.weight | grad mean: -2.0612738182990142e-09 | grad max: 0.0005578101263381541\n",
            "decoder.layers.3.ff.net.0.bias | grad mean: -2.9447560336848255e-06 | grad max: 0.00023809002595953643\n",
            "decoder.layers.3.ff.net.2.weight | grad mean: 1.043676661538484e-06 | grad max: 0.0012688649585470557\n",
            "decoder.layers.3.ff.net.2.bias | grad mean: 3.501525497995317e-06 | grad max: 0.0008975486853159964\n",
            "decoder.layers.3.norm3.weight | grad mean: -2.469054379616864e-05 | grad max: 0.0011867322027683258\n",
            "decoder.layers.3.norm3.bias | grad mean: 1.7149750419775955e-06 | grad max: 0.0013907335232943296\n",
            "decoder.layers.4.self_attn.heads.0.key.weight | grad mean: 1.3285226430070907e-07 | grad max: 0.00024142011534422636\n",
            "decoder.layers.4.self_attn.heads.0.query.weight | grad mean: 3.053586965506838e-07 | grad max: 0.00031660229433327913\n",
            "decoder.layers.4.self_attn.heads.0.value.weight | grad mean: -4.056369107274804e-08 | grad max: 0.0011637551942840219\n",
            "decoder.layers.4.self_attn.heads.1.key.weight | grad mean: 1.4560118444251202e-08 | grad max: 0.00013461658090818673\n",
            "decoder.layers.4.self_attn.heads.1.query.weight | grad mean: 2.4742384141518414e-08 | grad max: 0.0001550931774545461\n",
            "decoder.layers.4.self_attn.heads.1.value.weight | grad mean: 9.74163185674115e-07 | grad max: 0.0013871680712327361\n",
            "decoder.layers.4.self_attn.heads.2.key.weight | grad mean: -5.982212059052472e-09 | grad max: 0.00021505719632841647\n",
            "decoder.layers.4.self_attn.heads.2.query.weight | grad mean: 2.823625777637062e-10 | grad max: 0.00030803459230810404\n",
            "decoder.layers.4.self_attn.heads.2.value.weight | grad mean: 1.1364261354174232e-06 | grad max: 0.0013073391746729612\n",
            "decoder.layers.4.self_attn.heads.3.key.weight | grad mean: 5.5860596148704644e-08 | grad max: 0.00016746802430134267\n",
            "decoder.layers.4.self_attn.heads.3.query.weight | grad mean: -7.056383566350632e-08 | grad max: 0.00015723133401479572\n",
            "decoder.layers.4.self_attn.heads.3.value.weight | grad mean: -1.416515033270116e-06 | grad max: 0.0014773888979107141\n",
            "decoder.layers.4.self_attn.heads.4.key.weight | grad mean: 2.446380875653631e-08 | grad max: 0.00019699808035511523\n",
            "decoder.layers.4.self_attn.heads.4.query.weight | grad mean: 4.5785185420754715e-08 | grad max: 0.00011776486644521356\n",
            "decoder.layers.4.self_attn.heads.4.value.weight | grad mean: -4.4681542021862697e-07 | grad max: 0.0015440600691363215\n",
            "decoder.layers.4.self_attn.heads.5.key.weight | grad mean: -9.622439733902866e-08 | grad max: 0.0002724445366766304\n",
            "decoder.layers.4.self_attn.heads.5.query.weight | grad mean: 1.5164192745942273e-07 | grad max: 0.00016745904576964676\n",
            "decoder.layers.4.self_attn.heads.5.value.weight | grad mean: -8.818150831757521e-07 | grad max: 0.0014890814200043678\n",
            "decoder.layers.4.self_attn.heads.6.key.weight | grad mean: 8.493059056036145e-08 | grad max: 0.0001692196965450421\n",
            "decoder.layers.4.self_attn.heads.6.query.weight | grad mean: 3.590000829944984e-08 | grad max: 0.00027295437757857144\n",
            "decoder.layers.4.self_attn.heads.6.value.weight | grad mean: -5.954664175078506e-07 | grad max: 0.0011830917792394757\n",
            "decoder.layers.4.self_attn.heads.7.key.weight | grad mean: -3.613618559938914e-08 | grad max: 0.0006475219852291048\n",
            "decoder.layers.4.self_attn.heads.7.query.weight | grad mean: -3.182879027008312e-07 | grad max: 0.000635109725408256\n",
            "decoder.layers.4.self_attn.heads.7.value.weight | grad mean: -9.641547649152926e-07 | grad max: 0.0012299640802666545\n",
            "decoder.layers.4.self_attn.proj.weight | grad mean: -3.744333980648662e-07 | grad max: 0.001950284349732101\n",
            "decoder.layers.4.self_attn.proj.bias | grad mean: -8.944715773395728e-06 | grad max: 0.001283525605686009\n",
            "decoder.layers.4.norm1.weight | grad mean: -5.615719601337332e-06 | grad max: 0.0011580465361475945\n",
            "decoder.layers.4.norm1.bias | grad mean: 9.891949048324022e-06 | grad max: 0.0016411254182457924\n",
            "decoder.layers.4.cross_attn.heads.0.key.weight | grad mean: -1.155718752408961e-16 | grad max: 1.0146810692016062e-12\n",
            "decoder.layers.4.cross_attn.heads.0.query.weight | grad mean: -8.207773081695812e-17 | grad max: 9.031632863462646e-13\n",
            "decoder.layers.4.cross_attn.heads.0.value.weight | grad mean: 2.489067583155702e-07 | grad max: 0.0006813060608692467\n",
            "decoder.layers.4.cross_attn.heads.1.key.weight | grad mean: 1.4510756471056282e-15 | grad max: 5.361593330768799e-13\n",
            "decoder.layers.4.cross_attn.heads.1.query.weight | grad mean: 1.3052031269404e-16 | grad max: 5.432707777565382e-13\n",
            "decoder.layers.4.cross_attn.heads.1.value.weight | grad mean: 3.524044132063864e-06 | grad max: 0.00048187095671892166\n",
            "decoder.layers.4.cross_attn.heads.2.key.weight | grad mean: -9.485371404885194e-16 | grad max: 9.937147675900815e-13\n",
            "decoder.layers.4.cross_attn.heads.2.query.weight | grad mean: 1.3952762929140671e-15 | grad max: 1.0028362688874193e-12\n",
            "decoder.layers.4.cross_attn.heads.2.value.weight | grad mean: -6.486448000941891e-08 | grad max: 0.0004682251310441643\n",
            "decoder.layers.4.cross_attn.heads.3.key.weight | grad mean: -2.5039564470258e-15 | grad max: 1.1123134748339258e-12\n",
            "decoder.layers.4.cross_attn.heads.3.query.weight | grad mean: -9.556087009277904e-16 | grad max: 5.873583412176198e-13\n",
            "decoder.layers.4.cross_attn.heads.3.value.weight | grad mean: -8.988914714791463e-07 | grad max: 0.0005446812720037997\n",
            "decoder.layers.4.cross_attn.heads.4.key.weight | grad mean: -8.226840322734098e-16 | grad max: 1.1744209120825855e-12\n",
            "decoder.layers.4.cross_attn.heads.4.query.weight | grad mean: -1.7521655784266676e-15 | grad max: 7.211743680533844e-13\n",
            "decoder.layers.4.cross_attn.heads.4.value.weight | grad mean: 2.7084746534455917e-07 | grad max: 0.0006686466513201594\n",
            "decoder.layers.4.cross_attn.heads.5.key.weight | grad mean: 3.910851702635591e-17 | grad max: 9.32396087421905e-13\n",
            "decoder.layers.4.cross_attn.heads.5.query.weight | grad mean: 3.936320924568344e-17 | grad max: 6.822490163961581e-13\n",
            "decoder.layers.4.cross_attn.heads.5.value.weight | grad mean: 3.2404673788732907e-07 | grad max: 0.000489058147650212\n",
            "decoder.layers.4.cross_attn.heads.6.key.weight | grad mean: -6.776750621979074e-16 | grad max: 6.882208019622083e-13\n",
            "decoder.layers.4.cross_attn.heads.6.query.weight | grad mean: -7.175179038499736e-17 | grad max: 7.312015034253994e-13\n",
            "decoder.layers.4.cross_attn.heads.6.value.weight | grad mean: 1.0780814818645013e-06 | grad max: 0.000739307957701385\n",
            "decoder.layers.4.cross_attn.heads.7.key.weight | grad mean: -1.495961828804765e-16 | grad max: 1.2201399829728232e-12\n",
            "decoder.layers.4.cross_attn.heads.7.query.weight | grad mean: -9.8682527963568e-17 | grad max: 8.538731787605114e-13\n",
            "decoder.layers.4.cross_attn.heads.7.value.weight | grad mean: 1.472628241572238e-08 | grad max: 0.0004676823446061462\n",
            "decoder.layers.4.cross_attn.proj.weight | grad mean: 2.043850244604073e-08 | grad max: 0.0008792605949565768\n",
            "decoder.layers.4.cross_attn.proj.bias | grad mean: 6.012360245222226e-07 | grad max: 0.0017108055762946606\n",
            "decoder.layers.4.norm2.weight | grad mean: -2.8955105335626286e-07 | grad max: 0.0011466556461527944\n",
            "decoder.layers.4.norm2.bias | grad mean: -7.584088962175883e-06 | grad max: 0.0014312751591205597\n",
            "decoder.layers.4.ff.net.0.weight | grad mean: -5.388951507256934e-08 | grad max: 0.0005762439104728401\n",
            "decoder.layers.4.ff.net.0.bias | grad mean: 2.4327641767740715e-06 | grad max: 0.0002487602469045669\n",
            "decoder.layers.4.ff.net.2.weight | grad mean: 1.3879184734832961e-06 | grad max: 0.0015930215595290065\n",
            "decoder.layers.4.ff.net.2.bias | grad mean: 4.312570126785431e-06 | grad max: 0.0011778553016483784\n",
            "decoder.layers.4.norm3.weight | grad mean: -5.947191311861388e-06 | grad max: 0.0021905817557126284\n",
            "decoder.layers.4.norm3.bias | grad mean: 1.804968633223325e-06 | grad max: 0.0019250912591814995\n",
            "decoder.layers.5.self_attn.heads.0.key.weight | grad mean: 2.785219521683757e-07 | grad max: 0.000760778842959553\n",
            "decoder.layers.5.self_attn.heads.0.query.weight | grad mean: -2.471289803906984e-07 | grad max: 0.0005180192529223859\n",
            "decoder.layers.5.self_attn.heads.0.value.weight | grad mean: -1.0191298542849836e-06 | grad max: 0.002307161223143339\n",
            "decoder.layers.5.self_attn.heads.1.key.weight | grad mean: -2.530384790588869e-10 | grad max: 0.00014862850366625935\n",
            "decoder.layers.5.self_attn.heads.1.query.weight | grad mean: -4.415880994201871e-09 | grad max: 0.0002722055069170892\n",
            "decoder.layers.5.self_attn.heads.1.value.weight | grad mean: -1.1292669910289987e-07 | grad max: 0.0016616621287539601\n",
            "decoder.layers.5.self_attn.heads.2.key.weight | grad mean: 4.699086844084377e-07 | grad max: 0.0007924657547846437\n",
            "decoder.layers.5.self_attn.heads.2.query.weight | grad mean: -2.580549960384815e-07 | grad max: 0.0009106264915317297\n",
            "decoder.layers.5.self_attn.heads.2.value.weight | grad mean: -3.234511041227961e-07 | grad max: 0.0015346742002293468\n",
            "decoder.layers.5.self_attn.heads.3.key.weight | grad mean: 5.821678428219457e-08 | grad max: 0.00033179804449900985\n",
            "decoder.layers.5.self_attn.heads.3.query.weight | grad mean: -5.127285618300448e-08 | grad max: 0.00021147183724679053\n",
            "decoder.layers.5.self_attn.heads.3.value.weight | grad mean: -2.548321162976208e-07 | grad max: 0.0014784201048314571\n",
            "decoder.layers.5.self_attn.heads.4.key.weight | grad mean: 4.9393790391150105e-08 | grad max: 0.0003455063560977578\n",
            "decoder.layers.5.self_attn.heads.4.query.weight | grad mean: 1.887664069499806e-07 | grad max: 0.0002989749191328883\n",
            "decoder.layers.5.self_attn.heads.4.value.weight | grad mean: -8.380126246265718e-07 | grad max: 0.0017185896867886186\n",
            "decoder.layers.5.self_attn.heads.5.key.weight | grad mean: -3.804713628596801e-07 | grad max: 0.000311056908685714\n",
            "decoder.layers.5.self_attn.heads.5.query.weight | grad mean: -5.376898570830235e-08 | grad max: 0.00045235565630719066\n",
            "decoder.layers.5.self_attn.heads.5.value.weight | grad mean: 5.411711754277349e-07 | grad max: 0.0016770991496741772\n",
            "decoder.layers.5.self_attn.heads.6.key.weight | grad mean: -3.1833687330617977e-07 | grad max: 0.00021939001453574747\n",
            "decoder.layers.5.self_attn.heads.6.query.weight | grad mean: 1.2812039074105996e-07 | grad max: 0.0002504881704226136\n",
            "decoder.layers.5.self_attn.heads.6.value.weight | grad mean: 1.7289281686316826e-06 | grad max: 0.0017199635040014982\n",
            "decoder.layers.5.self_attn.heads.7.key.weight | grad mean: 1.1843417269119527e-07 | grad max: 0.00045478969695977867\n",
            "decoder.layers.5.self_attn.heads.7.query.weight | grad mean: 1.7008414943120442e-07 | grad max: 0.0004442690988071263\n",
            "decoder.layers.5.self_attn.heads.7.value.weight | grad mean: 4.983689905202482e-09 | grad max: 0.0016856711590662599\n",
            "decoder.layers.5.self_attn.proj.weight | grad mean: 1.0511523669265443e-06 | grad max: 0.0024762428365647793\n",
            "decoder.layers.5.self_attn.proj.bias | grad mean: 8.095939847407863e-06 | grad max: 0.0011221419554203749\n",
            "decoder.layers.5.norm1.weight | grad mean: -8.51215281727491e-06 | grad max: 0.0019654466304928064\n",
            "decoder.layers.5.norm1.bias | grad mean: 3.2204825402004644e-06 | grad max: 0.0017319568432867527\n",
            "decoder.layers.5.cross_attn.heads.0.key.weight | grad mean: 1.0598863976686752e-15 | grad max: 1.5869080138494751e-12\n",
            "decoder.layers.5.cross_attn.heads.0.query.weight | grad mean: -2.7713139264971473e-16 | grad max: 1.3675122232520431e-12\n",
            "decoder.layers.5.cross_attn.heads.0.value.weight | grad mean: 3.9331075640802737e-07 | grad max: 0.000616534030996263\n",
            "decoder.layers.5.cross_attn.heads.1.key.weight | grad mean: -3.009508061594529e-17 | grad max: 7.086568745012789e-13\n",
            "decoder.layers.5.cross_attn.heads.1.query.weight | grad mean: 7.014155428248733e-17 | grad max: 9.642244684984758e-13\n",
            "decoder.layers.5.cross_attn.heads.1.value.weight | grad mean: -3.2722465448387084e-07 | grad max: 0.0007360935560427606\n",
            "decoder.layers.5.cross_attn.heads.2.key.weight | grad mean: 3.2684699622509287e-15 | grad max: 6.69216526812122e-13\n",
            "decoder.layers.5.cross_attn.heads.2.query.weight | grad mean: -1.0631043023050739e-16 | grad max: 5.815264719419289e-13\n",
            "decoder.layers.5.cross_attn.heads.2.value.weight | grad mean: -9.564042784404592e-07 | grad max: 0.0005401129019446671\n",
            "decoder.layers.5.cross_attn.heads.3.key.weight | grad mean: -2.601770117708832e-15 | grad max: 1.9756425228417696e-12\n",
            "decoder.layers.5.cross_attn.heads.3.query.weight | grad mean: -2.798417922018101e-16 | grad max: 2.021956387043833e-12\n",
            "decoder.layers.5.cross_attn.heads.3.value.weight | grad mean: -1.6290302937704837e-06 | grad max: 0.0009318098891526461\n",
            "decoder.layers.5.cross_attn.heads.4.key.weight | grad mean: -4.1961050824582905e-15 | grad max: 1.2415696725928682e-12\n",
            "decoder.layers.5.cross_attn.heads.4.query.weight | grad mean: 4.704418342072424e-17 | grad max: 7.879926095313849e-13\n",
            "decoder.layers.5.cross_attn.heads.4.value.weight | grad mean: 2.6296768282918492e-06 | grad max: 0.0005939949187450111\n",
            "decoder.layers.5.cross_attn.heads.5.key.weight | grad mean: -8.319650252160344e-16 | grad max: 1.5236765842085997e-12\n",
            "decoder.layers.5.cross_attn.heads.5.query.weight | grad mean: 5.693238781345582e-16 | grad max: 1.622815055078719e-12\n",
            "decoder.layers.5.cross_attn.heads.5.value.weight | grad mean: -2.326060894120019e-06 | grad max: 0.0008269291138276458\n",
            "decoder.layers.5.cross_attn.heads.6.key.weight | grad mean: 1.0406853254247227e-15 | grad max: 1.4815865548301055e-12\n",
            "decoder.layers.5.cross_attn.heads.6.query.weight | grad mean: -2.729843193399577e-16 | grad max: 1.4480118493143124e-12\n",
            "decoder.layers.5.cross_attn.heads.6.value.weight | grad mean: -9.249804406863404e-07 | grad max: 0.0007876026211306453\n",
            "decoder.layers.5.cross_attn.heads.7.key.weight | grad mean: 7.856548223138856e-16 | grad max: 1.0063753216188465e-12\n",
            "decoder.layers.5.cross_attn.heads.7.query.weight | grad mean: -4.790357775505253e-17 | grad max: 1.0034942711859007e-12\n",
            "decoder.layers.5.cross_attn.heads.7.value.weight | grad mean: -9.108571248361841e-07 | grad max: 0.0004888558760285378\n",
            "decoder.layers.5.cross_attn.proj.weight | grad mean: -7.636472787453386e-09 | grad max: 0.0009697836940176785\n",
            "decoder.layers.5.cross_attn.proj.bias | grad mean: -4.797821020474657e-07 | grad max: 0.0015778144588693976\n",
            "decoder.layers.5.norm2.weight | grad mean: -9.785253496374935e-07 | grad max: 0.0019725763704627752\n",
            "decoder.layers.5.norm2.bias | grad mean: -6.280612069531344e-06 | grad max: 0.0015789042226970196\n",
            "decoder.layers.5.ff.net.0.weight | grad mean: -1.240098157495595e-07 | grad max: 0.0008557807886973023\n",
            "decoder.layers.5.ff.net.0.bias | grad mean: 4.713257112598512e-06 | grad max: 0.00037164025707170367\n",
            "decoder.layers.5.ff.net.2.weight | grad mean: 7.204271241789684e-07 | grad max: 0.0030744033865630627\n",
            "decoder.layers.5.ff.net.2.bias | grad mean: 7.695025487919338e-07 | grad max: 0.0013411047402769327\n",
            "decoder.layers.5.norm3.weight | grad mean: -0.00029828850529156625 | grad max: 0.0030647278763353825\n",
            "decoder.layers.5.norm3.bias | grad mean: 2.8424910851754248e-05 | grad max: 0.0022124473471194506\n",
            "fc_out.weight | grad mean: -2.9103830890414573e-12 | grad max: 0.02630987949669361\n",
            "fc_out.bias | grad mean: 1.7402926877352343e-09 | grad max: 0.008172914385795593\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"{name} | grad mean: {param.grad.mean().item()} | grad max: {param.grad.max().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n",
            "input tensor = tensor([[ 153,   14, 3190, 7926, 7962,  670,  238,   36,   60,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "decoder input = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "MASK TRIL\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1.]])\n",
            "FULL MASK\n",
            "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 1.]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "self att\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624]]])\n",
            "cross att\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         ...,\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723]]])\n",
            "Att scores\n",
            "tensor([[[-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066]]])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[452]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput tensor = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdecoder input = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the input tensor through the encoder-decoder model\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput print = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[450]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     31\u001b[39m src_emb = src_emb[:, :tgt.shape[\u001b[32m1\u001b[39m], :]\n\u001b[32m     33\u001b[39m encoder_out = \u001b[38;5;28mself\u001b[39m.encoder(src_emb, src_mask)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m decoder_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc_out(decoder_out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[449]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_out, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[447]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     24\u001b[39m     src_mask = src_mask.expand(-\u001b[32m1\u001b[39m, max_len-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcross att\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m cross_attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x + cross_attn_out)\n\u001b[32m     28\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[415]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m wei = \u001b[43mwei\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply mask\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores with mask\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n",
            "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "# 1. Preprocess the input sentence\n",
        "input_sentence = \"Hello, how are you?\"  # Simple English sentence to translate\n",
        "input_tokens = tokenize(input_sentence, sp_en)  # Tokenize using the English SentencePiece processor\n",
        "input_indices = tokens_to_indices(input_tokens, sp_en)  # Convert to indices\n",
        "input_indices = pad_sequence(input_indices, max_len, sp_en)  # Pad the sequence to max_len\n",
        "\n",
        "# 2. Feed the input sentence to the model\n",
        "input_tensor = torch.tensor(input_indices).unsqueeze(0)  # Add batch dimension (shape: [1, max_len])\n",
        "\n",
        "# Create a decoder input (usually just the <sos> token for start of sentence)\n",
        "sos_token = sp_fr.piece_to_id('<sos>')\n",
        "decoder_input = torch.tensor([sos_token] * (max_len-1)).unsqueeze(0)  # Add batch dimension and pad decoder input to max_len\n",
        "print(decoder_input.shape)\n",
        "\n",
        "# Perform the forward pass (assuming model.forward(src, tgt) works with the model)\n",
        "with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "    print(f\"input tensor = {input_tensor}\")\n",
        "    print(f\"decoder input = {decoder_input}\")\n",
        "    output = model(input_tensor, decoder_input)  # Pass the input tensor through the encoder-decoder model\n",
        "print(f\"output shape = {output.shape}\")\n",
        "print(f\"output print = {output}\")\n",
        "# 3. Decode the output tokens\n",
        "output_indices = output.argmax(dim=-1).squeeze().tolist()  # Get the token indices with the highest probability\n",
        "output_tokens = [sp_fr.id_to_piece(idx) for idx in output_indices if idx != sp_fr.piece_to_id('<pad>')]  # Decode tokens (remove <pad>)\n",
        "print(f\"output tokens = {output_tokens}\")\n",
        "\n",
        "second_column = output[0, 1, :]\n",
        "# Print the second column\n",
        "print(second_column)\n",
        "print(output)\n",
        "# 4. Post-process the output\n",
        "translated_sentence = \" \".join(output_tokens)  # Join tokens to form the translated sentence\n",
        "print(\"Translated sentence:\", translated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.11.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
