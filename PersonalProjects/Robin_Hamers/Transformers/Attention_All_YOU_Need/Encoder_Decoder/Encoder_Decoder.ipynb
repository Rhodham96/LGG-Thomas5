{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i30pFFO701R",
        "outputId": "d3fa40ee-2e52-433f-ed19-081d36886b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: pandas in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (2.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/robinhamers/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xlf46XgTtL8w"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# Name: Hamers Robin\n",
        "# GitHub: Rhodham96\n",
        "# Year: 2025\n",
        "# Description: Attention is all you need - Build a GPT from scratch, helped with Andrej Kartpathy video \"Let's build GPT: from scratch, in code, spelled out\"\n",
        "# =======================================================\n",
        "\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import string\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bjy9DkB_sqS"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "dropout_rate = 0.1\n",
        "vocab_size = 8000\n",
        "#max_len = 50 # max seq len\n",
        "n_embd = 384\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gorXBeeC4pa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "H3gjgLBh4sjM",
        "outputId": "3ad4c469-4dfc-45aa-eb41-694c70fbd2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"username\":\"robinhamers\",\"key\":\"f4476bf89c5cffe5f8fad8967f0f7b7c\"}\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/Users/robinhamers/Downloads/kaggle.json\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    contenu = f.read()\n",
        "    print(contenu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ35zeMS4sgE",
        "outputId": "fd837704-6eef-4cdf-b037-61332fefe955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
            "License(s): CC0-1.0\n",
            "Downloading language-translation-englishfrench.zip to /Users/robinhamers/Documents/DataAnalyst/BECODE/2_DataBootcamp/LGG-Thomas5/PersonalProjects/Robin_Hamers/Transformers/Attention_All_YOU_Need/Encoder_Decoder\n",
            " 85%|████████████████████████████████▍     | 3.00M/3.51M [00:00<00:00, 3.98MB/s]\n",
            "100%|██████████████████████████████████████| 3.51M/3.51M [00:01<00:00, 3.57MB/s]\n",
            "Archive:  language-translation-englishfrench.zip\n",
            "  inflating: eng_-french.csv         \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /Users/robinhamers/Downloads/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d devicharith/language-translation-englishfrench\n",
        "!unzip language-translation-englishfrench.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zmSvT1853lg",
        "outputId": "ecd68805-5fba-42df-f5ca-3df7f3c2abd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  English words/sentences French words/sentences\n",
            "0                     Hi.                 Salut!\n",
            "1                    Run!                Cours !\n",
            "2                    Run!               Courez !\n",
            "3                    Who?                  Qui ?\n",
            "4                    Wow!             Ça alors !\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the CSV file\n",
        "df_import = pd.read_csv('eng_-french.csv')\n",
        "print(df_import.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59L0AnMO66ca",
        "outputId": "0a493604-84c3-4077-c5d5-d86bbdcf5299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  english      french\n",
            "0     Hi.      Salut!\n",
            "1    Run!     Cours !\n",
            "2    Run!    Courez !\n",
            "3    Who?       Qui ?\n",
            "4    Wow!  Ça alors !\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "df['english'] = df_import['English words/sentences']\n",
        "df['french'] = df_import['French words/sentences']\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No need if file already created\n",
        "# Create a combined file with English and French sentences for SentencePiece training\n",
        "with open(\"combined_data.txt\", \"w\") as file:\n",
        "    for e, f in zip(df['english'], df['french']):\n",
        "        file.write(e + '\\n' + f + '\\n')  # Add each English-French sentence pair.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "z91dSmQX7znU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: combined_data.txt\n",
            "  input_format: \n",
            "  model_prefix: spm_model\n",
            "  model_type: BPE\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: combined_data.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 351242 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=12087108\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9523% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999523\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 351242 sentences.\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 351242\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 69178\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=190023 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70879 size=20 all=2212 active=1908 piece=▁n\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44616 size=40 all=2904 active=2600 piece=ar\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27972 size=60 all=3960 active=3656 piece=us\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18915 size=80 all=4890 active=4586 piece=ow\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14554 size=100 all=5702 active=5398 piece=ch\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14503 min_freq=1006\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12591 size=120 all=6503 active=1699 piece=ke\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11262 size=140 all=7327 active=2523 piece=tu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9328 size=160 all=8125 active=3321 piece=ad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8148 size=180 all=9086 active=4282 piece=▁ét\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7065 size=200 all=10023 active=5219 piece=oir\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7004 min_freq=951\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6246 size=220 all=10564 active=1505 piece=ang\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5561 size=240 all=11143 active=2084 piece=▁She\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4876 size=260 all=11916 active=2857 piece=ez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4573 size=280 all=12667 active=3608 piece=▁think\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4155 size=300 all=13240 active=4181 piece=▁him\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4144 min_freq=832\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3772 size=320 all=13773 active=1533 piece=▁us\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3489 size=340 all=14421 active=2181 piece=ate\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3189 size=360 all=14908 active=2668 piece=ind\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3046 size=380 all=15407 active=3167 piece=ten\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2886 size=400 all=15919 active=3679 piece=▁Ce\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2879 min_freq=707\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2756 size=420 all=16158 active=1236 piece=mer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2593 size=440 all=16537 active=1615 piece=▁était\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2459 size=460 all=16909 active=1987 piece=ak\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2311 size=480 all=17464 active=2542 piece=▁too\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2170 size=500 all=17841 active=2919 piece=▁there\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2168 min_freq=630\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2103 size=520 all=18252 active=1411 piece=▁imp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2000 size=540 all=18815 active=1974 piece=ign\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1880 size=560 all=19231 active=2390 piece=▁liv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1798 size=580 all=19540 active=2699 piece=▁somet\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1710 size=600 all=19877 active=3036 piece=▁avons\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1708 min_freq=566\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1644 size=620 all=20345 active=1469 piece=▁something\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1561 size=640 all=20721 active=1845 piece=ême\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1520 size=660 all=21001 active=2125 piece=▁heure\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1463 size=680 all=21272 active=2396 piece=rait\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1424 size=700 all=21543 active=2667 piece=▁thought\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1423 min_freq=479\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1382 size=720 all=21822 active=1354 piece=▁bon\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1317 size=740 all=22044 active=1576 piece=▁told\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1292 size=760 all=22324 active=1856 piece=▁avoir\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1256 size=780 all=22523 active=2055 piece=▁dois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1215 size=800 all=22840 active=2372 piece=▁vie\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1212 min_freq=419\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1171 size=820 all=23020 active=1319 piece=▁mo\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1124 size=840 all=23309 active=1608 piece=▁Who\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1091 size=860 all=23524 active=1823 piece=▁fam\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1072 size=880 all=23672 active=1971 piece=rench\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1048 size=900 all=23905 active=2204 piece=▁No\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1047 min_freq=364\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1020 size=920 all=24168 active=1453 piece=▁rest\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=990 size=940 all=24399 active=1684 piece=▁veu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=966 size=960 all=24598 active=1883 piece=▁child\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=940 size=980 all=24989 active=2274 piece=▁underst\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=908 size=1000 all=25270 active=2555 piece=▁père\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=907 min_freq=326\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=891 size=1020 all=25452 active=1446 piece=esterday\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=874 size=1040 all=25732 active=1726 piece=▁tomorrow\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=846 size=1060 all=26077 active=2071 piece=▁arriv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=830 size=1080 all=26221 active=2215 piece=▁inte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=805 size=1100 all=26503 active=2497 piece=▁end\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=804 min_freq=293\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=778 size=1120 all=26710 active=1516 piece=▁sens\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=761 size=1140 all=26886 active=1692 piece=tement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=748 size=1160 all=27172 active=1978 piece=▁why\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=726 size=1180 all=27454 active=2260 piece=▁hope\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=704 size=1200 all=27652 active=2458 piece=▁mont\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=702 min_freq=267\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=687 size=1220 all=27847 active=1553 piece=▁sûr\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=667 size=1240 all=27956 active=1662 piece=▁idea\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=651 size=1260 all=28082 active=1788 piece=▁St\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=634 size=1280 all=28400 active=2106 piece=ious\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=627 size=1300 all=28543 active=2249 piece=▁bed\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=627 min_freq=246\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=617 size=1320 all=28626 active=1507 piece=▁mother\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=606 size=1340 all=28804 active=1685 piece=▁rain\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=593 size=1360 all=29067 active=1948 piece=igh\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=582 size=1380 all=29308 active=2189 piece=▁heard\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=570 size=1400 all=29422 active=2303 piece=ale\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=570 min_freq=226\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=565 size=1420 all=29597 active=1577 piece=▁accident\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=554 size=1440 all=29742 active=1722 piece=jeun\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=540 size=1460 all=29857 active=1837 piece=▁piè\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=525 size=1480 all=30141 active=2121 piece=river\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=518 size=1500 all=30291 active=2271 piece=▁homme\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=518 min_freq=209\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=506 size=1520 all=30512 active=1736 piece=▁conv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=496 size=1540 all=30682 active=1906 piece=ations\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=483 size=1560 all=31010 active=2234 piece=▁acheté\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=474 size=1580 all=31158 active=2382 piece=reak\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=466 size=1600 all=31503 active=2727 piece=▁letter\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=465 min_freq=190\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=459 size=1620 all=31748 active=1820 piece=▁moins\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=1640 all=31885 active=1957 piece=▁ép\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=445 size=1660 all=32055 active=2127 piece=illez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=435 size=1680 all=32262 active=2334 piece=any\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=428 size=1700 all=32428 active=2500 piece=tiful\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=427 min_freq=179\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=423 size=1720 all=32524 active=1716 piece=▁invit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=414 size=1740 all=32626 active=1818 piece=▁â\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=403 size=1760 all=32768 active=1960 piece=▁ey\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=397 size=1780 all=32817 active=2009 piece=▁tennis\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=391 size=1800 all=32918 active=2110 piece=▁rel\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=391 min_freq=168\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=385 size=1820 all=33027 active=1731 piece=▁books\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=379 size=1840 all=33179 active=1883 piece=▁together\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=372 size=1860 all=33370 active=2074 piece=aught\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=366 size=1880 all=33520 active=2224 piece=▁chat\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=360 size=1900 all=33716 active=2420 piece=oyez\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=360 min_freq=158\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=1920 all=33837 active=1798 piece=▁fini\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=351 size=1940 all=33976 active=1937 piece=▁étais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=345 size=1960 all=34087 active=2048 piece=▁nager\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=339 size=1980 all=34237 active=2198 piece=uv\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=336 size=2000 all=34380 active=2341 piece=▁Merci\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=336 min_freq=148\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=333 size=2020 all=34461 active=1800 piece=▁réunion\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=329 size=2040 all=34535 active=1874 piece=▁lorsque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=2060 all=34620 active=1959 piece=▁Voulez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=320 size=2080 all=34682 active=2021 piece=▁fig\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=316 size=2100 all=34716 active=2055 piece=▁men\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=316 min_freq=139\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=312 size=2120 all=34824 active=1829 piece=▁bor\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=307 size=2140 all=34928 active=1933 piece=vision\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=303 size=2160 all=35023 active=2028 piece=▁serais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=2180 all=35204 active=2209 piece=▁supposed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=295 size=2200 all=35312 active=2317 piece=▁regret\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=295 min_freq=131\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=292 size=2220 all=35480 active=1926 piece=▁anybody\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288 size=2240 all=35645 active=2091 piece=oyer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=2260 all=35759 active=2205 piece=▁parlé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=280 size=2280 all=35912 active=2358 piece=▁advice\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=276 size=2300 all=36017 active=2463 piece=vert\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=276 min_freq=124\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=2320 all=36138 active=1900 piece=▁says\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=270 size=2340 all=36258 active=2020 piece=▁App\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=2360 all=36375 active=2137 piece=▁prochaine\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=265 size=2380 all=36530 active=2292 piece=▁lieu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=2400 all=36645 active=2407 piece=▁Tous\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=262 min_freq=117\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=260 size=2420 all=36775 active=1963 piece=▁patient\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=2440 all=36891 active=2079 piece=▁radio\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=254 size=2460 all=36971 active=2159 piece=▁cost\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=251 size=2480 all=37107 active=2295 piece=▁opinion\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=247 size=2500 all=37274 active=2462 piece=fect\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=247 min_freq=111\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=2520 all=37413 active=1989 piece=▁Votre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=242 size=2540 all=37448 active=2024 piece=▁Not\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=239 size=2560 all=37558 active=2134 piece=▁appreciate\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=2580 all=37669 active=2245 piece=▁hos\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=232 size=2600 all=37782 active=2358 piece=▁problèmes\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=231 min_freq=106\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=228 size=2620 all=37848 active=1956 piece=▁rac\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=226 size=2640 all=38031 active=2139 piece=▁lie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=2660 all=38144 active=2252 piece=▁short\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=2680 all=38256 active=2364 piece=▁œ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=218 size=2700 all=38389 active=2497 piece=▁team\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=218 min_freq=101\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=216 size=2720 all=38507 active=2036 piece=▁commencé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=213 size=2740 all=38624 active=2153 piece=▁lose\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=211 size=2760 all=38690 active=2219 piece=apeau\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=209 size=2780 all=38786 active=2315 piece=fois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=207 size=2800 all=38889 active=2418 piece=▁simple\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=207 min_freq=96\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=2820 all=38951 active=2007 piece=▁clothes\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=202 size=2840 all=39096 active=2152 piece=aig\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=2860 all=39183 active=2239 piece=tel\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=198 size=2880 all=39310 active=2366 piece=étud\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=2900 all=39403 active=2459 piece=▁plain\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=197 min_freq=92\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=2920 all=39474 active=2035 piece=ps\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=2940 all=39576 active=2137 piece=lent\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=190 size=2960 all=39751 active=2312 piece=▁Ton\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=2980 all=39857 active=2418 piece=▁lég\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=3000 all=39979 active=2540 piece=▁lucky\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=186 min_freq=87\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=184 size=3020 all=40047 active=2067 piece=hist\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=183 size=3040 all=40199 active=2219 piece=▁nois\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=3060 all=40308 active=2328 piece=set\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=3080 all=40416 active=2436 piece=▁choc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=3100 all=40467 active=2487 piece=▁Here\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=179 min_freq=83\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=3120 all=40544 active=2101 piece=▁Everything\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=176 size=3140 all=40712 active=2269 piece=▁ble\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=174 size=3160 all=40870 active=2427 piece=ove\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=3180 all=40959 active=2516 piece=ned\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=3200 all=41062 active=2619 piece=▁rather\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=171 min_freq=80\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=3220 all=41127 active=2119 piece=ieurs\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=3240 all=41195 active=2187 piece=▁earth\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=165 size=3260 all=41315 active=2307 piece=ortes\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=3280 all=41401 active=2393 piece=▁sense\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=162 size=3300 all=41473 active=2465 piece=ayez\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=162 min_freq=76\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=3320 all=41558 active=2156 piece=▁pluie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=160 size=3340 all=41626 active=2224 piece=▁désormais\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=3360 all=41698 active=2296 piece=▁manque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=3380 all=41712 active=2310 piece=▁embarrass\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=3400 all=41790 active=2388 piece=▁driv\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=73\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=3420 all=41893 active=2191 piece=mon\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=3440 all=42040 active=2338 piece=lu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=150 size=3460 all=42209 active=2507 piece=▁novel\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=148 size=3480 all=42268 active=2566 piece=▁mad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=147 size=3500 all=42362 active=2660 piece=▁tromp\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=147 min_freq=70\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=3520 all=42454 active=2195 piece=▁dent\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=3540 all=42504 active=2245 piece=▁recher\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=3560 all=42600 active=2341 piece=road\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=3580 all=42643 active=2384 piece=▁volé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=3600 all=42705 active=2446 piece=clock\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=140 min_freq=67\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=3620 all=42733 active=2164 piece=étranger\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=3640 all=42847 active=2278 piece=ived\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=3660 all=42884 active=2315 piece=écu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=3680 all=42951 active=2382 piece=▁danser\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=3700 all=43036 active=2467 piece=▁Put\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=133 min_freq=65\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=3720 all=43123 active=2239 piece=itch\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=131 size=3740 all=43211 active=2327 piece=ement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=3760 all=43302 active=2418 piece=▁ship\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=3780 all=43391 active=2507 piece=▁consc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=3800 all=43450 active=2566 piece=▁Lorsque\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=128 min_freq=62\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=127 size=3820 all=43502 active=2225 piece=▁medicine\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=3840 all=43602 active=2325 piece=ained\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3860 all=43670 active=2393 piece=ator\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3880 all=43763 active=2486 piece=▁college\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=3900 all=43839 active=2562 piece=▁excell\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=123 min_freq=60\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=3920 all=43878 active=2226 piece=▁sudden\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121 size=3940 all=43929 active=2277 piece=▁triste\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=3960 all=43987 active=2335 piece=▁garçons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=3980 all=44026 active=2374 piece=rom\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=4000 all=44110 active=2458 piece=▁ice\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=117 min_freq=58\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=4020 all=44143 active=2236 piece=dge\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=4040 all=44197 active=2290 piece=lax\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=4060 all=44263 active=2356 piece=uellement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=4080 all=44331 active=2424 piece=▁tempête\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=4100 all=44426 active=2519 piece=▁egg\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=112 min_freq=56\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=4120 all=44486 active=2279 piece=so\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=4140 all=44570 active=2363 piece=▁continuer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=4160 all=44705 active=2498 piece=▁volont\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=4180 all=44739 active=2532 piece=come\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=4200 all=44847 active=2640 piece=▁erreurs\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=108 min_freq=54\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=4220 all=44925 active=2321 piece=▁Quels\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=4240 all=44985 active=2381 piece=▁dès\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=4260 all=45029 active=2425 piece=▁anywhere\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=4280 all=45082 active=2478 piece=▁risque\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=4300 all=45123 active=2519 piece=▁decide\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=104 min_freq=53\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=4320 all=45189 active=2323 piece=▁mail\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=4340 all=45223 active=2357 piece=adm\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=4360 all=45340 active=2474 piece=▁continue\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=4380 all=45402 active=2536 piece=▁rends\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=4400 all=45478 active=2612 piece=▁avions\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=100 min_freq=51\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=4420 all=45537 active=2333 piece=oie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=4440 all=45571 active=2367 piece=▁superm\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=4460 all=45608 active=2404 piece=▁œil\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=4480 all=45656 active=2452 piece=nez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=4500 all=45751 active=2547 piece=▁herself\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=97 min_freq=49\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=4520 all=45839 active=2376 piece=▁failed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=95 size=4540 all=45940 active=2477 piece=▁censé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=94 size=4560 all=46015 active=2552 piece=▁bird\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=4580 all=46055 active=2592 piece=alif\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=4600 all=46094 active=2631 piece=▁jealous\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=93 min_freq=48\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=4620 all=46152 active=2362 piece=▁seemed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=4640 all=46262 active=2472 piece=▁Didn\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=4660 all=46323 active=2533 piece=▁rom\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=4680 all=46387 active=2597 piece=▁montagne\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=4700 all=46500 active=2710 piece=▁pier\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=89 min_freq=46\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=4720 all=46540 active=2362 piece=apprendre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=4740 all=46674 active=2496 piece=▁prêts\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=4760 all=46718 active=2540 piece=▁loi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=4780 all=46752 active=2574 piece=▁ordered\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=4800 all=46827 active=2649 piece=▁fallu\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=86 min_freq=45\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=4820 all=46838 active=2353 piece=cts\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=4840 all=46907 active=2422 piece=▁project\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=4860 all=47054 active=2569 piece=ousand\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=4880 all=47062 active=2577 piece=▁pén\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=4900 all=47119 active=2634 piece=▁nerveux\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=83 min_freq=44\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=4920 all=47165 active=2402 piece=▁Sont\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=4940 all=47208 active=2445 piece=▁ouverte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=4960 all=47261 active=2498 piece=▁mille\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=4980 all=47340 active=2577 piece=omas\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=5000 all=47393 active=2630 piece=▁Thomas\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=80 min_freq=42\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=5020 all=47392 active=2369 piece=▁explanation\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=5040 all=47418 active=2395 piece=affaire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=5060 all=47493 active=2470 piece=uits\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=5080 all=47555 active=2532 piece=▁church\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=5100 all=47604 active=2581 piece=▁ant\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=41\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=5120 all=47657 active=2421 piece=▁guilty\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=5140 all=47693 active=2457 piece=ctobre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=5160 all=47736 active=2500 piece=ode\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=5180 all=47782 active=2546 piece=▁enemy\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=5200 all=47839 active=2603 piece=▁dom\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=74 min_freq=40\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=5220 all=47880 active=2427 piece=▁serons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5240 all=47925 active=2472 piece=âte\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5260 all=48028 active=2575 piece=▁bored\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=5280 all=48041 active=2588 piece=▁absolument\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=5300 all=48093 active=2640 piece=▁croit\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=72 min_freq=39\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=5320 all=48137 active=2449 piece=▁prisonn\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=5340 all=48248 active=2560 piece=▁rule\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=5360 all=48273 active=2585 piece=▁accepter\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=5380 all=48333 active=2645 piece=accep\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=5400 all=48368 active=2680 piece=▁cousin\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=70 min_freq=38\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=5420 all=48355 active=2406 piece=▁na\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=5440 all=48401 active=2452 piece=▁pencil\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=5460 all=48430 active=2481 piece=accomp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=5480 all=48463 active=2514 piece=▁lettres\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=5500 all=48499 active=2550 piece=over\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=67 min_freq=37\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=5520 all=48543 active=2451 piece=▁venus\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5540 all=48532 active=2440 piece=ss\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5560 all=48639 active=2547 piece=▁suivi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=5580 all=48640 active=2548 piece=▁particip\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=5600 all=48729 active=2637 piece=riger\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=65 min_freq=36\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=5620 all=48749 active=2449 piece=▁échoué\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5640 all=48764 active=2464 piece=hor\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5660 all=48844 active=2544 piece=▁guest\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=5680 all=48838 active=2538 piece=▁mistaken\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5700 all=48923 active=2623 piece=▁pil\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=63 min_freq=35\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5720 all=48991 active=2506 piece=▁règle\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=5740 all=49012 active=2527 piece=▁expérience\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=5760 all=49131 active=2646 piece=ession\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=5780 all=49183 active=2698 piece=▁temper\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5800 all=49182 active=2697 piece=ix\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=61 min_freq=34\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5820 all=49296 active=2557 piece=▁Voyez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=5840 all=49315 active=2576 piece=▁oiseaux\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5860 all=49353 active=2614 piece=chons\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5880 all=49393 active=2654 piece=▁soupe\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=5900 all=49421 active=2682 piece=embrasser\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=60 min_freq=33\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5920 all=49485 active=2536 piece=umée\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5940 all=49528 active=2579 piece=▁pénét\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=5960 all=49537 active=2588 piece=▁hearing\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=5980 all=49575 active=2626 piece=▁dad\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=6000 all=49647 active=2698 piece=▁ended\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=58 min_freq=32\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=6020 all=49650 active=2486 piece=▁responsib\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=6040 all=49695 active=2531 piece=▁ours\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=6060 all=49728 active=2564 piece=▁fassiez\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6080 all=49748 active=2584 piece=arr\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6100 all=49851 active=2687 piece=▁step\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=56 min_freq=32\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6120 all=49887 active=2526 piece=▁intend\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=6140 all=49889 active=2528 piece=▁préoccup\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6160 all=49963 active=2602 piece=▁diss\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6180 all=50027 active=2666 piece=▁distur\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=6200 all=50023 active=2662 piece=▁incapable\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=55 min_freq=31\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6220 all=50115 active=2594 piece=▁sof\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6240 all=50161 active=2640 piece=▁hesit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=6260 all=50171 active=2650 piece=▁pousser\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6280 all=50234 active=2713 piece=apped\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6300 all=50279 active=2758 piece=▁sépar\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=30\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=6320 all=50307 active=2532 piece=▁argument\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6340 all=50392 active=2617 piece=▁wet\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6360 all=50419 active=2644 piece=▁hobby\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=6380 all=50417 active=2642 piece=▁attitude\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6400 all=50452 active=2677 piece=assé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=51 min_freq=30\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6420 all=50524 active=2581 piece=▁birds\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6440 all=50520 active=2577 piece=▁punctu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=6460 all=50517 active=2574 piece=▁perfectly\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6480 all=50620 active=2677 piece=▁paie\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6500 all=50626 active=2683 piece=▁anyway\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=50 min_freq=29\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=6520 all=50636 active=2542 piece=▁effrayé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6540 all=50638 active=2544 piece=ok\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6560 all=50803 active=2709 piece=agers\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6580 all=50836 active=2742 piece=▁sales\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6600 all=50857 active=2763 piece=▁répéter\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=49 min_freq=28\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=6620 all=50846 active=2532 piece=▁impressionné\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6640 all=50951 active=2637 piece=▁Last\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6660 all=50988 active=2674 piece=▁accus\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6680 all=51018 active=2704 piece=▁England\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=6700 all=51012 active=2698 piece=▁traversé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=48 min_freq=28\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6720 all=51107 active=2646 piece=ashed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6740 all=51161 active=2700 piece=▁fixed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=6760 all=51152 active=2691 piece=▁suicide\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6780 all=51164 active=2703 piece=lit\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6800 all=51242 active=2781 piece=▁fich\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=46 min_freq=27\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6820 all=51281 active=2595 piece=▁miles\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=6840 all=51269 active=2583 piece=▁pleasure\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6860 all=51302 active=2616 piece=▁cla\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6880 all=51360 active=2674 piece=▁coupé\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6900 all=51366 active=2680 piece=▁trompé\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=45 min_freq=27\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=6920 all=51360 active=2563 piece=▁proximité\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6940 all=51402 active=2605 piece=▁bud\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6960 all=51439 active=2642 piece=▁wore\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=6980 all=51452 active=2655 piece=▁guests\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=7000 all=51445 active=2648 piece=▁treated\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=44 min_freq=26\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7020 all=51438 active=2566 piece=ram\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7040 all=51535 active=2663 piece=riage\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7060 all=51593 active=2721 piece=▁drown\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7080 all=51597 active=2725 piece=▁repair\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=7100 all=51587 active=2715 piece=▁exciting\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43 min_freq=25\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7120 all=51606 active=2599 piece=anic\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7140 all=51716 active=2709 piece=▁Tour\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7160 all=51746 active=2739 piece=▁loyer\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7180 all=51743 active=2736 piece=▁remain\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7200 all=51741 active=2734 piece=▁plantes\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=42 min_freq=25\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=7220 all=51733 active=2580 piece=▁succeeded\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7240 all=51828 active=2675 piece=rise\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7260 all=51903 active=2750 piece=▁mett\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7280 all=51924 active=2771 piece=▁attach\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7300 all=51936 active=2783 piece=▁facture\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=24\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=7320 all=51930 active=2590 piece=▁volontaire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7340 all=51992 active=2652 piece=▁Ind\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7360 all=52044 active=2704 piece=érence\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7380 all=52046 active=2706 piece=▁handed\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=7400 all=52043 active=2703 piece=▁parcour\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=40 min_freq=24\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7420 all=52037 active=2594 piece=mi\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7440 all=52194 active=2751 piece=ining\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7460 all=52264 active=2821 piece=▁Jusqu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7480 all=52287 active=2844 piece=▁ruined\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=7500 all=52275 active=2832 piece=enseigner\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39 min_freq=23\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7520 all=52263 active=2599 piece=tée\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7540 all=52317 active=2653 piece=▁loan\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7560 all=52366 active=2702 piece=▁forêt\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7580 all=52356 active=2692 piece=▁retire\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7600 all=52362 active=2698 piece=▁rentrée\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=23\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=7620 all=52374 active=2631 piece=▁continued\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7640 all=52421 active=2678 piece=réh\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7660 all=52495 active=2752 piece=▁bête\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7680 all=52515 active=2772 piece=▁crack\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7700 all=52526 active=2783 piece=▁poches\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=37 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=7720 all=52513 active=2614 piece=▁cleaning\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7740 all=52523 active=2624 piece=▁cy\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7760 all=52585 active=2686 piece=▁Time\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7780 all=52623 active=2724 piece=▁round\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7800 all=52630 active=2731 piece=▁member\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7820 all=52631 active=2633 piece=▁teaches\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=7840 all=52616 active=2618 piece=▁médicaments\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7860 all=52728 active=2730 piece=anced\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7880 all=52761 active=2763 piece=ricité\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7900 all=52769 active=2771 piece=▁pêche\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=35 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=7920 all=52791 active=2659 piece=▁maître\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: spm_model.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_model.vocab\n"
          ]
        }
      ],
      "source": [
        "# Train the SentencePiece model (bpe-based)\n",
        "spm.SentencePieceTrainer.train(input='combined_data.txt', model_prefix='spm_model', vocab_size=vocab_size, model_type='bpe')\n",
        "\n",
        "sp_en = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "sp_fr = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "\n",
        "def tokenize(text, sp_processor):\n",
        "    return sp_processor.encode(text, out_type=str)  # Encode to subword tokens\n",
        "\n",
        "# Tokenize English and French sentences\n",
        "df['english_tokens'] = df['english'].apply(lambda x: tokenize(x, sp_en))\n",
        "df['french_tokens'] = df['french'].apply(lambda x: tokenize(x, sp_fr))\n",
        "\n",
        "def tokens_to_indices(tokens, sp_processor):\n",
        "    return sp_processor.encode(' '.join(tokens), out_type=int)  # Convert to indices\n",
        "\n",
        "df['english_indices'] = df['english_tokens'].apply(lambda x: tokens_to_indices(x, sp_en))\n",
        "df['french_indices'] = df['french_tokens'].apply(lambda x: tokens_to_indices(x, sp_fr))\n",
        "\n",
        "max_len = max(df['english_indices'].apply(len).max(), df['french_indices'].apply(len).max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2huGprHJCA4z",
        "outputId": "cad6e662-d71d-4de0-ae73-12a35c8297ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  english      french english_tokens      french_tokens  \\\n",
            "0     Hi.      Salut!     [▁H, i, .]    [▁S, al, ut, !]   \n",
            "1    Run!     Cours !    [▁R, un, !]     [▁C, ours, ▁!]   \n",
            "2    Run!    Courez !    [▁R, un, !]  [▁C, ou, rez, ▁!]   \n",
            "3    Who?       Qui ?      [▁Who, ?]         [▁Qui, ▁?]   \n",
            "4    Wow!  Ça alors !    [▁W, ow, !]  [▁Ça, ▁alors, ▁!]   \n",
            "\n",
            "           english_indices              french_indices  \n",
            "0  [100, 7636, 7926, 7938]  [118, 303, 7926, 190, 244]  \n",
            "1           [570, 68, 244]             [84, 6042, 244]  \n",
            "2           [570, 68, 244]    [84, 695, 70, 7961, 244]  \n",
            "3                [842, 60]                   [964, 60]  \n",
            "4          [79, 6855, 244]            [652, 2327, 244]  \n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "df = df[:10000]\n",
        "print(df.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvAbHD4_9Dbl",
        "outputId": "88786be9-167f-4a48-9bcc-26ec192991d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max len = 21\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour ajouter du padding\n",
        "def pad_sequence(seq, max_len, sp_model):\n",
        "    pad_id = sp_model.piece_to_id('<pad>')  # Obtenir l'ID du token PAD\n",
        "    return seq + [pad_id] * (max_len - len(seq))  # Ajouter le padding jusqu'à max_len\n",
        "\n",
        "# Calculer la longueur maximale des phrases dans les deux langues\n",
        "max_len = max(\n",
        "    df['english_indices'].apply(len).max(),  # Longueur maximale pour l'anglais\n",
        "    df['french_indices'].apply(len).max()   # Longueur maximale pour le français\n",
        ")\n",
        "\n",
        "# Dataset pour la traduction\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_sentences, french_sentences, sp_en, sp_fr, max_len):\n",
        "        # Tokenisation des phrases en anglais et français\n",
        "        self.english_sentences = [sp_en.encode(sent, out_type=int) for sent in english_sentences]\n",
        "        self.french_sentences = [sp_fr.encode(sent, out_type=int) for sent in french_sentences]\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Padding des séquences\n",
        "        self.english_sentences = [pad_sequence(sent, max_len, sp_en) for sent in self.english_sentences]\n",
        "        self.french_sentences = [pad_sequence(sent, max_len, sp_fr) for sent in self.french_sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.english_sentences[idx]), torch.tensor(self.french_sentences[idx])\n",
        "\n",
        "# Exemple : Charger les phrases depuis ton DataFrame\n",
        "english_sentences = df['english'].tolist()\n",
        "french_sentences = df['french'].tolist()\n",
        "\n",
        "print(f\"max len = {max_len}\")\n",
        "\n",
        "# Créer le dataset\n",
        "dataset = TranslationDataset(english_sentences, french_sentences, sp_en, sp_fr, max_len)\n",
        "\n",
        "# Créer un DataLoader pour charger les données en lots\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "mfDqnTbwthCq",
        "outputId": "05ed56f1-286a-489c-aafd-a731f8b90f61"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention (for encoder/decoder) \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, encoder_out=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Input tensor.\n",
        "            mask: Optional mask for attention.\n",
        "            encoder_out: Optional encoder output for cross-attention.\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape  # Get dimensions of the input tensor\n",
        "        print(f\"x shape = {x.shape}\")\n",
        "        print(f\"x = {x}\")\n",
        "        # If encoder_out is provided, use it for keys and values (cross-attention)\n",
        "        encoder_seq_len = []\n",
        "\n",
        "        if encoder_out is not None:\n",
        "            # Project encoder_out to embed_dim\n",
        "            #print(f\"encoder out = {encoder_out.shape}\")\n",
        "            _, encoder_seq_len, encoder_dim = encoder_out.shape  # Get encoder_out dimensions\n",
        "            #print(f\"encoder dim = {encoder_dim}\")\n",
        "            self.encoder_proj = nn.Linear(encoder_dim, self.embed_dim)  # Initialize encoder_proj\n",
        "            encoder_out = self.encoder_proj(encoder_out)  # Project to embed_dim\n",
        "\n",
        "            # Calculate keys and values from encoder_out\n",
        "            k = self.key(encoder_out)  # (B, T, head_size)\n",
        "            v = self.value(encoder_out)  # (B, T, head_size)\n",
        "        else:  # Otherwise, use x for keys and values (self-attention)\n",
        "            k = self.key(x)  # (B, T, head_size)\n",
        "            v = self.value(x)  # (B, T, head_size)\n",
        "\n",
        "        # Calculate query from input x\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        wei = torch.bmm(q, k.transpose(1, 2)) * (C ** -0.5)  # (B, T, T)\n",
        "        #print(\"Attention weights (wei):\", wei)\n",
        "        # Apply optional padding mask\n",
        "        if mask is not None:\n",
        "            # Reshape mask to match wei's shape for self-attention or cross-attention\n",
        "            # Assuming mask shape is (batch_size, 1, target_sequence_length, source_sequence_length)\n",
        "\n",
        "            # Apply mask to attention scores\n",
        "            if encoder_seq_len:\n",
        "                # If the mask has a dimension of size 256 in the third axis, we need to reshape it\n",
        "                if mask.shape[2] == 256:\n",
        "                    mask = mask[:, :, :max_len-1]  # Slice the mask to have shape [batch_size, seq_len, seq_len]\n",
        "                    \n",
        "                #mask = mask.unsqueeze(1)  # Shape: [batch_size, 1, seq_len]\n",
        "                \n",
        "                mask = mask.expand(-1, encoder_seq_len, encoder_seq_len)\n",
        "                print(\"MASK\")\n",
        "                print(f\"mask.shape = {mask.shape}\")\n",
        "            print(f\"Wei = {wei.shape}\")\n",
        "            wei = wei.masked_fill(mask == 0, float('-inf'))  # Apply mask\n",
        "            \n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        \n",
        "\n",
        "        # Apply dropout\n",
        "        #wei = self.dropout(wei)\n",
        "\n",
        "        # Expand v to match the sequence length T\n",
        "        v = v.expand(-1, T, -1)  # Expands the second dimension (sequence length)\n",
        "        #print(f\"wei shape = {wei.shape}\")\n",
        "        #print(f\"v shape = {v.shape}\")\n",
        "        \n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "        #print(f\"out shape = {out.shape}\")\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "svscC5PItjMB",
        "outputId": "d160d871-6abf-4ac0-d99d-044a3d7cecc6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head attention mechanism \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, head_size, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size, embed_dim=embed_dim) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, embed_dim)  # Projection layer with correct input/output dimensions\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, encoder_out=None):\n",
        "        # Apply each head to the input and concatenate the results\n",
        "        \n",
        "        out = torch.cat([h(x, mask, encoder_out) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Project the concatenated outputs to the original embedding dimension\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ZuezIDxftliz",
        "outputId": "2eab5ec0-93f7-4808-ac4b-1846c49b7f1f"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non linearity\"\"\"\n",
        "\n",
        "    def __init__(self, embd_dim, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embd_dim, 4*ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*ff_dim, embd_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "M-3gvxbFzo6T",
        "outputId": "ac26e05b-11e1-40ac-acbb-18c74c6c0b59"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder Layer\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # Self-Attention + Add & Norm\n",
        "        attn_out = self.self_attn(x, mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # Feedforward + Add & Norm\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        #print(x.shape)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-ip1MeSvzo3B",
        "outputId": "9571d00a-0202-4b32-ff4c-026047b6c2e5"
      },
      "outputs": [],
      "source": [
        "# Transformer Decoder Layer\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads # Calculate head_size here\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, head_size, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout=dropout)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_out, src_mask=None, tgt_mask=None):\n",
        "\n",
        "        # Self-Attention + Add & Norm\n",
        "        print(\"self att\")\n",
        "        self_attn_out = self.self_attn(x, tgt_mask)\n",
        "        x = self.norm1(x + self_attn_out)\n",
        "        x = self.dropout(x)\n",
        "        # Cross-Attention (Encoder-Decoder)\n",
        "        if src_mask is not None:\n",
        "            src_mask = src_mask.expand(-1, max_len-1, -1)\n",
        "        print(\"cross att\")\n",
        "        cross_attn_out = self.cross_attn(x, encoder_out, src_mask)\n",
        "        x = self.norm2(x + cross_attn_out)\n",
        "        x = self.dropout(x)\n",
        "        # Feedforward + Add & Norm\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-AFhwDJgzo0a",
        "outputId": "32410707-18d6-48e7-ab8c-bca717f8a8a7"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "tcE2faABzoyA",
        "outputId": "cfc55696-8101-43f4-aa52-57b6d4450835"
      },
      "outputs": [],
      "source": [
        "# Transformer Decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "    def forward(self, x, encoder_out, src_mask=None, tgt_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_out, src_mask, tgt_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UWXCZ1lDzovD",
        "outputId": "ef9eb9dc-ab1e-4761-9b45-21bc786e9a18"
      },
      "outputs": [],
      "source": [
        "# Full Transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, ff_dim, dropout=dropout_rate):\n",
        "      super().__init__()\n",
        "      self.src_pad_idx = sp_en.piece_to_id('<pad>')\n",
        "      self.trg_pad_idx = sp_fr.piece_to_id('<pad>')\n",
        "      self.trg_sos_idx = sp_fr.piece_to_id('<sos>')\n",
        "      self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "      self.encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "      self.decoder = TransformerDecoder(num_layers, embed_dim, num_heads, ff_dim, dropout=dropout)\n",
        "      self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "      # Dropout layer\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        print(f\"src shape = {src.shape}\")\n",
        "        print(f\"tgt shape = {tgt.shape}\")\n",
        "        src_emb = self.dropout(self.embedding(src))\n",
        "        tgt_emb = self.dropout(self.embedding(tgt))\n",
        "\n",
        "        # Generate masks if not provided\n",
        "        if src_mask is None:\n",
        "            src_mask = self.make_src_mask(src)\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        # Removing the sequence length adjustment\n",
        "        #src_mask = src_mask[:, :, :tgt.shape[1]] # Adjust src_mask's sequence length\n",
        "\n",
        "        #src_mask = src_mask.unsqueeze(1)  # Add a dimension for heads (if necessary)\n",
        "        #tgt_mask = tgt_mask.unsqueeze(1)  # Add a dimension for heads (if necessary)\n",
        "\n",
        "        # Align src_emb with target sequence length before passing to encoder\n",
        "        src_emb = src_emb[:, :tgt.shape[1], :]\n",
        "        print(f\"src embd shape = {src_emb.shape}\")\n",
        "        encoder_out = self.encoder(src_emb, src_mask)\n",
        "        decoder_out = self.decoder(tgt_emb, encoder_out, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc_out(decoder_out)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        print(f\"src mask shape = {src_mask.shape}\")\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor)\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        print(f\"tgt mask = {trg_mask.shape}\")\n",
        "        return trg_mask\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ywlHKYEEDOG9",
        "outputId": "ab0e238e-706d-4bcd-eb7b-f40a14cfd1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_size = 8000\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'SentencePieceProcessor' object has no attribute 'sos_id'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[492]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Remettre à zéro les gradients\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#print(f\"src shape: {src.shape}\")   # Should be (batch_size, seq_len)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#print(f\"tgt shape: {tgt.shape}\")   # Should be (batch_size, seq_len)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#print(f\"tgt[:, :-1] shape: {tgt[:, :-1].shape}\")  # Should be (batch_size, seq_len - 1)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#print(f\"tgt[:, 1:] shape: {tgt[:, 1:].shape}\")    # Should also be (batch_size, seq_len - 1)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Passer les entrées à travers le modèle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m sos_token = \u001b[43msp_en\u001b[49m\u001b[43m.\u001b[49m\u001b[43msos_id\u001b[49m()  \u001b[38;5;66;03m# Indice du token de début\u001b[39;00m\n\u001b[32m     26\u001b[39m eos_token = sp_en.eos_id()  \u001b[38;5;66;03m# Indice du token de fin\u001b[39;00m\n\u001b[32m     28\u001b[39m tgt = torch.cat([torch.full((tgt.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m), sos_token, dtype=torch.long), tgt], dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'SentencePieceProcessor' object has no attribute 'sos_id'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Supposons que tu as déjà défini le modèle Transformer (comme montré précédemment)\n",
        "model = Transformer(vocab_size=len(sp_en), embed_dim=256, num_layers=6, num_heads=8, ff_dim=512, dropout=dropout_rate)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "# Définir un optimiseur (par exemple Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Définir une fonction de perte (par exemple CrossEntropy pour la traduction)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=sp_en.pad_id())  # Ignorer le PAD token pendant le calcul de la perte\n",
        "\n",
        "# Mettre le modèle en mode entraînement\n",
        "model.train()\n",
        "\n",
        "# Boucle d'entraînement\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0  # Variable pour suivre la perte totale sur un epoch\n",
        "\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()  # Remettre à zéro les gradients\n",
        "        #print(f\"src shape: {src.shape}\")   # Should be (batch_size, seq_len)\n",
        "        #print(f\"tgt shape: {tgt.shape}\")   # Should be (batch_size, seq_len)\n",
        "        #print(f\"tgt[:, :-1] shape: {tgt[:, :-1].shape}\")  # Should be (batch_size, seq_len - 1)\n",
        "        #print(f\"tgt[:, 1:] shape: {tgt[:, 1:].shape}\")    # Should also be (batch_size, seq_len - 1)\n",
        "        # Passer les entrées à travers le modèle\n",
        "        output = model(src, tgt[:, :-1])  # Entrée : src, sortie : tgt décalé d'une position (pour prédire le mot suivant)\n",
        "\n",
        "        # Calculer la perte\n",
        "        # Utilisation de la dernière colonne de la sortie (cible) pour le calcul de la perte\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))  # La sortie sans le token de début, et le target sans le token de début\n",
        "        \n",
        "        # Calculer les gradients et mettre à jour les poids\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:  # Afficher la perte tous les 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {total_loss/100:.4f}\")\n",
        "            total_loss = 0  # Réinitialiser la perte\n",
        "            #print(f\"Last output shape = {output.shape}\")\n",
        "            #print(f\"Last output = {output}\")\n",
        "\n",
        "    # Affichage de la perte à la fin de chaque époque\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncVi1QIZECK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer(\n",
            "  (embedding): Embedding(8000, 256)\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (cross_attn): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0-7): 8 x Head(\n",
            "              (key): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (query): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (value): Linear(in_features=256, out_features=32, bias=False)\n",
            "              (encoder_proj): Linear(in_features=20, out_features=256, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=256, out_features=8000, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if torch.isnan(param).any():\n",
        "        print(f\"⚠️ NaN found in {name}\")\n",
        "    if torch.max(torch.abs(param)) > 1e6:  # Too large?\n",
        "        print(f\"⚠️ Large weights in {name}: {torch.max(torch.abs(param))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding.weight | grad mean: 1.0910219394588694e-09 | grad max: 0.0014179187128320336\n",
            "decoder.layers.0.self_attn.heads.0.key.weight | grad mean: -1.7163264374175924e-06 | grad max: 0.00046530787949450314\n",
            "decoder.layers.0.self_attn.heads.0.query.weight | grad mean: 5.403609293352929e-07 | grad max: 0.0008278907043859363\n",
            "decoder.layers.0.self_attn.heads.0.value.weight | grad mean: -1.4911188372934703e-07 | grad max: 0.00284948805347085\n",
            "decoder.layers.0.self_attn.heads.1.key.weight | grad mean: -1.6824567694584402e-07 | grad max: 0.00024645341909490526\n",
            "decoder.layers.0.self_attn.heads.1.query.weight | grad mean: 8.843265959512792e-07 | grad max: 0.000500938855111599\n",
            "decoder.layers.0.self_attn.heads.1.value.weight | grad mean: 5.182630047784187e-06 | grad max: 0.0029646572656929493\n",
            "decoder.layers.0.self_attn.heads.2.key.weight | grad mean: 1.4277583204602706e-07 | grad max: 0.00034390174550935626\n",
            "decoder.layers.0.self_attn.heads.2.query.weight | grad mean: 3.7206081060503493e-07 | grad max: 0.0007505752728320658\n",
            "decoder.layers.0.self_attn.heads.2.value.weight | grad mean: -4.962373168382328e-06 | grad max: 0.003029735991731286\n",
            "decoder.layers.0.self_attn.heads.3.key.weight | grad mean: -4.6065224523772486e-08 | grad max: 0.0009043036843650043\n",
            "decoder.layers.0.self_attn.heads.3.query.weight | grad mean: 1.5551382830381044e-06 | grad max: 0.0008624177426099777\n",
            "decoder.layers.0.self_attn.heads.3.value.weight | grad mean: 5.2208902161510196e-06 | grad max: 0.003509114496409893\n",
            "decoder.layers.0.self_attn.heads.4.key.weight | grad mean: 2.417490918560361e-07 | grad max: 0.0009551250259391963\n",
            "decoder.layers.0.self_attn.heads.4.query.weight | grad mean: -8.56033784657484e-06 | grad max: 0.0014031463069841266\n",
            "decoder.layers.0.self_attn.heads.4.value.weight | grad mean: -6.497680828942975e-07 | grad max: 0.002511376980692148\n",
            "decoder.layers.0.self_attn.heads.5.key.weight | grad mean: -1.246977490154677e-07 | grad max: 0.00036405707942321897\n",
            "decoder.layers.0.self_attn.heads.5.query.weight | grad mean: 4.745064359212847e-07 | grad max: 0.0004710362700279802\n",
            "decoder.layers.0.self_attn.heads.5.value.weight | grad mean: 5.405594492913224e-06 | grad max: 0.0025798024144023657\n",
            "decoder.layers.0.self_attn.heads.6.key.weight | grad mean: -9.70461201177386e-07 | grad max: 0.00039202504558488727\n",
            "decoder.layers.0.self_attn.heads.6.query.weight | grad mean: -3.690483083573781e-07 | grad max: 0.0004831029218621552\n",
            "decoder.layers.0.self_attn.heads.6.value.weight | grad mean: 5.899428288103081e-06 | grad max: 0.0027538363356143236\n",
            "decoder.layers.0.self_attn.heads.7.key.weight | grad mean: 2.2742048599866393e-07 | grad max: 0.0006908522336743772\n",
            "decoder.layers.0.self_attn.heads.7.query.weight | grad mean: 1.605199031473603e-06 | grad max: 0.001718347892165184\n",
            "decoder.layers.0.self_attn.heads.7.value.weight | grad mean: 2.3785405574017204e-06 | grad max: 0.002634248696267605\n",
            "decoder.layers.0.self_attn.proj.weight | grad mean: -7.384783202724066e-07 | grad max: 0.002702199388295412\n",
            "decoder.layers.0.self_attn.proj.bias | grad mean: -5.165302354726009e-06 | grad max: 0.002136469818651676\n",
            "decoder.layers.0.norm1.weight | grad mean: -5.453703124658205e-07 | grad max: 0.002391712972894311\n",
            "decoder.layers.0.norm1.bias | grad mean: -8.351344149559736e-06 | grad max: 0.0019793263636529446\n",
            "decoder.layers.0.cross_attn.heads.0.key.weight | grad mean: -7.220635696888848e-15 | grad max: 1.679591253404833e-12\n",
            "decoder.layers.0.cross_attn.heads.0.query.weight | grad mean: 2.520010103656433e-16 | grad max: 1.7458914088747113e-12\n",
            "decoder.layers.0.cross_attn.heads.0.value.weight | grad mean: -1.4266117887018481e-06 | grad max: 0.0007963706739246845\n",
            "decoder.layers.0.cross_attn.heads.1.key.weight | grad mean: -1.107968725845378e-17 | grad max: 7.949066209954336e-13\n",
            "decoder.layers.0.cross_attn.heads.1.query.weight | grad mean: 5.590612269456251e-16 | grad max: 1.542691972430954e-12\n",
            "decoder.layers.0.cross_attn.heads.1.value.weight | grad mean: 1.5751408000141964e-07 | grad max: 0.0007338040159083903\n",
            "decoder.layers.0.cross_attn.heads.2.key.weight | grad mean: 5.305191812384705e-16 | grad max: 1.2615944530378065e-12\n",
            "decoder.layers.0.cross_attn.heads.2.query.weight | grad mean: 1.818034460295188e-17 | grad max: 1.6816840888583817e-12\n",
            "decoder.layers.0.cross_attn.heads.2.value.weight | grad mean: -2.8720387490466237e-07 | grad max: 0.0008075898513197899\n",
            "decoder.layers.0.cross_attn.heads.3.key.weight | grad mean: 1.100664046057155e-15 | grad max: 3.7880611733313863e-13\n",
            "decoder.layers.0.cross_attn.heads.3.query.weight | grad mean: 1.4789716713718237e-16 | grad max: 5.217941963925332e-13\n",
            "decoder.layers.0.cross_attn.heads.3.value.weight | grad mean: 2.1392554572230438e-07 | grad max: 0.0003856416733469814\n",
            "decoder.layers.0.cross_attn.heads.4.key.weight | grad mean: 2.2100724367452897e-15 | grad max: 1.075555877000367e-12\n",
            "decoder.layers.0.cross_attn.heads.4.query.weight | grad mean: 4.9756615851893e-16 | grad max: 1.6358975805930154e-12\n",
            "decoder.layers.0.cross_attn.heads.4.value.weight | grad mean: -4.4930402509635314e-08 | grad max: 0.0006070734816603363\n",
            "decoder.layers.0.cross_attn.heads.5.key.weight | grad mean: -4.2083482025570226e-17 | grad max: 1.1552323767732853e-12\n",
            "decoder.layers.0.cross_attn.heads.5.query.weight | grad mean: 2.5476273482038635e-16 | grad max: 9.523973406797004e-13\n",
            "decoder.layers.0.cross_attn.heads.5.value.weight | grad mean: -1.8640573529182802e-08 | grad max: 0.0007708438788540661\n",
            "decoder.layers.0.cross_attn.heads.6.key.weight | grad mean: 1.477111004684502e-15 | grad max: 1.1906788489196574e-12\n",
            "decoder.layers.0.cross_attn.heads.6.query.weight | grad mean: 3.3150026701204093e-16 | grad max: 8.862321683802965e-13\n",
            "decoder.layers.0.cross_attn.heads.6.value.weight | grad mean: -2.9950567181913357e-07 | grad max: 0.0009156465530395508\n",
            "decoder.layers.0.cross_attn.heads.7.key.weight | grad mean: 3.5821451091022725e-15 | grad max: 1.479254761217741e-12\n",
            "decoder.layers.0.cross_attn.heads.7.query.weight | grad mean: 9.193932514025611e-17 | grad max: 1.6483210196066578e-12\n",
            "decoder.layers.0.cross_attn.heads.7.value.weight | grad mean: -2.3614206838828977e-06 | grad max: 0.0009351811022497714\n",
            "decoder.layers.0.cross_attn.proj.weight | grad mean: 1.6127972912727273e-07 | grad max: 0.000995903043076396\n",
            "decoder.layers.0.cross_attn.proj.bias | grad mean: -1.1021067621186376e-05 | grad max: 0.0019609173759818077\n",
            "decoder.layers.0.norm2.weight | grad mean: -1.4656793609901797e-06 | grad max: 0.002411504043266177\n",
            "decoder.layers.0.norm2.bias | grad mean: 7.940627256175503e-06 | grad max: 0.0018450797069817781\n",
            "decoder.layers.0.ff.net.0.weight | grad mean: -8.936360984534986e-08 | grad max: 0.0023909439332783222\n",
            "decoder.layers.0.ff.net.0.bias | grad mean: -8.112856448860839e-06 | grad max: 0.000753016909584403\n",
            "decoder.layers.0.ff.net.2.weight | grad mean: -1.2766960253429716e-06 | grad max: 0.0016854631248861551\n",
            "decoder.layers.0.ff.net.2.bias | grad mean: -6.1905338952783495e-06 | grad max: 0.001180208520963788\n",
            "decoder.layers.0.norm3.weight | grad mean: -1.0933824341918807e-05 | grad max: 0.0014772180002182722\n",
            "decoder.layers.0.norm3.bias | grad mean: 4.8504200094612315e-05 | grad max: 0.0016644158167764544\n",
            "decoder.layers.1.self_attn.heads.0.key.weight | grad mean: -4.5778847379551735e-09 | grad max: 0.00041809037793427706\n",
            "decoder.layers.1.self_attn.heads.0.query.weight | grad mean: 2.48365523702887e-07 | grad max: 0.0005818491918034852\n",
            "decoder.layers.1.self_attn.heads.0.value.weight | grad mean: -3.3918462349902256e-07 | grad max: 0.0015818229876458645\n",
            "decoder.layers.1.self_attn.heads.1.key.weight | grad mean: 4.6556593247260025e-08 | grad max: 0.0002500014961697161\n",
            "decoder.layers.1.self_attn.heads.1.query.weight | grad mean: -4.022907660328201e-07 | grad max: 0.0003315315116196871\n",
            "decoder.layers.1.self_attn.heads.1.value.weight | grad mean: 5.548251920117764e-07 | grad max: 0.00149614829570055\n",
            "decoder.layers.1.self_attn.heads.2.key.weight | grad mean: -2.8814454822168045e-07 | grad max: 0.00024062342708930373\n",
            "decoder.layers.1.self_attn.heads.2.query.weight | grad mean: -3.8041093830543105e-07 | grad max: 0.00033477821853011847\n",
            "decoder.layers.1.self_attn.heads.2.value.weight | grad mean: -3.916015430149855e-06 | grad max: 0.0012493334943428636\n",
            "decoder.layers.1.self_attn.heads.3.key.weight | grad mean: 1.9840666709569632e-07 | grad max: 0.0002490546612534672\n",
            "decoder.layers.1.self_attn.heads.3.query.weight | grad mean: 2.445787572469271e-07 | grad max: 0.0003947452059946954\n",
            "decoder.layers.1.self_attn.heads.3.value.weight | grad mean: 1.5778880424477393e-06 | grad max: 0.0014417333295568824\n",
            "decoder.layers.1.self_attn.heads.4.key.weight | grad mean: 2.5243195977964206e-07 | grad max: 0.00042739565833471715\n",
            "decoder.layers.1.self_attn.heads.4.query.weight | grad mean: -2.3049273067954346e-07 | grad max: 0.0006130394176580012\n",
            "decoder.layers.1.self_attn.heads.4.value.weight | grad mean: 8.653780696477043e-07 | grad max: 0.0012118982849642634\n",
            "decoder.layers.1.self_attn.heads.5.key.weight | grad mean: 1.775814126858677e-07 | grad max: 0.00030069041531533003\n",
            "decoder.layers.1.self_attn.heads.5.query.weight | grad mean: -3.070685181683075e-07 | grad max: 0.00030864670407027006\n",
            "decoder.layers.1.self_attn.heads.5.value.weight | grad mean: 2.580631189630367e-06 | grad max: 0.0018612123094499111\n",
            "decoder.layers.1.self_attn.heads.6.key.weight | grad mean: -1.1208953765162732e-08 | grad max: 0.0002312395954504609\n",
            "decoder.layers.1.self_attn.heads.6.query.weight | grad mean: 1.5323996649385663e-07 | grad max: 0.0002934351796284318\n",
            "decoder.layers.1.self_attn.heads.6.value.weight | grad mean: -1.295744823437417e-06 | grad max: 0.002211215440183878\n",
            "decoder.layers.1.self_attn.heads.7.key.weight | grad mean: -1.9102543546978268e-07 | grad max: 0.0003946857468690723\n",
            "decoder.layers.1.self_attn.heads.7.query.weight | grad mean: -4.7275375436584e-07 | grad max: 0.00048124202294275165\n",
            "decoder.layers.1.self_attn.heads.7.value.weight | grad mean: -1.7485523358118371e-06 | grad max: 0.0018684235401451588\n",
            "decoder.layers.1.self_attn.proj.weight | grad mean: 7.277927238646953e-07 | grad max: 0.0016857075970619917\n",
            "decoder.layers.1.self_attn.proj.bias | grad mean: 6.0337815739330836e-06 | grad max: 0.001195121556520462\n",
            "decoder.layers.1.norm1.weight | grad mean: -3.454767011135118e-06 | grad max: 0.0012887460179626942\n",
            "decoder.layers.1.norm1.bias | grad mean: 6.686587312287884e-06 | grad max: 0.0014382812660187483\n",
            "decoder.layers.1.cross_attn.heads.0.key.weight | grad mean: 4.556700560631602e-16 | grad max: 8.997085845438568e-13\n",
            "decoder.layers.1.cross_attn.heads.0.query.weight | grad mean: 2.7100001231211647e-16 | grad max: 1.6776618072186777e-12\n",
            "decoder.layers.1.cross_attn.heads.0.value.weight | grad mean: 1.4126595715424628e-07 | grad max: 0.0005216080462560058\n",
            "decoder.layers.1.cross_attn.heads.1.key.weight | grad mean: 5.819036938298238e-17 | grad max: 3.6333116419998146e-13\n",
            "decoder.layers.1.cross_attn.heads.1.query.weight | grad mean: 4.552652801934911e-16 | grad max: 6.686093193854215e-13\n",
            "decoder.layers.1.cross_attn.heads.1.value.weight | grad mean: 8.888947604646091e-07 | grad max: 0.0006791269988752902\n",
            "decoder.layers.1.cross_attn.heads.2.key.weight | grad mean: 2.021354131371742e-16 | grad max: 4.96846812823859e-13\n",
            "decoder.layers.1.cross_attn.heads.2.query.weight | grad mean: -1.9340276814877838e-18 | grad max: 5.728664070892009e-13\n",
            "decoder.layers.1.cross_attn.heads.2.value.weight | grad mean: -2.3905383272904146e-07 | grad max: 0.0007069580024108291\n",
            "decoder.layers.1.cross_attn.heads.3.key.weight | grad mean: 4.960506842273941e-17 | grad max: 4.886146825686111e-13\n",
            "decoder.layers.1.cross_attn.heads.3.query.weight | grad mean: 3.891243419302246e-16 | grad max: 5.556400066615563e-13\n",
            "decoder.layers.1.cross_attn.heads.3.value.weight | grad mean: 5.705851435777731e-07 | grad max: 0.0005780550418421626\n",
            "decoder.layers.1.cross_attn.heads.4.key.weight | grad mean: -1.0613214303000016e-16 | grad max: 8.252945310656401e-13\n",
            "decoder.layers.1.cross_attn.heads.4.query.weight | grad mean: -9.598358982604017e-17 | grad max: 1.5444365621467004e-12\n",
            "decoder.layers.1.cross_attn.heads.4.value.weight | grad mean: 4.422296058237407e-07 | grad max: 0.00046332646161317825\n",
            "decoder.layers.1.cross_attn.heads.5.key.weight | grad mean: -8.208795344584029e-16 | grad max: 6.295301194399194e-13\n",
            "decoder.layers.1.cross_attn.heads.5.query.weight | grad mean: 3.83190029101761e-16 | grad max: 5.641458437552482e-13\n",
            "decoder.layers.1.cross_attn.heads.5.value.weight | grad mean: 4.930694217364362e-07 | grad max: 0.0004372294351924211\n",
            "decoder.layers.1.cross_attn.heads.6.key.weight | grad mean: -1.6093051074218758e-15 | grad max: 2.4172979156550367e-12\n",
            "decoder.layers.1.cross_attn.heads.6.query.weight | grad mean: -1.6627611449648578e-16 | grad max: 2.0814606548763548e-12\n",
            "decoder.layers.1.cross_attn.heads.6.value.weight | grad mean: -9.996506378229242e-07 | grad max: 0.0006091052782721817\n",
            "decoder.layers.1.cross_attn.heads.7.key.weight | grad mean: -4.3358981295237194e-16 | grad max: 8.653262026196706e-13\n",
            "decoder.layers.1.cross_attn.heads.7.query.weight | grad mean: -6.934254280938643e-16 | grad max: 8.72198851770839e-13\n",
            "decoder.layers.1.cross_attn.heads.7.value.weight | grad mean: -4.661282559936808e-07 | grad max: 0.0005676813307218254\n",
            "decoder.layers.1.cross_attn.proj.weight | grad mean: -3.6227149280421145e-08 | grad max: 0.000873554206918925\n",
            "decoder.layers.1.cross_attn.proj.bias | grad mean: 7.5160460255574435e-06 | grad max: 0.0016212302725762129\n",
            "decoder.layers.1.norm2.weight | grad mean: -1.444041117792949e-07 | grad max: 0.0013055651215836406\n",
            "decoder.layers.1.norm2.bias | grad mean: -1.065155811375007e-05 | grad max: 0.0014706449583172798\n",
            "decoder.layers.1.ff.net.0.weight | grad mean: 1.5986906021225877e-08 | grad max: 0.0008050905889831483\n",
            "decoder.layers.1.ff.net.0.bias | grad mean: -3.18315755976073e-07 | grad max: 0.0004438667674548924\n",
            "decoder.layers.1.ff.net.2.weight | grad mean: -1.7084647652154672e-07 | grad max: 0.0014871266903355718\n",
            "decoder.layers.1.ff.net.2.bias | grad mean: -1.910184437292628e-07 | grad max: 0.0009473562822677195\n",
            "decoder.layers.1.norm3.weight | grad mean: -2.6303459890186787e-06 | grad max: 0.0013608381850644946\n",
            "decoder.layers.1.norm3.bias | grad mean: -9.025598046719097e-06 | grad max: 0.0012832803186029196\n",
            "decoder.layers.2.self_attn.heads.0.key.weight | grad mean: -5.5650247077210224e-08 | grad max: 0.00019081291975453496\n",
            "decoder.layers.2.self_attn.heads.0.query.weight | grad mean: 1.5838978129067982e-07 | grad max: 0.0001594458008185029\n",
            "decoder.layers.2.self_attn.heads.0.value.weight | grad mean: -1.934304691530997e-06 | grad max: 0.0012786593288183212\n",
            "decoder.layers.2.self_attn.heads.1.key.weight | grad mean: -1.9796536321337044e-07 | grad max: 0.00021205376833677292\n",
            "decoder.layers.2.self_attn.heads.1.query.weight | grad mean: 6.681563036181615e-08 | grad max: 0.0002919751568697393\n",
            "decoder.layers.2.self_attn.heads.1.value.weight | grad mean: 3.933150765078608e-07 | grad max: 0.0011545688612386584\n",
            "decoder.layers.2.self_attn.heads.2.key.weight | grad mean: -1.0561603858150193e-07 | grad max: 0.00016266154125332832\n",
            "decoder.layers.2.self_attn.heads.2.query.weight | grad mean: -1.8567023474247435e-08 | grad max: 0.00017889933951664716\n",
            "decoder.layers.2.self_attn.heads.2.value.weight | grad mean: -1.0821561318152817e-06 | grad max: 0.0016231619520112872\n",
            "decoder.layers.2.self_attn.heads.3.key.weight | grad mean: -2.106442309468548e-08 | grad max: 0.00013210513861849904\n",
            "decoder.layers.2.self_attn.heads.3.query.weight | grad mean: -6.218465387064498e-08 | grad max: 0.00012322174734435976\n",
            "decoder.layers.2.self_attn.heads.3.value.weight | grad mean: 3.913673367605952e-07 | grad max: 0.0010055231396108866\n",
            "decoder.layers.2.self_attn.heads.4.key.weight | grad mean: 1.1487740891880094e-07 | grad max: 0.00015986338257789612\n",
            "decoder.layers.2.self_attn.heads.4.query.weight | grad mean: -1.5819988163912058e-07 | grad max: 0.00015586700465064496\n",
            "decoder.layers.2.self_attn.heads.4.value.weight | grad mean: 1.1986685422016308e-06 | grad max: 0.0011881699319928885\n",
            "decoder.layers.2.self_attn.heads.5.key.weight | grad mean: 3.728678166226018e-08 | grad max: 0.00018747508875094354\n",
            "decoder.layers.2.self_attn.heads.5.query.weight | grad mean: 4.825000132768764e-08 | grad max: 0.00024964805925264955\n",
            "decoder.layers.2.self_attn.heads.5.value.weight | grad mean: -7.969724151735136e-07 | grad max: 0.001181675703264773\n",
            "decoder.layers.2.self_attn.heads.6.key.weight | grad mean: -1.1860322501888731e-08 | grad max: 0.0001903039519675076\n",
            "decoder.layers.2.self_attn.heads.6.query.weight | grad mean: 6.8094081484559865e-09 | grad max: 0.0002207463257946074\n",
            "decoder.layers.2.self_attn.heads.6.value.weight | grad mean: -2.379379111516755e-07 | grad max: 0.0012998658930882812\n",
            "decoder.layers.2.self_attn.heads.7.key.weight | grad mean: -7.601441609494941e-08 | grad max: 0.0001450685813324526\n",
            "decoder.layers.2.self_attn.heads.7.query.weight | grad mean: 1.9593159095165902e-08 | grad max: 0.00019580914522521198\n",
            "decoder.layers.2.self_attn.heads.7.value.weight | grad mean: 6.218978683136811e-07 | grad max: 0.0009121999028138816\n",
            "decoder.layers.2.self_attn.proj.weight | grad mean: 2.506310181615845e-07 | grad max: 0.001559887663461268\n",
            "decoder.layers.2.self_attn.proj.bias | grad mean: 1.0129478141607251e-05 | grad max: 0.0013567989226430655\n",
            "decoder.layers.2.norm1.weight | grad mean: 1.4297802408691496e-06 | grad max: 0.0014056203654035926\n",
            "decoder.layers.2.norm1.bias | grad mean: -4.6719901547476184e-07 | grad max: 0.0015153916319832206\n",
            "decoder.layers.2.cross_attn.heads.0.key.weight | grad mean: -4.4014928903546844e-16 | grad max: 5.653926762536066e-13\n",
            "decoder.layers.2.cross_attn.heads.0.query.weight | grad mean: 3.7032497506150745e-16 | grad max: 9.92966884931501e-13\n",
            "decoder.layers.2.cross_attn.heads.0.value.weight | grad mean: -1.3855685665475903e-07 | grad max: 0.0006563778151758015\n",
            "decoder.layers.2.cross_attn.heads.1.key.weight | grad mean: 2.6829758016368123e-16 | grad max: 7.915669530435265e-13\n",
            "decoder.layers.2.cross_attn.heads.1.query.weight | grad mean: -1.279291859688299e-16 | grad max: 6.45071019170218e-13\n",
            "decoder.layers.2.cross_attn.heads.1.value.weight | grad mean: 2.815074253703642e-07 | grad max: 0.0006704565021209419\n",
            "decoder.layers.2.cross_attn.heads.2.key.weight | grad mean: 8.785723249244326e-16 | grad max: 6.318617504219581e-13\n",
            "decoder.layers.2.cross_attn.heads.2.query.weight | grad mean: -3.0164833779151683e-16 | grad max: 6.803946512104475e-13\n",
            "decoder.layers.2.cross_attn.heads.2.value.weight | grad mean: 6.637997103098314e-07 | grad max: 0.0004546923446469009\n",
            "decoder.layers.2.cross_attn.heads.3.key.weight | grad mean: -1.0194844083923028e-15 | grad max: 3.505994322985928e-13\n",
            "decoder.layers.2.cross_attn.heads.3.query.weight | grad mean: -2.5291244428666854e-16 | grad max: 6.283279018609589e-13\n",
            "decoder.layers.2.cross_attn.heads.3.value.weight | grad mean: -7.221548514735332e-08 | grad max: 0.0005250498070381582\n",
            "decoder.layers.2.cross_attn.heads.4.key.weight | grad mean: -1.2106588975546511e-16 | grad max: 1.0692348699145593e-12\n",
            "decoder.layers.2.cross_attn.heads.4.query.weight | grad mean: 1.4659887738727834e-16 | grad max: 2.1705172381647486e-12\n",
            "decoder.layers.2.cross_attn.heads.4.value.weight | grad mean: 3.681565772239992e-07 | grad max: 0.0006146710366010666\n",
            "decoder.layers.2.cross_attn.heads.5.key.weight | grad mean: 1.6434607581078398e-15 | grad max: 1.1493344253751814e-12\n",
            "decoder.layers.2.cross_attn.heads.5.query.weight | grad mean: -2.0487858227641645e-16 | grad max: 1.509470608403174e-12\n",
            "decoder.layers.2.cross_attn.heads.5.value.weight | grad mean: -8.874175705386733e-07 | grad max: 0.0004499476926866919\n",
            "decoder.layers.2.cross_attn.heads.6.key.weight | grad mean: -8.983154982546279e-16 | grad max: 8.882922067181276e-13\n",
            "decoder.layers.2.cross_attn.heads.6.query.weight | grad mean: 1.1060712926341896e-15 | grad max: 1.0378345318556859e-12\n",
            "decoder.layers.2.cross_attn.heads.6.value.weight | grad mean: 1.8800471934810048e-06 | grad max: 0.00048438861267641187\n",
            "decoder.layers.2.cross_attn.heads.7.key.weight | grad mean: 3.0552660519546273e-15 | grad max: 1.5267729571930011e-12\n",
            "decoder.layers.2.cross_attn.heads.7.query.weight | grad mean: -5.72831653327375e-16 | grad max: 2.749231181251033e-12\n",
            "decoder.layers.2.cross_attn.heads.7.value.weight | grad mean: -6.258933638036979e-08 | grad max: 0.0005193384713493288\n",
            "decoder.layers.2.cross_attn.proj.weight | grad mean: -4.190377467239159e-08 | grad max: 0.0007448457181453705\n",
            "decoder.layers.2.cross_attn.proj.bias | grad mean: 7.1186705099535175e-06 | grad max: 0.0016736615216359496\n",
            "decoder.layers.2.norm2.weight | grad mean: 1.381442416459322e-06 | grad max: 0.0012179521145299077\n",
            "decoder.layers.2.norm2.bias | grad mean: -1.6880447219591588e-05 | grad max: 0.0016882915515452623\n",
            "decoder.layers.2.ff.net.0.weight | grad mean: 9.63682111887465e-08 | grad max: 0.0008272810373455286\n",
            "decoder.layers.2.ff.net.0.bias | grad mean: -1.4387251212610863e-06 | grad max: 0.0003157559549435973\n",
            "decoder.layers.2.ff.net.2.weight | grad mean: -5.994239700157777e-07 | grad max: 0.0013511477736756206\n",
            "decoder.layers.2.ff.net.2.bias | grad mean: -2.405622126389062e-06 | grad max: 0.0010823823977261782\n",
            "decoder.layers.2.norm3.weight | grad mean: -4.3230102164670825e-06 | grad max: 0.0011734955478459597\n",
            "decoder.layers.2.norm3.bias | grad mean: 5.756418886448955e-06 | grad max: 0.0015724904369562864\n",
            "decoder.layers.3.self_attn.heads.0.key.weight | grad mean: -6.63703829673068e-08 | grad max: 0.00014456592907663435\n",
            "decoder.layers.3.self_attn.heads.0.query.weight | grad mean: -2.4606782744740485e-07 | grad max: 0.00021634524455294013\n",
            "decoder.layers.3.self_attn.heads.0.value.weight | grad mean: 9.274820058635669e-07 | grad max: 0.0012514725094661117\n",
            "decoder.layers.3.self_attn.heads.1.key.weight | grad mean: -1.0365209135443365e-07 | grad max: 0.00011148315388709307\n",
            "decoder.layers.3.self_attn.heads.1.query.weight | grad mean: -1.4364403000399761e-09 | grad max: 0.00011960849224124104\n",
            "decoder.layers.3.self_attn.heads.1.value.weight | grad mean: -2.1933408334007254e-07 | grad max: 0.00123024289496243\n",
            "decoder.layers.3.self_attn.heads.2.key.weight | grad mean: 1.2514971103883e-07 | grad max: 0.0001552232715766877\n",
            "decoder.layers.3.self_attn.heads.2.query.weight | grad mean: 1.0683100271080548e-07 | grad max: 0.00016199439414776862\n",
            "decoder.layers.3.self_attn.heads.2.value.weight | grad mean: -1.3879755442758324e-06 | grad max: 0.001431800308637321\n",
            "decoder.layers.3.self_attn.heads.3.key.weight | grad mean: -5.776470857199456e-09 | grad max: 9.480024891672656e-05\n",
            "decoder.layers.3.self_attn.heads.3.query.weight | grad mean: -8.175074128757842e-08 | grad max: 0.00018028297927230597\n",
            "decoder.layers.3.self_attn.heads.3.value.weight | grad mean: 3.624722353379184e-07 | grad max: 0.0009931762469932437\n",
            "decoder.layers.3.self_attn.heads.4.key.weight | grad mean: -7.292668158243032e-08 | grad max: 0.00021100250887684524\n",
            "decoder.layers.3.self_attn.heads.4.query.weight | grad mean: 2.316209020136739e-07 | grad max: 0.00028298282995820045\n",
            "decoder.layers.3.self_attn.heads.4.value.weight | grad mean: 1.4877413150315988e-06 | grad max: 0.0012996188597753644\n",
            "decoder.layers.3.self_attn.heads.5.key.weight | grad mean: 1.9407806917115522e-08 | grad max: 0.00017613137606531382\n",
            "decoder.layers.3.self_attn.heads.5.query.weight | grad mean: -7.252532441270887e-08 | grad max: 0.0001669951161602512\n",
            "decoder.layers.3.self_attn.heads.5.value.weight | grad mean: -3.024348188773729e-06 | grad max: 0.0010239507537335157\n",
            "decoder.layers.3.self_attn.heads.6.key.weight | grad mean: 3.768359135847277e-08 | grad max: 0.00010342076711822301\n",
            "decoder.layers.3.self_attn.heads.6.query.weight | grad mean: -1.510718590225224e-07 | grad max: 0.00012537547445390373\n",
            "decoder.layers.3.self_attn.heads.6.value.weight | grad mean: 9.68443828242016e-07 | grad max: 0.0011099579278379679\n",
            "decoder.layers.3.self_attn.heads.7.key.weight | grad mean: -2.015044913150632e-07 | grad max: 0.00028816514532081783\n",
            "decoder.layers.3.self_attn.heads.7.query.weight | grad mean: 1.6127293633871886e-07 | grad max: 0.00020562206918839365\n",
            "decoder.layers.3.self_attn.heads.7.value.weight | grad mean: -1.5259495285135927e-06 | grad max: 0.0013136467896401882\n",
            "decoder.layers.3.self_attn.proj.weight | grad mean: 2.270710126595077e-07 | grad max: 0.0012749653542414308\n",
            "decoder.layers.3.self_attn.proj.bias | grad mean: 8.175735274562612e-06 | grad max: 0.001486693974584341\n",
            "decoder.layers.3.norm1.weight | grad mean: 9.849845810094848e-07 | grad max: 0.0012897023698315024\n",
            "decoder.layers.3.norm1.bias | grad mean: -1.025709934765473e-06 | grad max: 0.0016570283332839608\n",
            "decoder.layers.3.cross_attn.heads.0.key.weight | grad mean: -1.834468170021342e-15 | grad max: 5.973905181297368e-13\n",
            "decoder.layers.3.cross_attn.heads.0.query.weight | grad mean: 3.174514579582199e-16 | grad max: 6.719591246276413e-13\n",
            "decoder.layers.3.cross_attn.heads.0.value.weight | grad mean: -6.023628884577192e-07 | grad max: 0.0004471490974538028\n",
            "decoder.layers.3.cross_attn.heads.1.key.weight | grad mean: 2.689647774283216e-16 | grad max: 5.941570477806246e-13\n",
            "decoder.layers.3.cross_attn.heads.1.query.weight | grad mean: -1.0099994393280676e-17 | grad max: 1.1629990590358852e-12\n",
            "decoder.layers.3.cross_attn.heads.1.value.weight | grad mean: -2.2298159763067815e-07 | grad max: 0.0005101742572151124\n",
            "decoder.layers.3.cross_attn.heads.2.key.weight | grad mean: 3.141555204115554e-16 | grad max: 3.4750839900989094e-13\n",
            "decoder.layers.3.cross_attn.heads.2.query.weight | grad mean: 9.925406344472783e-17 | grad max: 3.989336801440285e-13\n",
            "decoder.layers.3.cross_attn.heads.2.value.weight | grad mean: 5.345734734873986e-07 | grad max: 0.0005617317510768771\n",
            "decoder.layers.3.cross_attn.heads.3.key.weight | grad mean: 8.325798123170634e-16 | grad max: 1.0026593270928696e-12\n",
            "decoder.layers.3.cross_attn.heads.3.query.weight | grad mean: 4.5851746319447396e-17 | grad max: 8.189820349668864e-13\n",
            "decoder.layers.3.cross_attn.heads.3.value.weight | grad mean: -1.2196070429126848e-06 | grad max: 0.00052583625074476\n",
            "decoder.layers.3.cross_attn.heads.4.key.weight | grad mean: 3.0539534685237383e-17 | grad max: 5.893773967133309e-13\n",
            "decoder.layers.3.cross_attn.heads.4.query.weight | grad mean: 3.851167643589685e-17 | grad max: 6.730380142094816e-13\n",
            "decoder.layers.3.cross_attn.heads.4.value.weight | grad mean: -1.1648842246358981e-06 | grad max: 0.00041295646224170923\n",
            "decoder.layers.3.cross_attn.heads.5.key.weight | grad mean: -2.492506149765698e-16 | grad max: 8.82060050000355e-13\n",
            "decoder.layers.3.cross_attn.heads.5.query.weight | grad mean: -1.7298063438388775e-15 | grad max: 1.1727778036901837e-12\n",
            "decoder.layers.3.cross_attn.heads.5.value.weight | grad mean: -3.93983100366313e-07 | grad max: 0.0004180590040050447\n",
            "decoder.layers.3.cross_attn.heads.6.key.weight | grad mean: 1.7356380068620457e-17 | grad max: 5.332821315616465e-13\n",
            "decoder.layers.3.cross_attn.heads.6.query.weight | grad mean: 1.1840787389720152e-16 | grad max: 5.319932320189957e-13\n",
            "decoder.layers.3.cross_attn.heads.6.value.weight | grad mean: -3.936840187179769e-08 | grad max: 0.0005346532561816275\n",
            "decoder.layers.3.cross_attn.heads.7.key.weight | grad mean: -2.901856526745612e-15 | grad max: 8.777778308897977e-13\n",
            "decoder.layers.3.cross_attn.heads.7.query.weight | grad mean: -3.440339723512404e-16 | grad max: 8.078330213968921e-13\n",
            "decoder.layers.3.cross_attn.heads.7.value.weight | grad mean: -3.146223832573014e-07 | grad max: 0.000458411785075441\n",
            "decoder.layers.3.cross_attn.proj.weight | grad mean: 1.4630319356001564e-07 | grad max: 0.0007023068028502166\n",
            "decoder.layers.3.cross_attn.proj.bias | grad mean: 1.1489924872876145e-05 | grad max: 0.0018214117735624313\n",
            "decoder.layers.3.norm2.weight | grad mean: 1.1167558113811538e-06 | grad max: 0.001288978150114417\n",
            "decoder.layers.3.norm2.bias | grad mean: -1.2495715054683387e-05 | grad max: 0.00171602051705122\n",
            "decoder.layers.3.ff.net.0.weight | grad mean: -2.0612738182990142e-09 | grad max: 0.0005578101263381541\n",
            "decoder.layers.3.ff.net.0.bias | grad mean: -2.9447560336848255e-06 | grad max: 0.00023809002595953643\n",
            "decoder.layers.3.ff.net.2.weight | grad mean: 1.043676661538484e-06 | grad max: 0.0012688649585470557\n",
            "decoder.layers.3.ff.net.2.bias | grad mean: 3.501525497995317e-06 | grad max: 0.0008975486853159964\n",
            "decoder.layers.3.norm3.weight | grad mean: -2.469054379616864e-05 | grad max: 0.0011867322027683258\n",
            "decoder.layers.3.norm3.bias | grad mean: 1.7149750419775955e-06 | grad max: 0.0013907335232943296\n",
            "decoder.layers.4.self_attn.heads.0.key.weight | grad mean: 1.3285226430070907e-07 | grad max: 0.00024142011534422636\n",
            "decoder.layers.4.self_attn.heads.0.query.weight | grad mean: 3.053586965506838e-07 | grad max: 0.00031660229433327913\n",
            "decoder.layers.4.self_attn.heads.0.value.weight | grad mean: -4.056369107274804e-08 | grad max: 0.0011637551942840219\n",
            "decoder.layers.4.self_attn.heads.1.key.weight | grad mean: 1.4560118444251202e-08 | grad max: 0.00013461658090818673\n",
            "decoder.layers.4.self_attn.heads.1.query.weight | grad mean: 2.4742384141518414e-08 | grad max: 0.0001550931774545461\n",
            "decoder.layers.4.self_attn.heads.1.value.weight | grad mean: 9.74163185674115e-07 | grad max: 0.0013871680712327361\n",
            "decoder.layers.4.self_attn.heads.2.key.weight | grad mean: -5.982212059052472e-09 | grad max: 0.00021505719632841647\n",
            "decoder.layers.4.self_attn.heads.2.query.weight | grad mean: 2.823625777637062e-10 | grad max: 0.00030803459230810404\n",
            "decoder.layers.4.self_attn.heads.2.value.weight | grad mean: 1.1364261354174232e-06 | grad max: 0.0013073391746729612\n",
            "decoder.layers.4.self_attn.heads.3.key.weight | grad mean: 5.5860596148704644e-08 | grad max: 0.00016746802430134267\n",
            "decoder.layers.4.self_attn.heads.3.query.weight | grad mean: -7.056383566350632e-08 | grad max: 0.00015723133401479572\n",
            "decoder.layers.4.self_attn.heads.3.value.weight | grad mean: -1.416515033270116e-06 | grad max: 0.0014773888979107141\n",
            "decoder.layers.4.self_attn.heads.4.key.weight | grad mean: 2.446380875653631e-08 | grad max: 0.00019699808035511523\n",
            "decoder.layers.4.self_attn.heads.4.query.weight | grad mean: 4.5785185420754715e-08 | grad max: 0.00011776486644521356\n",
            "decoder.layers.4.self_attn.heads.4.value.weight | grad mean: -4.4681542021862697e-07 | grad max: 0.0015440600691363215\n",
            "decoder.layers.4.self_attn.heads.5.key.weight | grad mean: -9.622439733902866e-08 | grad max: 0.0002724445366766304\n",
            "decoder.layers.4.self_attn.heads.5.query.weight | grad mean: 1.5164192745942273e-07 | grad max: 0.00016745904576964676\n",
            "decoder.layers.4.self_attn.heads.5.value.weight | grad mean: -8.818150831757521e-07 | grad max: 0.0014890814200043678\n",
            "decoder.layers.4.self_attn.heads.6.key.weight | grad mean: 8.493059056036145e-08 | grad max: 0.0001692196965450421\n",
            "decoder.layers.4.self_attn.heads.6.query.weight | grad mean: 3.590000829944984e-08 | grad max: 0.00027295437757857144\n",
            "decoder.layers.4.self_attn.heads.6.value.weight | grad mean: -5.954664175078506e-07 | grad max: 0.0011830917792394757\n",
            "decoder.layers.4.self_attn.heads.7.key.weight | grad mean: -3.613618559938914e-08 | grad max: 0.0006475219852291048\n",
            "decoder.layers.4.self_attn.heads.7.query.weight | grad mean: -3.182879027008312e-07 | grad max: 0.000635109725408256\n",
            "decoder.layers.4.self_attn.heads.7.value.weight | grad mean: -9.641547649152926e-07 | grad max: 0.0012299640802666545\n",
            "decoder.layers.4.self_attn.proj.weight | grad mean: -3.744333980648662e-07 | grad max: 0.001950284349732101\n",
            "decoder.layers.4.self_attn.proj.bias | grad mean: -8.944715773395728e-06 | grad max: 0.001283525605686009\n",
            "decoder.layers.4.norm1.weight | grad mean: -5.615719601337332e-06 | grad max: 0.0011580465361475945\n",
            "decoder.layers.4.norm1.bias | grad mean: 9.891949048324022e-06 | grad max: 0.0016411254182457924\n",
            "decoder.layers.4.cross_attn.heads.0.key.weight | grad mean: -1.155718752408961e-16 | grad max: 1.0146810692016062e-12\n",
            "decoder.layers.4.cross_attn.heads.0.query.weight | grad mean: -8.207773081695812e-17 | grad max: 9.031632863462646e-13\n",
            "decoder.layers.4.cross_attn.heads.0.value.weight | grad mean: 2.489067583155702e-07 | grad max: 0.0006813060608692467\n",
            "decoder.layers.4.cross_attn.heads.1.key.weight | grad mean: 1.4510756471056282e-15 | grad max: 5.361593330768799e-13\n",
            "decoder.layers.4.cross_attn.heads.1.query.weight | grad mean: 1.3052031269404e-16 | grad max: 5.432707777565382e-13\n",
            "decoder.layers.4.cross_attn.heads.1.value.weight | grad mean: 3.524044132063864e-06 | grad max: 0.00048187095671892166\n",
            "decoder.layers.4.cross_attn.heads.2.key.weight | grad mean: -9.485371404885194e-16 | grad max: 9.937147675900815e-13\n",
            "decoder.layers.4.cross_attn.heads.2.query.weight | grad mean: 1.3952762929140671e-15 | grad max: 1.0028362688874193e-12\n",
            "decoder.layers.4.cross_attn.heads.2.value.weight | grad mean: -6.486448000941891e-08 | grad max: 0.0004682251310441643\n",
            "decoder.layers.4.cross_attn.heads.3.key.weight | grad mean: -2.5039564470258e-15 | grad max: 1.1123134748339258e-12\n",
            "decoder.layers.4.cross_attn.heads.3.query.weight | grad mean: -9.556087009277904e-16 | grad max: 5.873583412176198e-13\n",
            "decoder.layers.4.cross_attn.heads.3.value.weight | grad mean: -8.988914714791463e-07 | grad max: 0.0005446812720037997\n",
            "decoder.layers.4.cross_attn.heads.4.key.weight | grad mean: -8.226840322734098e-16 | grad max: 1.1744209120825855e-12\n",
            "decoder.layers.4.cross_attn.heads.4.query.weight | grad mean: -1.7521655784266676e-15 | grad max: 7.211743680533844e-13\n",
            "decoder.layers.4.cross_attn.heads.4.value.weight | grad mean: 2.7084746534455917e-07 | grad max: 0.0006686466513201594\n",
            "decoder.layers.4.cross_attn.heads.5.key.weight | grad mean: 3.910851702635591e-17 | grad max: 9.32396087421905e-13\n",
            "decoder.layers.4.cross_attn.heads.5.query.weight | grad mean: 3.936320924568344e-17 | grad max: 6.822490163961581e-13\n",
            "decoder.layers.4.cross_attn.heads.5.value.weight | grad mean: 3.2404673788732907e-07 | grad max: 0.000489058147650212\n",
            "decoder.layers.4.cross_attn.heads.6.key.weight | grad mean: -6.776750621979074e-16 | grad max: 6.882208019622083e-13\n",
            "decoder.layers.4.cross_attn.heads.6.query.weight | grad mean: -7.175179038499736e-17 | grad max: 7.312015034253994e-13\n",
            "decoder.layers.4.cross_attn.heads.6.value.weight | grad mean: 1.0780814818645013e-06 | grad max: 0.000739307957701385\n",
            "decoder.layers.4.cross_attn.heads.7.key.weight | grad mean: -1.495961828804765e-16 | grad max: 1.2201399829728232e-12\n",
            "decoder.layers.4.cross_attn.heads.7.query.weight | grad mean: -9.8682527963568e-17 | grad max: 8.538731787605114e-13\n",
            "decoder.layers.4.cross_attn.heads.7.value.weight | grad mean: 1.472628241572238e-08 | grad max: 0.0004676823446061462\n",
            "decoder.layers.4.cross_attn.proj.weight | grad mean: 2.043850244604073e-08 | grad max: 0.0008792605949565768\n",
            "decoder.layers.4.cross_attn.proj.bias | grad mean: 6.012360245222226e-07 | grad max: 0.0017108055762946606\n",
            "decoder.layers.4.norm2.weight | grad mean: -2.8955105335626286e-07 | grad max: 0.0011466556461527944\n",
            "decoder.layers.4.norm2.bias | grad mean: -7.584088962175883e-06 | grad max: 0.0014312751591205597\n",
            "decoder.layers.4.ff.net.0.weight | grad mean: -5.388951507256934e-08 | grad max: 0.0005762439104728401\n",
            "decoder.layers.4.ff.net.0.bias | grad mean: 2.4327641767740715e-06 | grad max: 0.0002487602469045669\n",
            "decoder.layers.4.ff.net.2.weight | grad mean: 1.3879184734832961e-06 | grad max: 0.0015930215595290065\n",
            "decoder.layers.4.ff.net.2.bias | grad mean: 4.312570126785431e-06 | grad max: 0.0011778553016483784\n",
            "decoder.layers.4.norm3.weight | grad mean: -5.947191311861388e-06 | grad max: 0.0021905817557126284\n",
            "decoder.layers.4.norm3.bias | grad mean: 1.804968633223325e-06 | grad max: 0.0019250912591814995\n",
            "decoder.layers.5.self_attn.heads.0.key.weight | grad mean: 2.785219521683757e-07 | grad max: 0.000760778842959553\n",
            "decoder.layers.5.self_attn.heads.0.query.weight | grad mean: -2.471289803906984e-07 | grad max: 0.0005180192529223859\n",
            "decoder.layers.5.self_attn.heads.0.value.weight | grad mean: -1.0191298542849836e-06 | grad max: 0.002307161223143339\n",
            "decoder.layers.5.self_attn.heads.1.key.weight | grad mean: -2.530384790588869e-10 | grad max: 0.00014862850366625935\n",
            "decoder.layers.5.self_attn.heads.1.query.weight | grad mean: -4.415880994201871e-09 | grad max: 0.0002722055069170892\n",
            "decoder.layers.5.self_attn.heads.1.value.weight | grad mean: -1.1292669910289987e-07 | grad max: 0.0016616621287539601\n",
            "decoder.layers.5.self_attn.heads.2.key.weight | grad mean: 4.699086844084377e-07 | grad max: 0.0007924657547846437\n",
            "decoder.layers.5.self_attn.heads.2.query.weight | grad mean: -2.580549960384815e-07 | grad max: 0.0009106264915317297\n",
            "decoder.layers.5.self_attn.heads.2.value.weight | grad mean: -3.234511041227961e-07 | grad max: 0.0015346742002293468\n",
            "decoder.layers.5.self_attn.heads.3.key.weight | grad mean: 5.821678428219457e-08 | grad max: 0.00033179804449900985\n",
            "decoder.layers.5.self_attn.heads.3.query.weight | grad mean: -5.127285618300448e-08 | grad max: 0.00021147183724679053\n",
            "decoder.layers.5.self_attn.heads.3.value.weight | grad mean: -2.548321162976208e-07 | grad max: 0.0014784201048314571\n",
            "decoder.layers.5.self_attn.heads.4.key.weight | grad mean: 4.9393790391150105e-08 | grad max: 0.0003455063560977578\n",
            "decoder.layers.5.self_attn.heads.4.query.weight | grad mean: 1.887664069499806e-07 | grad max: 0.0002989749191328883\n",
            "decoder.layers.5.self_attn.heads.4.value.weight | grad mean: -8.380126246265718e-07 | grad max: 0.0017185896867886186\n",
            "decoder.layers.5.self_attn.heads.5.key.weight | grad mean: -3.804713628596801e-07 | grad max: 0.000311056908685714\n",
            "decoder.layers.5.self_attn.heads.5.query.weight | grad mean: -5.376898570830235e-08 | grad max: 0.00045235565630719066\n",
            "decoder.layers.5.self_attn.heads.5.value.weight | grad mean: 5.411711754277349e-07 | grad max: 0.0016770991496741772\n",
            "decoder.layers.5.self_attn.heads.6.key.weight | grad mean: -3.1833687330617977e-07 | grad max: 0.00021939001453574747\n",
            "decoder.layers.5.self_attn.heads.6.query.weight | grad mean: 1.2812039074105996e-07 | grad max: 0.0002504881704226136\n",
            "decoder.layers.5.self_attn.heads.6.value.weight | grad mean: 1.7289281686316826e-06 | grad max: 0.0017199635040014982\n",
            "decoder.layers.5.self_attn.heads.7.key.weight | grad mean: 1.1843417269119527e-07 | grad max: 0.00045478969695977867\n",
            "decoder.layers.5.self_attn.heads.7.query.weight | grad mean: 1.7008414943120442e-07 | grad max: 0.0004442690988071263\n",
            "decoder.layers.5.self_attn.heads.7.value.weight | grad mean: 4.983689905202482e-09 | grad max: 0.0016856711590662599\n",
            "decoder.layers.5.self_attn.proj.weight | grad mean: 1.0511523669265443e-06 | grad max: 0.0024762428365647793\n",
            "decoder.layers.5.self_attn.proj.bias | grad mean: 8.095939847407863e-06 | grad max: 0.0011221419554203749\n",
            "decoder.layers.5.norm1.weight | grad mean: -8.51215281727491e-06 | grad max: 0.0019654466304928064\n",
            "decoder.layers.5.norm1.bias | grad mean: 3.2204825402004644e-06 | grad max: 0.0017319568432867527\n",
            "decoder.layers.5.cross_attn.heads.0.key.weight | grad mean: 1.0598863976686752e-15 | grad max: 1.5869080138494751e-12\n",
            "decoder.layers.5.cross_attn.heads.0.query.weight | grad mean: -2.7713139264971473e-16 | grad max: 1.3675122232520431e-12\n",
            "decoder.layers.5.cross_attn.heads.0.value.weight | grad mean: 3.9331075640802737e-07 | grad max: 0.000616534030996263\n",
            "decoder.layers.5.cross_attn.heads.1.key.weight | grad mean: -3.009508061594529e-17 | grad max: 7.086568745012789e-13\n",
            "decoder.layers.5.cross_attn.heads.1.query.weight | grad mean: 7.014155428248733e-17 | grad max: 9.642244684984758e-13\n",
            "decoder.layers.5.cross_attn.heads.1.value.weight | grad mean: -3.2722465448387084e-07 | grad max: 0.0007360935560427606\n",
            "decoder.layers.5.cross_attn.heads.2.key.weight | grad mean: 3.2684699622509287e-15 | grad max: 6.69216526812122e-13\n",
            "decoder.layers.5.cross_attn.heads.2.query.weight | grad mean: -1.0631043023050739e-16 | grad max: 5.815264719419289e-13\n",
            "decoder.layers.5.cross_attn.heads.2.value.weight | grad mean: -9.564042784404592e-07 | grad max: 0.0005401129019446671\n",
            "decoder.layers.5.cross_attn.heads.3.key.weight | grad mean: -2.601770117708832e-15 | grad max: 1.9756425228417696e-12\n",
            "decoder.layers.5.cross_attn.heads.3.query.weight | grad mean: -2.798417922018101e-16 | grad max: 2.021956387043833e-12\n",
            "decoder.layers.5.cross_attn.heads.3.value.weight | grad mean: -1.6290302937704837e-06 | grad max: 0.0009318098891526461\n",
            "decoder.layers.5.cross_attn.heads.4.key.weight | grad mean: -4.1961050824582905e-15 | grad max: 1.2415696725928682e-12\n",
            "decoder.layers.5.cross_attn.heads.4.query.weight | grad mean: 4.704418342072424e-17 | grad max: 7.879926095313849e-13\n",
            "decoder.layers.5.cross_attn.heads.4.value.weight | grad mean: 2.6296768282918492e-06 | grad max: 0.0005939949187450111\n",
            "decoder.layers.5.cross_attn.heads.5.key.weight | grad mean: -8.319650252160344e-16 | grad max: 1.5236765842085997e-12\n",
            "decoder.layers.5.cross_attn.heads.5.query.weight | grad mean: 5.693238781345582e-16 | grad max: 1.622815055078719e-12\n",
            "decoder.layers.5.cross_attn.heads.5.value.weight | grad mean: -2.326060894120019e-06 | grad max: 0.0008269291138276458\n",
            "decoder.layers.5.cross_attn.heads.6.key.weight | grad mean: 1.0406853254247227e-15 | grad max: 1.4815865548301055e-12\n",
            "decoder.layers.5.cross_attn.heads.6.query.weight | grad mean: -2.729843193399577e-16 | grad max: 1.4480118493143124e-12\n",
            "decoder.layers.5.cross_attn.heads.6.value.weight | grad mean: -9.249804406863404e-07 | grad max: 0.0007876026211306453\n",
            "decoder.layers.5.cross_attn.heads.7.key.weight | grad mean: 7.856548223138856e-16 | grad max: 1.0063753216188465e-12\n",
            "decoder.layers.5.cross_attn.heads.7.query.weight | grad mean: -4.790357775505253e-17 | grad max: 1.0034942711859007e-12\n",
            "decoder.layers.5.cross_attn.heads.7.value.weight | grad mean: -9.108571248361841e-07 | grad max: 0.0004888558760285378\n",
            "decoder.layers.5.cross_attn.proj.weight | grad mean: -7.636472787453386e-09 | grad max: 0.0009697836940176785\n",
            "decoder.layers.5.cross_attn.proj.bias | grad mean: -4.797821020474657e-07 | grad max: 0.0015778144588693976\n",
            "decoder.layers.5.norm2.weight | grad mean: -9.785253496374935e-07 | grad max: 0.0019725763704627752\n",
            "decoder.layers.5.norm2.bias | grad mean: -6.280612069531344e-06 | grad max: 0.0015789042226970196\n",
            "decoder.layers.5.ff.net.0.weight | grad mean: -1.240098157495595e-07 | grad max: 0.0008557807886973023\n",
            "decoder.layers.5.ff.net.0.bias | grad mean: 4.713257112598512e-06 | grad max: 0.00037164025707170367\n",
            "decoder.layers.5.ff.net.2.weight | grad mean: 7.204271241789684e-07 | grad max: 0.0030744033865630627\n",
            "decoder.layers.5.ff.net.2.bias | grad mean: 7.695025487919338e-07 | grad max: 0.0013411047402769327\n",
            "decoder.layers.5.norm3.weight | grad mean: -0.00029828850529156625 | grad max: 0.0030647278763353825\n",
            "decoder.layers.5.norm3.bias | grad mean: 2.8424910851754248e-05 | grad max: 0.0022124473471194506\n",
            "fc_out.weight | grad mean: -2.9103830890414573e-12 | grad max: 0.02630987949669361\n",
            "fc_out.bias | grad mean: 1.7402926877352343e-09 | grad max: 0.008172914385795593\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"{name} | grad mean: {param.grad.mean().item()} | grad max: {param.grad.max().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20])\n",
            "input tensor = tensor([[ 153,   14, 3190, 7926, 7962,  670,  238,   36,   60,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "decoder input = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "MASK TRIL\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1.]])\n",
            "FULL MASK\n",
            "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          0., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 0., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 0.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "          1., 1., 1.]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 3.3975e-01,  2.3503e+00,  7.0235e-01,  ...,  1.7404e+00,\n",
            "          -8.1165e-02, -1.8740e+00],\n",
            "         [-2.0843e+00,  2.3829e+00,  9.0700e-01,  ...,  4.1443e-01,\n",
            "          -2.1690e-03, -1.5109e-01],\n",
            "         [-9.6420e-01,  1.8519e+00, -4.2471e-01,  ..., -1.1321e+00,\n",
            "           1.0177e+00,  9.3898e-02],\n",
            "         ...,\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00],\n",
            "         [-1.5378e+00, -1.7423e+00,  1.7504e-01,  ...,  9.1594e-02,\n",
            "           5.4962e-01,  1.8518e+00]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.3247,  1.8687,  0.0172,  ...,  1.0867, -0.1087, -1.2071],\n",
            "         [-1.7556,  1.9579,  0.5744,  ...,  0.2268,  0.4988,  0.1757],\n",
            "         [-0.9076,  1.2372, -1.1530,  ..., -0.9629,  1.0549,  0.1907],\n",
            "         ...,\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694],\n",
            "         [-1.8780, -2.2848, -0.4106,  ..., -0.2656,  0.1387,  2.0694]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[ 0.4140,  2.1740,  0.3811,  ...,  1.3833, -0.3396, -0.7233],\n",
            "         [-1.7431,  1.9869,  0.3461,  ...,  0.5093,  0.2604,  0.2364],\n",
            "         [-0.6753,  1.1102, -0.8858,  ..., -0.2411,  0.8978,  0.4250],\n",
            "         ...,\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262],\n",
            "         [-1.5574, -1.9505, -0.6182,  ...,  0.2345, -0.1218,  2.2262]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5529,  1.9411,  0.3686,  ...,  1.4839, -0.4206, -0.6491],\n",
            "         [-2.6552,  1.6770,  0.1025,  ...,  0.7661,  0.2847,  0.1456],\n",
            "         [-1.4270,  0.7740, -1.2348,  ...,  0.0941,  1.0192,  0.2908],\n",
            "         ...,\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110],\n",
            "         [-2.1785, -1.8180, -0.4788,  ...,  0.3581, -0.4485,  1.9110]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.5211,  1.7587, -0.2807,  ...,  1.7147, -0.1812, -0.5178],\n",
            "         [-2.6454,  1.3715,  0.1986,  ...,  0.9269,  0.0900,  0.0701],\n",
            "         [-1.2479,  0.5902, -1.6659,  ...,  0.0990,  0.7114, -0.2180],\n",
            "         ...,\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702],\n",
            "         [-2.0785, -1.9848, -0.7389,  ...,  0.5327, -0.5965,  1.7702]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-0.6122,  1.6147, -0.0351,  ...,  1.9943, -0.3795, -0.1123],\n",
            "         [-2.9632,  1.3682,  0.1421,  ...,  1.0330, -0.3064,  0.5807],\n",
            "         [-1.6140,  0.3631, -1.3619,  ...,  0.2255,  0.6303,  0.1443],\n",
            "         ...,\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556],\n",
            "         [-2.2904, -1.5736, -0.9545,  ...,  0.6038, -0.8725,  2.1556]]])\n",
            "self att\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744,   -inf,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744,   -inf,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744,   -inf],\n",
            "         [0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744, 0.0744,\n",
            "          0.0744, 0.0744, 0.0744, 0.0744]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476,   -inf,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476,   -inf,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476,   -inf],\n",
            "         [0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476, 0.1476,\n",
            "          0.1476, 0.1476, 0.1476, 0.1476]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352,    -inf,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352,    -inf,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,    -inf],\n",
            "         [-0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352,\n",
            "          -0.0352, -0.0352, -0.0352, -0.0352, -0.0352, -0.0352]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368,    -inf,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368,    -inf,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,    -inf],\n",
            "         [-0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368,\n",
            "          -0.0368, -0.0368, -0.0368, -0.0368, -0.0368, -0.0368]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635]]])\n",
            "Att scores with mask\n",
            "tensor([[[0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "            -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,   -inf,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "            -inf,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635,   -inf,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635,   -inf,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635,   -inf],\n",
            "         [0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635, 0.1635,\n",
            "          0.1635, 0.1635, 0.1635, 0.1635]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730,    -inf,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730,    -inf,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,    -inf],\n",
            "         [-0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730,\n",
            "          -0.0730, -0.0730, -0.0730, -0.0730, -0.0730, -0.0730]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958,    -inf,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958,    -inf,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,    -inf],\n",
            "         [-0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958,\n",
            "          -0.2958, -0.2958, -0.2958, -0.2958, -0.2958, -0.2958]]])\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         ...,\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518],\n",
            "         [-1.5378, -1.7423,  0.1750,  ...,  0.0916,  0.5496,  1.8518]]])\n",
            "Att scores\n",
            "tensor([[[-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624]]])\n",
            "Att scores with mask\n",
            "tensor([[[-0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "             -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624,    -inf,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624,    -inf,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624,    -inf,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,    -inf],\n",
            "         [-0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624,\n",
            "          -0.0624, -0.0624, -0.0624, -0.0624, -0.0624, -0.0624]]])\n",
            "cross att\n",
            "x shape = torch.Size([1, 20, 256])\n",
            "x = tensor([[[-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         ...,\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723],\n",
            "         [-1.5007, -1.6734, -0.6427,  ..., -0.3928,  0.4158,  2.3723]]])\n",
            "Att scores\n",
            "tensor([[[-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066],\n",
            "         [-0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066,\n",
            "          -0.1066, -0.1066, -0.1066, -0.1066, -0.1066, -0.1066]]])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[452]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput tensor = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdecoder input = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the input tensor through the encoder-decoder model\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput print = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[450]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     31\u001b[39m src_emb = src_emb[:, :tgt.shape[\u001b[32m1\u001b[39m], :]\n\u001b[32m     33\u001b[39m encoder_out = \u001b[38;5;28mself\u001b[39m.encoder(src_emb, src_mask)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m decoder_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc_out(decoder_out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[449]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_out, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[447]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, x, encoder_out, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     24\u001b[39m     src_mask = src_mask.expand(-\u001b[32m1\u001b[39m, max_len-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcross att\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m cross_attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x + cross_attn_out)\n\u001b[32m     28\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[416]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m, encoder_out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Apply each head to the input and concatenate the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Project the concatenated outputs to the original embedding dimension\u001b[39;00m\n\u001b[32m     17\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[415]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x, mask, encoder_out)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m wei = \u001b[43mwei\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply mask\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAtt scores with mask\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(wei)\n",
            "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (20) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "# 1. Preprocess the input sentence\n",
        "input_sentence = \"Hello, how are you?\"  # Simple English sentence to translate\n",
        "input_tokens = tokenize(input_sentence, sp_en)  # Tokenize using the English SentencePiece processor\n",
        "input_indices = tokens_to_indices(input_tokens, sp_en)  # Convert to indices\n",
        "input_indices = pad_sequence(input_indices, max_len, sp_en)  # Pad the sequence to max_len\n",
        "\n",
        "# 2. Feed the input sentence to the model\n",
        "input_tensor = torch.tensor(input_indices).unsqueeze(0)  # Add batch dimension (shape: [1, max_len])\n",
        "\n",
        "# Create a decoder input (usually just the <sos> token for start of sentence)\n",
        "sos_token = sp_fr.piece_to_id('<sos>')\n",
        "decoder_input = torch.tensor([sos_token] * (max_len-1)).unsqueeze(0)  # Add batch dimension and pad decoder input to max_len\n",
        "print(decoder_input.shape)\n",
        "\n",
        "# Perform the forward pass (assuming model.forward(src, tgt) works with the model)\n",
        "with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "    print(f\"input tensor = {input_tensor}\")\n",
        "    print(f\"decoder input = {decoder_input}\")\n",
        "    output = model(input_tensor, decoder_input)  # Pass the input tensor through the encoder-decoder model\n",
        "print(f\"output shape = {output.shape}\")\n",
        "print(f\"output print = {output}\")\n",
        "# 3. Decode the output tokens\n",
        "output_indices = output.argmax(dim=-1).squeeze().tolist()  # Get the token indices with the highest probability\n",
        "output_tokens = [sp_fr.id_to_piece(idx) for idx in output_indices if idx != sp_fr.piece_to_id('<pad>')]  # Decode tokens (remove <pad>)\n",
        "print(f\"output tokens = {output_tokens}\")\n",
        "\n",
        "second_column = output[0, 1, :]\n",
        "# Print the second column\n",
        "print(second_column)\n",
        "print(output)\n",
        "# 4. Post-process the output\n",
        "translated_sentence = \" \".join(output_tokens)  # Join tokens to form the translated sentence\n",
        "print(\"Translated sentence:\", translated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.11.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
